{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bedc781",
   "metadata": {},
   "source": [
    "Setup and Configuration : Weaviate Connection with Huggingface\n",
    "\n",
    "For making use of weaviate, check this link (https://docs.weaviate.io/weaviate/model-providers/huggingface/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c31c912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "# from langchain_community.vectorstores import Weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Configure\n",
    "import os\n",
    "\n",
    "# Best practice: store your credentials in environment variables\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Recommended: save sensitive data as environment variables\n",
    "huggingface_key = os.getenv(\"HUGGINGFACE_APIKEY\")\n",
    "headers = {\n",
    "    \"X-HuggingFace-Api-Key\": huggingface_key,\n",
    "}\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,                       # `weaviate_url`: your Weaviate URL\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),      # `weaviate_key`: your Weaviate API key\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "# Work with Weaviate\n",
    "client.is_ready()\n",
    "\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9ea010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7590861",
   "metadata": {},
   "source": [
    "Data Ingestion\n",
    "\n",
    "<pre style=\"font-size: 12px;\">\n",
    "PDFs   → Text Extraction   →  Chunking   →    Embedding     →   Weaviate Storage\n",
    "  ↓           ↓                 ↓              ↓                    ↓\n",
    "[PDF1,2,3] → [Text1,2,3] → [Chunk1,2,3...] → [Vector1,2,3...] → Collection\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f24c7",
   "metadata": {},
   "source": [
    "Document Processing : PDF Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22b7bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 0, 'page_label': 'Build a Large Language Model (From Scratch)'}, page_content='MANNING\\nSebastian Raschka\\nFROMSCRATCH\\nBUILD A'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 1, 'page_label': 'IFC'}, page_content='1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nThe three main stages of coding a large language model (LLM) are implementing the LLM architecture and data \\npreparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the \\nfoundation model to become a personal assistant or text classifier (stage 3). Each of these stages is explored \\nand implemented in this book.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 2, 'page_label': 'i'}, page_content='Build a Large Language Model (From Scratch)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 3, 'page_label': 'ii'}, page_content='Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 4, 'page_label': 'iii'}, page_content='Build a Large\\nLanguage Model\\n(From Scratch)\\nSEBASTIAN RASCHKA\\nMANNING\\nSHELTER ISLAND\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 5, 'page_label': 'iv'}, page_content='For online information and ordering of this and other Manning books, please visit\\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \\nFor more information, please contact\\nSpecial Sales Department\\nManning Publications Co.\\n20 Baldwin Road\\nPO Box 761\\nShelter Island, NY 11964\\nEmail: orders@manning.com\\n©2025 by Manning Publications Co. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \\npermission of the publisher.\\nMany of the designations used by manufacturers and sellers to distinguish their products are \\nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \\nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \\nRecognizing also our responsibility to conserve the resources of our planet, Manning books\\nare printed on paper that is at least 15 percent recycled and processed without the use of \\nelemental chlorine.\\nThe authors and publisher have made every effort to ensure that the information in this book \\nwas correct at press time. The authors and publisher do not assume and hereby disclaim any \\nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \\nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \\nof the information herein.\\nManning Publications Co. Development editor: Dustin Archibald\\n20 Baldwin Road Technical editor: David Caswell\\nPO Box 761 Review editor: Kishor Rit\\nShelter Island, NY 11964 Production editor: Aleksandar Dragosavljevic ´\\nCopy editors: Kari Lucke and Alisa Larson\\nProofreader: Mike Beady\\nTechnical proofreader: Jerry Kuch\\nTypesetter: Dennis Dalinnik\\nCover designer: Marija Tudor\\nISBN: 9781633437166\\nPrinted in the United States of America\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 6, 'page_label': 'v'}, page_content='v\\nbrief contents\\n1 ■ Understanding large language models 1\\n2 ■ Working with text data 17\\n3 ■ Coding attention mechanisms 50\\n4 ■ Implementing a GPT model from scratch to \\ngenerate text 92\\n5 ■ Pretraining on unlabeled data 128\\n6 ■ Fine-tuning for classification 169\\n7 ■ Fine-tuning to follow instructions 204\\nA ■ Introduction to PyTorch 251\\nB ■ References and further reading 289\\nC ■ Exercise solutions 300\\nD ■ Adding bells and whistles to the training loop 313\\nE ■ Parameter-efficient fine-tuning with LoRA 322\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 7, 'page_label': 'vi'}, page_content='Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 8, 'page_label': 'vii'}, page_content='vii\\ncontents\\npreface xi\\nacknowledgments xiii\\nabout this book xv\\nabout the author xix\\nabout the cover illustration xx\\n1 Understanding large language models 1\\n1.1 What is an LLM? 2\\n1.2 Applications of LLMs 4\\n1.3 Stages of building and using LLMs 5\\n1.4 Introducing the transformer architecture 7\\n1.5 Utilizing large datasets 10\\n1.6 A closer look at the GPT architecture 12\\n1.7 Building a large language model 14\\n2 Working with text data 17\\n2.1 Understanding word embeddings 18\\n2.2 Tokenizing text 21\\n2.3 Converting tokens into token IDs 24\\n2.4 Adding special context tokens 29\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 9, 'page_label': 'viii'}, page_content='CONTENTSviii\\n2.5 Byte pair encoding 33\\n2.6 Data sampling with a sliding window 35\\n2.7 Creating token embeddings 41\\n2.8 Encoding word positions 43\\n3 Coding attention mechanisms 50\\n3.1 The problem with modeling long sequences 52\\n3.2 Capturing data depend encies with attention \\nmechanisms 54\\n3.3 Attending to different parts of the input with \\nself-attention 55\\nA simple self-attention mechanism without trainable weights 56\\nComputing attention weights for all input tokens 61\\n3.4 Implementing self-attention with trainable weights 64\\nComputing the attention weights step by step 65 ■ Implementing a \\ncompact self-attention Python class 70\\n3.5 Hiding future words with causal attention 74\\nApplying a causal attention mask 75 ■ Masking additional \\nattention weights with dropout 78 ■ Implementing a compact \\ncausal attention class 80\\n3.6 Extending single-head attention to multi-head \\nattention 82\\nStacking multiple single-head attention layers 82 ■ Implementing \\nmulti-head attention with weight splits 86\\n4 Implementing a GPT model from scratch to generate text 92\\n4.1 Coding an LLM architecture 93\\n4.2 Normalizing activations with layer normalization 99\\n4.3 Implementing a feed forward network with GELU \\nactivations 105\\n4.4 Adding shortcut connections 109\\n4.5 Connecting attention and line ar layers in a transformer \\nblock 113\\n4.6 Coding the GPT model 117\\n4.7 Generating text 122\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 10, 'page_label': 'ix'}, page_content='CONTENTS ix\\n5 Pretraining on unlabeled data 128\\n5.1 Evaluating generative text models 129\\nUsing GPT to generate text 130 ■ Calculating the text \\ngeneration loss 132 ■ Calculating the training and validation \\nset losses 140\\n5.2 Training an LLM 146\\n5.3 Decoding strategies to control randomness 151\\nTemperature scaling 152 ■ Top-k sampling 155\\nModifying the text generation function 157\\n5.4 Loading and saving model weights in PyTorch 159\\n5.5 Loading pretrained weights from OpenAI 160\\n6 Fine-tuning for classification 169\\n6.1 Different categories of fine-tuning 170\\n6.2 Preparing the dataset 172\\n6.3 Creating data loaders 175\\n6.4 Initializing a model with pretrained weights 181\\n6.5 Adding a classification head 183\\n6.6 Calculating the classification loss and accuracy 190\\n6.7 Fine-tuning the model on supervised data 195\\n6.8 Using the LLM as a spam classifier 200\\n7 Fine-tuning to follow instructions 204\\n7.1 Introduction to instruction fine-tuning 205\\n7.2 Preparing a dataset for supervised instruction \\nfine-tuning 207\\n7.3 Organizing data into training batches 211\\n7.4 Creating data loaders fo r an instruction dataset 223\\n7.5 Loading a pretrained LLM 226\\n7.6 Fine-tuning the LLM on instruction data 229\\n7.7 Extracting and saving responses 233\\n7.8 Evaluating the fine-tuned LLM 238\\n7.9 Conclusions 247\\nWhat’s next? 247 ■ Staying up to date in a fast-moving \\nfield 248 ■ Final words 248\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 11, 'page_label': 'x'}, page_content='CONTENTSx\\nappendix A Introduction to PyTorch 251\\nappendix B References and further reading 289\\nappendix C Exercise solutions 300\\nappendix D Adding bells and wh istles to the training loop 313\\nappendix E Parameter-efficient fine-tuning with LoRA 322\\nindex 337\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 12, 'page_label': 'xi'}, page_content='xi\\npreface\\nI’ve always been fascinated with language models. More than a decade ago, my jour-\\nney into AI began with a statistical pattern classification class, which led to my first\\nindependent project: developing a model an d web application to detect the mood of\\na song based on its lyrics.\\n Fast forward to 2022, with the release of ChatGPT, large language models (LLMs)\\nhave taken the world by storm and have re volutionized how many of us work. These\\nmodels are incredibly versatile, aiding in tasks such as checking grammar, composing\\nemails, summarizing lengthy documents, and much more. This is owed to their ability\\nto parse and generate human-like text, whic h is important in various fields, from cus-\\ntomer service to content creation, and even  in more technical domains like coding\\nand data analysis. \\n As their name implies, a hallmark of LLMs is that they are “large”—very large—\\nencompassing millions to bi llions of parameters. (For co mparison, using more tradi-\\ntional machine learning or statistical methods, the Iris flower dataset can be classified\\nwith more than 90% accuracy using a small  model with only two parameters.) How-\\never, despite the large size of LLMs comp ared to more traditional methods, LLMs\\ndon’t have to be a black box. \\n In this book, you will learn how to buil d an LLM one step at a time. By the end,\\nyou will have a solid understanding of ho w an LLM, like the ones used in ChatGPT,\\nworks on a fundamental level. I believe that  developing confidence with each part of\\nthe fundamental concepts and underlying co de is crucial for success. This not only\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 13, 'page_label': 'xii'}, page_content='PREFACExii\\nhelps in fixing bugs and improving perf ormance but also enables experimentation\\nwith new ideas.\\n Several years ago, when I started working with LLMs, I had to learn how to imple-\\nment them the hard way, sifting through many research papers and incomplete code\\nrepositories to develop a general understa nding. With this book, I hope to make\\nLLMs more accessible by develo ping and sharing a step-by-step implementation tuto-\\nrial detailing all the major components and development phases of an LLM.\\n I strongly believe that the best way to  understand LLMs is to code one from\\nscratch—and you’ll see that this can be fun too! \\n Happy reading and coding!\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 14, 'page_label': 'xiii'}, page_content='xiii\\nacknowledgments\\nWriting a book is a significant undertaking,  and I would like to express my sincere\\ngratitude to my wife, Liza, for her patience  and support throughout this process. Her\\nunconditional love and constant encouragement have been absolutely essential.\\n I am incredibly grateful to Daniel Klei ne, whose invaluable feedback on the in-\\nprogress chapters and code went above and beyond. With his keen eye for detail and\\ninsightful suggestions, Daniel’s contribu tions have undoubtedly made this book a\\nsmoother and more enjoyable reading experience.\\n I would also like to thank the wonderfu l staff at Manning Publications, including\\nMichael Stephens, for the many  productive discussions th at helped shape the direc-\\ntion of this book, and Dustin Archibald, whose constructive feedback and guidance in\\nadhering to the Manning guidelines have been crucial. I also appreciate your flexibil-\\nity in accommodating the unique requiremen ts of this unconventional from-scratch\\napproach. A special thanks to Aleksandar Dragosavljevic´, Kari Lucke, and Mike Beady\\nfor their work on the professional layout s and to Susan Honeywell and her team for\\nrefining and polishing the graphics.\\n I want to express my heartfelt gratit ude to Robin Campbell and her outstanding\\nmarketing team for their invaluable support throughout the writing process.\\n Finally, I extend my thanks to the re viewers: Anandaganesh  Balakrishnan, Anto\\nAravinth, Ayush Bihani, Bassam Ismail, Benjamin Muskalla, Bruno Sonnino, Christian\\nProkopp, Daniel Kleine, David Curran, Di byendu Roy Chowdhury, Gary Pass, Georg\\nSommer, Giovanni Alzetta, Guillermo Al cántara, Jonathan Reeves, Kunal Ghosh,\\nNicolas Modrzyk, Paul Silisteanu, Raul Ciot escu, Scott Ling, Sriram Macharla, Sumit\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 15, 'page_label': 'xiv'}, page_content='ACKNOWLEDGMENTSxiv\\nPal, Vahid Mirjalili, Vaijanath Rao, and Wa lter Reade for their thorough feedback on\\nthe drafts. Your keen eyes and insightful co mments have been essential in improving\\nthe quality of this book.\\n To everyone who has contributed to this journey, I am sincerely grateful. Your sup-\\nport, expertise, and dedication have been in strumental in bringing this book to frui-\\ntion. Thank you!\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 16, 'page_label': 'xv'}, page_content='xv\\nabout this book\\nBuild a Large Language Model (From Scratch)  was written to help you understand and\\nc r e a t e  y o u r  o w n  G P T - l i k e  l a r g e  l a n g u a g e  m o d e l s  ( L L M s )  f r o m  t h e  g r o u n d  u p .  I t\\nbegins by focusing on the fu ndamentals of working with text data and coding atten-\\ntion mechanisms and then guides you through implementing a complete GPT\\nmodel from scratch. The book then cove rs the pretraining mechanism as well as\\nfine-tuning for specific tasks such as text classification and following instructions. By\\nthe end of this book, you’ll have a deep understanding of how LLMs work and the\\nskills to build your own models. While the models you’ll create are smaller in scale\\ncompared to the large founda tional models, they use the same concepts and serve\\nas powerful educational tools to grasp th e core mechanisms and techniques used in\\nbuilding state-of-the-art LLMs.\\nWho should read this book\\nBuild a Large Language  Model (From Scratch)  is for machine learning enthusiasts, engi-\\nneers, researchers, students , and practitioners who want to gain a deep understand-\\ning of how LLMs work and learn to buil d their own models from scratch. Both\\nbeginners and experienced developers will be able to use their existing skills and\\nknowledge to grasp the co ncepts and techniques used in creating LLMs.\\n What sets this book apar t is its comprehensive coverage of the entire process of\\nbuilding LLMs, from working with datasets  to implementing the model architecture,\\npretraining on unlabeled data, and fine-tuning for specific tasks. As of this writing, no\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 17, 'page_label': 'xvi'}, page_content='ABOUT THIS BOOKxvi\\nother resource provides su ch a complete and hands-on approach to building LLMs\\nfrom the ground up.\\n To understand the code examples in th is book, you should have a solid grasp of\\nPython programming. While some familiarity  with machine learning, deep learning,\\nand artificial intelligence can be beneficial , an extensive background in these areas is\\nnot required. LLMs are a unique subset of AI, so even if you’re relatively new to the\\nfield, you’ll be able to follow along.\\n If you have some experience with deep neural networks, you may find certain con-\\ncepts more familiar, as LLMs are built upon these architectures. However, proficiency\\nin PyTorch is not a prerequisite. Append ix A provides a conc ise introduction to\\nPyTorch, equipping you with the necessary skills to comprehend the code examples\\nthroughout the book.\\n A high school–level understanding of mathematics, particularly working with vec-\\ntors and matrices, can be helpful as we explore the inner workings of LLMs. However,\\nadvanced mathematical know ledge is not necessary to grasp the key concepts and\\nideas presented in this book.\\n The most important prerequisite is a strong foundation in Python programming.\\nWith this knowledge, you’ll be well prepared to explore the fascinating world of LLMs\\nand understand the concepts and code examples presented in this book.\\nHow this book is organized: A roadmap\\nThis book is designed to be read sequentially, as each chapter builds upon the con-\\ncepts and techniques introduced in the previous ones. The book is divided into seven\\nchapters that cover the essential aspects of LLMs and their implementation.\\n Chapter 1 provides a high-level introduc tion to the fundamental concepts behind\\nLLMs. It explores the transformer archit ecture, which forms the basis for LLMs such\\nas those used on the ChatGPT platform.\\n Chapter 2 lays out a plan for building an LLM from scratch. It covers the process of\\npreparing text for LLM training, includin g splitting text into word and subword\\ntokens, using byte pair encoding for adva nced tokenization, sampling training exam-\\nples with a sliding window approach, and converting tokens into vectors that feed into\\nthe LLM.\\n Chapter 3 focuses on the attention mechanisms used in LLMs. It introduces a basic\\nself-attention framework and progresses to  an enhanced self-attention mechanism.\\nThe chapter also covers the implementation of a causal attention module that enables\\nLLMs to generate one token at a time, mask ing randomly selected attention weights\\nwith dropout to reduce over fitting and stacking  multiple causal attention modules\\ninto a multihead attention module.\\n Chapter 4 focuses on coding a GPT-like  LLM that can be trained to generate\\nhuman-like text. It covers techniques such as normalizing layer activations to stabilize\\nneural network training, adding shortcut connections in deep neural networks to\\ntrain models more effectively, implementing transformer blocks to create GPT models\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 18, 'page_label': 'xvii'}, page_content='ABOUT THIS BOOK xvii\\nof various sizes, and computing the number  of parameters and storage requirements\\nof GPT models.\\n Chapter 5 implements the pretraining pr ocess of LLMs. It co vers computing the\\ntraining and validation set losses to assess the quality of LLM-generated text, imple-\\nmenting a training function and pretra ining the LLM, saving and loading model\\nweights to continue training an LLM, an d loading pretrained weights from OpenAI.\\n Chapter 6 introduces different LLM fine-t uning approaches. It covers preparing a\\ndataset for text classification, modifying a pretrained LLM for fine-tuning, fine-tuning\\nan LLM to identify spam messages, and ev aluating the accuracy of a fine-tuned LLM\\nclassifier.\\n Chapter 7 explores the instruction fine-t uning process of LLMs.  It covers prepar-\\ning a dataset for supervised instruction fi ne-tuning, organizing instruction data in\\ntraining batches, loading a pretrained LLM and fine-tuning it to follow human\\ninstructions, extracting LLM-generated instruction responses for evaluation, and eval-\\nuating an instruction-fine-tuned LLM.\\nAbout the code\\nTo make it as easy as possible to follow al ong, all code examples in this book are con-\\nveniently available on the Manning website at https:/ /www.manning.com/books/\\nbuild-a-large-language-model-from-scratch, as well as in Jupyter notebook format on\\nGitHub at https:/ /github.com/rasbt/LLMs-from-scratch. And don’t worry about get-\\nting stuck—solutions to all the code exercises can be found in appendix C.\\n This book contains many examples of source code both in numbered listings and\\nin line with normal text. In both ca ses, source code is formatted in a fixed-width\\nfont like this to separate it from ordinary text.\\n In many cases, the original source co de has been reformatted; we’ve added line\\nbreaks and reworked indent ation to accommodate the available page space in the\\nbook. In rare cases, even this was not en ough, and listings include line-continuation\\nmarkers (➥). Additionally, comments in the source code have often been removed\\nfrom the listings when the code is described in the text. Code annotations accompany\\nmany of the listings, highlighting important concepts.\\n One of the key goals of this book is accessibility, so the code examples have been\\ncarefully designed to run efficiently on a regular laptop, without the need for any spe-\\ncial hardware. But if you do have access to a GPU, certain sections provide helpful tips\\non scaling up the datasets and models to take advantage of that extra power.\\n Throughout the book, we’ll be using PyTorch as our go-to tensor and a deep learn-\\ning library to implement LLMs from the ground up. If PyTorch is new to you, I recom-\\nmend you start with appendix  A, which provides an in-depth introduction, complete\\nwith setup recommendations.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 19, 'page_label': 'xviii'}, page_content='ABOUT THIS BOOKxviii\\nliveBook discussion forum\\nPurchase of Build a Large Language Model (From Scratch)  includes free access to live-\\nBook, Manning’s online reading platform. Us ing liveBook’s exclusive discussion fea-\\ntures, you can attach comments to the book globally or to specific sections or\\nparagraphs. It’s a snap to make notes for yourself, ask and answer technical questions,\\nand receive help from the author and other users. To access the forum, go to https:/ /\\nlivebook.manning.com/book/build-a-large-language-model-from-scratch/discussion.\\nYou can also learn more about Manning’s forums and the rules of conduct at https:/ /\\nlivebook.manning.com/discussion.\\n Manning’s commitment to readers is to provide a venue where a meaningful dia-\\nlogue between individual readers and between readers and the author can take place.\\nIt is not a commitment to any specific amou nt of participation on the part of the\\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\\nyou try asking the author some challenging questions lest his interest stray! The forum\\nand the archives of previous discussions will be accessible from the publisher’s website\\nas long as the book is in print.\\nOther online resources\\nInterested in the latest AI and LLM research trends?\\n\\uf0a1 Check out my blog at https:/ /magazine.sebastianraschka.com, where I regularly\\ndiscusses the latest AI research with a focus on LLMs.\\nNeed help getting up to speed with deep learning and PyTorch?\\n\\uf0a1 I offer several free cour ses on my website at https:/ /sebastianraschka.com/\\nteaching. These resources can help you quickl y get up to speed with the latest\\ntechniques.\\nLooking for bonus materials related to the book?\\n\\uf0a1 Visit the book’s GitHub repository at https:/ /github.com/rasbt/LLMs-from\\n-scratch to find additional resources and examples to supplement your learning.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 20, 'page_label': 'xix'}, page_content='xix\\nabout the author\\nSEBASTIAN RASCHKA, PhD, has been working in machine learn-\\ning and AI for more than a decade. In addition to being a\\nresearcher, Sebastian has a strong  passion for education. He is\\nknown for his bestselling book s on machine learning with\\nPython and his contributions to open source.\\n      Sebastian is a staff research engineer at Lightning AI, focus-\\ning on implementing and training LLMs. Before his industry\\nexperience, Sebastian was an assi stant professor in the Depart-\\nment of Statistics at the University of Wisconsin-Madison, where\\nhe focused on deep learning research. You can learn more about Sebastian at https:/ /\\nsebastianraschka.com.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 21, 'page_label': 'xx'}, page_content='xx\\nabout the cover illustration\\nThe figure on the cover of Build a Large Language Model (From Scratch), titled “Le duch-\\nesse,” or “The duchess,” is  taken from a book by Louis Curmer published in 1841.\\nEach illustration is finely drawn and colored by hand. \\n In those days, it was easy to identify where people lived and what their trade or sta-\\ntion in life was just by their dress. Manni ng celebrates the inventiveness and initiative\\nof the computer business with book covers ba sed on the rich diversity of regional cul-\\nture centuries ago, brought back to life by pictures from collections such as this one.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 22, 'page_label': '1'}, page_content='1\\nUnderstanding large\\nlanguage models\\nLarge language models (LLMs), such as those offered in OpenAI’s ChatGPT, are\\ndeep neural network models that have been developed over the past few years.\\nThey ushered in a new era for natural la nguage processing (NLP). Before the\\nadvent of LLMs, traditional methods excelled at categorization tasks such as email\\nspam classification and straightforward pattern recognition that could be captured\\nwith handcrafted rules or simpler models. However, they typically underperformed\\nin language tasks that demanded complex understanding and generation abilities,\\nsuch as parsing detailed instructions, co nducting contextual analysis, and creating\\ncoherent and contextually appropriate original text. For example, previous genera-\\ntions of language models could not write an email from a list of keywords—a task\\nthat is trivial for contemporary LLMs.\\nThis chapter covers\\n\\uf0a1 High-level explanations of the fundamental \\nconcepts behind large language models (LLMs)\\n\\uf0a1 Insights into the transformer architecture from \\nwhich LLMs are derived\\n\\uf0a1 A plan for building an LLM from scratch\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 23, 'page_label': '2'}, page_content='2 CHAPTER 1 Understanding large language models\\n LLMs have remarkable capabilities to understand, generate, and interpret human\\nlanguage. However, it’s important to clarify that when we say language models “under-\\nstand,” we mean that they can process and generate text in ways that appear coher-\\nent and contextually relevant, not that they possess human-like consciousness or\\ncomprehension. \\n Enabled by advancements in deep learning, which is a subset of machine learn-\\ning and artificial intelligen ce (AI) focused on neural networks, LLMs are trained on\\nvast quantities of text data . This large-scale training allows LLMs to capture deeper\\ncontextual information and subtleties of  human language compared to previous\\napproaches. As a result, LLMs have signif icantly improved performance in a wide\\nrange of NLP tasks, including text translat ion, sentiment analysis, question answer-\\ning, and many more. \\n Another important distinction between contemporary LLMs and earlier NLP mod-\\nels is that earlier NLP models were typically  designed for specific  tasks, such as text\\ncategorization, language translation, etc.  While those earlier NLP models excelled in\\ntheir narrow applications, LLMs demons trate a broader prof iciency across a wide\\nrange of NLP tasks. \\n The success behind LLMs can be attribut ed to the transformer architecture that\\nunderpins many LLMs and the vast amount s of data on which LLMs are trained,\\nallowing them to capture a wide variety of  linguistic nuances, contexts, and patterns\\nthat would be challenging to encode manually. \\n This shift toward implementing models based on the transformer architecture and\\nusing large training datasets to train LL Ms has fundamentally transformed NLP, pro-\\nviding more capable tools for understanding and interacting with human language. \\n The following discussion sets a foundati on to accomplish the primary objective of\\nthis book: understanding LLMs by implem enting a ChatGPT-like LLM based on the\\ntransformer architecture step by step in code.\\n1.1 What is an LLM?\\nAn LLM is a neural network designed to understand, generate, and respond to human-\\nlike text. These models are deep neural ne tworks trained on massive amounts of text\\ndata, sometimes encompassing large portions of the entire publicly available text on\\nthe internet.\\n The “large” in “large language model” refers to both the model’s size in terms of\\nparameters and the immense dataset on which it’s trained. Models like this often have\\ntens or even hundreds of billions of pa rameters, which are the adjustable weights in\\nthe network that are optimized during training to predict the next word in a sequence.\\nNext-word prediction is sensible because it  harnesses the inherent sequential nature\\nof language to train models on understand ing context, structure, and relationships\\nwithin text. Yet, it is a very simple task, an d so it is surprising to many researchers that\\nit can produce such capable models. In late r chapters, we will discuss and implement\\nthe next-word training procedure step by step.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 24, 'page_label': '3'}, page_content='31.1 What is an LLM?\\n LLMs utilize an architecture called the transformer, which allows them to pay selec-\\ntive attention to different parts of the input when making predictions, making them\\nespecially adept at handling the nuances and complexities of human language. \\n Since LLMs are capable of generating text, LLMs are also often referred to as a form\\nof generative artificial intelligence, often abbreviated as generative AI or GenAI. As illus-\\ntrated in figure 1.1, AI encompasses the broader field of creating machines that can\\nperform tasks requiring human-like intelligence, including understanding lan-\\nguage, recognizing patterns, and making decisions, and includes subfields like machine\\nlearning and deep learning. \\nThe algorithms used to implement AI are th e focus of the field of machine learning.\\nSpecifically, machine learning involves the development of algorithms that can learn\\nfrom and make predictions or  decisions based on data wi thout being explicitly pro-\\ngrammed. To illustrate this, imagine a sp am filter as a prac tical application of\\nmachine learning. Instead of manually wr iting rules to identify spam emails, a\\nmachine learning algorithm is fed examples of emails la beled as spam and legitimate\\nemails. By minimizing the error in its pred ictions on a training dataset, the model\\nthen learns to recognize patterns and characteristics indicative of spam, enabling it to\\nclassify new emails as either spam or not spam.\\n As illustrated in figure 1.1, deep learning is a subset of machine learning that focuses\\non utilizing neural networks with three or more layers (also called deep neural net-\\nworks) to model complex patterns and abstractions in data. In contrast to deep learn-\\ning, traditional machine learning requires manual feature extraction. This means that\\nhuman experts need to identify and select the most relevant features for the model.\\nArtiﬁcial intelligence\\nMachine learning\\nDeep learning\\nLarge language models\\nSystems with\\nhuman-like intelligence\\nAlgorithms that learn rules\\nautomatically from data\\nMachine learning with\\nneural networks consisting\\nof many layers\\nDeep neural network for\\nparsing and generating\\nhuman-like text\\nGenAI\\nGenAI involves the use of\\ndeep neural networks to\\ncreate new content, such\\nas text, images, or various\\nforms of media\\nFigure 1.1 As this hierarchical depiction of the rela tionship between the different fields suggests, LLMs \\nrepresent a specific application of deep learning techniques, using their ability to process and generate human-\\nlike text. Deep learning is a specialized branch of machine learning that focuses on using multilayer neural \\nnetworks. Machine learning and deep learning are fields aimed at implementing algorithms that enable computers \\nto learn from data and perform tasks that typically require human intelligence.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 25, 'page_label': '4'}, page_content='4 CHAPTER 1 Understanding large language models\\n While the field of AI is now dominated by machine learning and deep learning, it\\nalso includes other approaches—for example,  using rule-based sy stems, genetic algo-\\nrithms, expert systems, fuzzy logic, or symbolic reasoning.\\n Returning to the spam classification ex ample, in traditional machine learning,\\nhuman experts might manually extract feat ures from email text such as the fre-\\nquency of certain trigger words (for exampl e, “prize,” “win,” “free”), the number of\\nexclamation marks, use of all uppercase word s, or the presence of suspicious links.\\nThis dataset, created based on these expert -defined features, would then be used to\\ntrain the model. In contrast to traditional machine learning, deep learning does not\\nrequire manual feature extraction. This means that human experts do not need to\\nidentify and select the most relevant feat ures for a deep learning model. (However,\\nboth traditional machine learning and deep learning for spam cl assification still\\nrequire the collection of labe ls, such as spam or non-spam, which need to be gath-\\nered either by an expert or users.)\\n Let’s look at some of the problems LLMs can solve today, the challenges that LLMs\\naddress, and the general LLM architecture we will implement later.\\n1.2 Applications of LLMs\\nOwing to their advanced capabi lities to parse and understand unstructured text data,\\nLLMs have a broad range of applications across various domains. Today, LLMs are\\nemployed for machine translation, generation  of novel texts (see figure 1.2), senti-\\nment analysis, text summarization, and many  other tasks. LLMs have recently been\\nused for content creation, such as writing fiction, articles, and even computer code. \\n LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s\\nChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries\\nand augment traditional search engines such as Google Search or Microsoft Bing.\\n Moreover, LLMs may be used for effective knowledge retrieval from vast volumes\\nof text in specialized areas such as medicine or law. This includes sifting through doc-\\numents, summarizing lengthy passages, and answering technical questions.\\n In short, LLMs are invaluable for automati ng almost any task that involves parsing\\nand generating text. Their applications are virtually endless, and as we continue to\\ninnovate and explore new ways to use these models, it’s clear that LLMs have the\\npotential to redefine our relationship with technology, making it more conversational,\\nintuitive, and accessible.\\n We will focus on understanding how LLM s work from the ground up, coding an\\nLLM that can generate texts. You will also  learn about techniques that allow LLMs to\\ncarry out queries, ranging from answering questions to summarizing text, translating\\ntext into different languages, and more. In  other words, you wi ll learn how complex\\nLLM assistants such as ChatGPT work by building one step by step.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 26, 'page_label': '5'}, page_content='51.3 Stages of building and using LLMs\\n1.3 Stages of building and using LLMs\\nWhy should we build our own LLMs? Coding an LLM from the ground up is an excel-\\nlent exercise to understand its mechanics an d limitations. Also, it equips us with the\\nrequired knowledge for pretraining or fine-tuning existing open source LLM architec-\\ntures to our own domain-specific datasets or tasks. \\nNOTE Most LLMs today are implemented using the PyTorch deep learning\\nlibrary, which is what we will use. Readers can find a comprehensive introduc-\\ntion to PyTorch in appendix A.\\nResearch has shown that when it comes to modeling perfor mance, custom-built\\nLLMs—those tailored for specif ic tasks or domains—can ou tperform general-purpose\\nLLMs, such as those provided by ChatGPT,  which are designed for a wide array of\\napplications. Examples of these include BloombergGPT (specialized for finance) and\\nLLMs tailored for medical question answering (see appendix B for more details).\\n Using custom-built LLMs offers several advantages, particularly regarding data pri-\\nvacy. For instance, companies may prefer not to share sensitive data with third-party\\nLLM providers like OpenAI due to confiden tiality concerns. Additionally, developing\\nsmaller custom LLMs enables deployment directly on customer devices, such as laptops\\nand smartphones, which is something compan ies like Apple are currently exploring.\\nUser input\\n(instructions)\\nModel output\\nFigure 1.2 LLM interfaces enable natural language communication between users and AI systems. This \\nscreenshot shows ChatGPT writing a poem according to a user’s specifications.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 27, 'page_label': '6'}, page_content='6 CHAPTER 1 Understanding large language models\\nThis local implementation can significantly decrease latency and reduce server-related\\ncosts. Furthermore, custom  LLMs grant developers complete autonomy, allowing\\nthem to control updates and modifications to the model as needed.\\n The general process of creating an LLM includes pretraining and fine-tuning. The\\n“pre” in “pretraining” refers to the initial phase where a model like an LLM is trained\\non a large, diverse dataset to develop a broad understanding of language. This pre-\\ntrained model then serves as a foundation al resource that can be further refined\\nthrough fine-tuning, a process where the mo del is specifically trained on a narrower\\ndataset that is more specific  to particular tasks or doma ins. This two-stage training\\napproach consisting of pretraining and fine-tuning is depicted in figure 1.3.\\nThe first step in creating an LLM is to train it on a large corpus of text data, sometimes\\nreferred to as raw text. Here, “raw” refers to the fact that this data is just regular text\\nwithout any labeling information. (Filtering may be applied, such as removing format-\\nting characters or documents in unknown languages.)\\nNOTE Readers with a background in mach ine learning may note that label-\\ning information is typically required for traditional machine learning models\\nand deep neural networks trained via the conventional supervised learning\\nparadigm. However, this is not the case for the pretraining stage of LLMs. In\\nthis phase, LLMs use self-supervised learning, where the model generates its\\nown labels from the input data.\\n• Internet texts\\n• Books\\n• Wikipedia\\n• Research articles\\nTrain\\nPretrained LLM\\n(foundation model)Raw, unlabeled text\\n(trillions of words)\\nTrain\\nLabeled dataset\\nFine-tuned LLM\\n• Text completion\\n• Few-shot capabilities\\n• Classiﬁcation\\n• Summarization\\n• Translation\\n• Personal assistant\\n•…\\nAn LLM is pretrained\\non unlabeled text data.\\nThe LLM has a few\\nbasic capabilities\\nafter pretraining.\\nA pretrained LLM can be\\nfurther trained on a labeled\\ndataset to obtain a ﬁne-tuned\\nLLM for speciﬁc tasks.\\nFigure 1.3 Pretraining an LLM involves next-word prediction on large text datasets. A pretrained LLM \\ncan then be fine-tuned using a smaller labeled dataset.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 28, 'page_label': '7'}, page_content='71.4 Introducing the transformer architecture\\nThis first training stage of  an LLM is also known as pretraining, creating an initial pre-\\ntrained LLM, often called a base or foundation model. A typical example of such a model\\nis the GPT-3 model (the precursor of the original model offered in ChatGPT). This\\nmodel is capable of text completion—that is, finishing a half-written sentence pro-\\nvided by a user. It also has limited few-shot capabilities, which means it can learn to\\nperform new tasks based on only a few exam ples instead of need ing extensive train-\\ning data.\\n After obtaining a pretrained LLM from tr aining on large text datasets, where the\\nLLM is trained to predict the next word in the text, we can further train the LLM on\\nlabeled data, also known as fine-tuning.\\n The two most popular categories of fine-tuning LLMs are instruction fine-tuning and\\nclassification fine-tuning. In instruction fine-tuning, the labeled dataset consists of\\ninstruction and answer pairs, such as a qu ery to translate a text accompanied by the\\ncorrectly translated text. In classification fine-tuning, the labeled dataset consists of\\ntexts and associated class labels—for exampl e, emails associated with “spam” and “not\\nspam” labels.\\n We will cover code implementations for pretraining and fine-tuning an LLM, and\\nwe will delve deeper into the specifics of both instruction and classification fine-tuning\\nafter pretraining a base LLM.\\n1.4 Introducing the tr ansformer architecture\\nMost modern LLMs rely on the transformer architecture, which is a deep neural net-\\nwork architecture introduced in the 2017 paper “Attention Is All You Need” (https:/ /\\narxiv.org/abs/1706.03762). To understand LLMs, we must understand the original\\ntransformer, which was developed for machine translation, translating English texts to\\nGerman and French. A simplified version of  the transformer arch itecture is depicted\\nin figure 1.4. \\n The transformer architecture consists  of two submodules: an encoder and a\\ndecoder. The encoder module processes the input text and encodes it into a series of\\nnumerical representations or vectors that capture the contextual information of the\\ninput. Then, the decoder module takes thes e encoded vectors and generates the out-\\nput text. In a translation task, for example, the encoder would encode the text from\\nthe source language into vectors, and the decoder would decode these vectors to gen-\\nerate text in the target language. Both the encoder and decoder consist of many layers\\nconnected by a so-called self-attention mechanism. You may have many questions\\nregarding how the inputs are preprocessed and encoded. These will be addressed in a\\nstep-by-step implementation in subsequent chapters.\\n A key component of transformers and LL Ms is the self-attention mechanism (not\\nshown), which allows the model to weigh the importance of different words or tokens\\nin a sequence relative to each other. Th is mechanism enables the model to capture\\nlong-range dependencies and contextual relationships within the input data, enhanc-\\ning its ability to generate coherent and co ntextually relevant output. However, due to\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 29, 'page_label': '8'}, page_content='8 CHAPTER 1 Understanding large language models\\nits complexity, we will defer further explan ation to chapter 3, where we will discuss\\nand implement it step by step.\\n Later variants of the transformer architecture, such as BERT (short for bidirectional\\nencoder representations from transformers ) and the various GPT models (short for genera-\\ntive pretrained transformers), built on this concept to adapt this architecture for different\\ntasks. If interested, refer to appendix B for further reading suggestions.\\n BERT, which is built upon  the original transformer’s encoder submodule, differs\\nin its training approach from GPT. While GPT is designed for generative tasks, BERT\\nand its variants specialize in masked word prediction, where the model predicts masked\\nInput text\\nOutput layers\\nEncoder\\nEmbeddings\\nDecoder\\n1. The input text to\\nbe translated.\\n8. The complete output\\n(translation)\\n5. A partial output\\ntext: the model\\ncompletes the\\ntranslation one\\nword at a time.\\n3. The encoder has\\naccess to the\\ncomplete input\\ntext to produce\\ntext encodings\\nused by the\\ndecoder.\\n7. The decoder\\ngenerates the\\ntranslated text\\none word at a\\ntime.\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\n2. The input text is\\nprepared for the\\nencoder.\\n4. The encoder returns\\nembedding vectors as\\ninput to the decoder.\\n6. The input text is\\nprepared for the\\ndecoder.\\nFigure 1.4 A simplified depiction of the original transfo rmer architecture, which is a deep learning model for \\nlanguage translation. The transformer consists of two parts: (a) an encoder that processes the input text and \\nproduces an embedding representation (a numerical representation that captures many different factors in \\ndifferent dimensions) of the text that the (b) decoder can use to generate the translated text one word at a time. \\nThis figure shows the final stage of the translation process where the decoder has to generate only the final word \\n(“Beispiel”), given the original input text (“This is an example”) and a partially translated sentence (“Das ist \\nein”), to complete the translation.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 30, 'page_label': '9'}, page_content='91.4 Introducing the transformer architecture\\nor hidden words in a given sentence, as shown in figure 1.5. This unique training strategy\\nequips BERT with strengths in text classification tasks, including sentiment prediction\\nand document categorization. As an application of its capabilities, as of this writing, X\\n(formerly Twitter) uses BERT to detect toxic content.\\nGPT, on the other hand, focuses on the de coder portion of the original transformer\\narchitecture and is designed for tasks that  require generating texts. This includes\\nmachine translation, text summarization, fiction writing, writing computer code,\\nand more. \\n GPT models, primarily designed and trai ned to perform text completion tasks,\\nalso show remarkable versatil ity in their capabilities. Th ese models are adept at exe-\\ncuting both zero-shot and few-shot learning tasks. Zero-shot learning refers to the abil-\\nity to generalize to completely unseen ta sks without any prior specific examples. On\\nthe other hand, few-shot learning involves learning from a minimal number of exam-\\nples the user provides as input, as shown in figure 1.6.\\nInput text\\nEncoder Decoder\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\nBERT GPT\\nReceives inputs where words\\nare randomly masked during\\ntraining\\nLearns to\\ngenerate one\\nword at a\\ntime\\nThis is an __ of how concise I __ be\\nThis is an example of how concise I can be This is an example of how concise I can be\\nThis is an example of how concise I can\\nFills in the\\nmissing\\nwords to\\ngenerate\\nthe original\\nsentence\\nReceives incomplete texts\\nFigure 1.5 A visual representation of the transfo rmer’s encoder and decoder submodules. On the left, the \\nencoder segment exemplifies BERT-like LLMs, which focus on masked word prediction and are primarily used for \\ntasks like text classification. On the right, the decoder segment showcases GPT-like LLMs, designed for \\ngenerative tasks and producing coherent text sequences.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 31, 'page_label': '10'}, page_content='10 CHAPTER 1 Understanding large language models\\n1.5 Utilizing large datasets\\nThe large training datasets for popular GP T- and BERT-like models represent diverse\\nand comprehensive text corpora encompassing billions of words, which include a vast\\narray of topics and natural and computer languages. To provide a concrete example,\\ntable 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base\\nmodel for the first version of ChatGPT.\\n \\nTransformers vs. LLMs\\nToday’s LLMs are based on the transformer architecture. Hence, transformers and\\nLLMs are terms that are often used synonymously in the literature. However, note\\nthat not all transformers are LLMs since transformers can also be used for com-\\nputer vision. Also, not all LLMs are transformers, as there are LLMs based on recur-\\nrent and convolutional architectures. T he main motivation behind these alternative\\napproaches is to improve the computational efficiency of LLMs. Whether these alter-\\nnative LLM architectures can compete wi th the capabilities of transformer-based\\nLLMs and whether they are going to be adopted in practice remains to be seen. For\\nsimplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT.\\n(Interested readers can find literature refe rences describing these architectures in\\nappendix B.) \\nBreakfast is the\\nInput Output\\nmost important meal of the day.TEXT COMPLETION\\nZERO-SHOT\\nFEW-SHOT\\nTranslate English to German:\\nbreakfast =>\\ngaot => goat\\nsheo => shoe\\npohne =>\\nphone\\nCreates plausible text\\ngiven a partial input text\\nCompletes a task given a\\nfew examples of the task\\nCompletes a\\ntask without an\\nexplicit exampleFigure 1.6 In addition to text completion, GPT-like LLMs can solve various tasks based on their inputs without \\nneeding retraining, fine-tuning, or task-specific model architecture changes. Sometimes it is helpful to provide \\nexamples of the target within the input, which is known as a few-shot setting. However, GPT-like LLMs are also \\ncapable of carrying out tasks without a specific example, which is called zero-shot setting.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 32, 'page_label': '11'}, page_content='111.5 Utilizing large datasets\\nTable 1.1 reports the number of tokens, wher e a token is a unit of text that a model\\nreads and the number of tokens in a datase t is roughly equivalent to the number of\\nwords and punctuation characters in the te xt. Chapter 2 addresses tokenization, the\\nprocess of converting text into tokens.\\n The main takeaway is that the scale and diversity of this training dataset allow these\\nmodels to perform well on diverse tasks, including lang uage syntax, semantics, and\\ncontext—even some requir ing general knowledge. \\nThe pretrained nature of these models ma kes them incredibly versatile for further\\nfine-tuning on downstream tasks, which is wh y they are also known as base or founda-\\ntion models. Pretraining LLMs requires ac cess to significant resources and is very\\nexpensive. For example, the GPT-3 pretrainin g cost is estimated to be $4.6 million in\\nterms of cloud computing credits (https:/ /mng.bz/VxEW). \\nTable 1.1 The pretraining dataset of the popular GPT-3 LLM\\nDataset name Dataset description Number of tokens Proportion \\nin training data\\nCommonCrawl (filtered) Web crawl data 410 billion 60%\\nWebText2 Web crawl data 19 billion 22%\\nBooks1 Internet-based book corpus 12 billion 8%\\nBooks2 Internet-based book corpus 55 billion 8%\\nWikipedia High-quality text 3 billion 3%\\nGPT-3 dataset details\\nTable 1.1 displays the dataset used for GPT-3. The proportions column in the table\\nsums up to 100% of the sampled data, adjusted for rounding errors. Although the\\nsubsets in the Number of Tokens column total 499 billion, the model was trained on\\nonly 300 billion tokens. The authors of the GPT-3 paper did not specify why the model\\nwas not trained on all 499 billion tokens.\\nFor context, consider the size of the CommonCrawl dataset, which alone consists of\\n410 billion tokens and requires about 570 GB of storage. In comparison, later itera-\\ntions of models like GPT-3, such as Meta’s LLaMA, have expanded their training\\nscope to include additional data sources like Arxiv research papers (92 GB) and\\nStackExchange’s code-related Q&As (78 GB). \\nThe authors of the GPT-3 paper did not share the training dataset, but a comparable\\ndataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for\\nLLM Pretraining Research by Soldaini et al. 2024 (https:/ /arxiv.org/abs/2402.00159).\\nHowever, the collection may contain copyrighted works, and the exact usage terms\\nmay depend on the intended use case and country.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 33, 'page_label': '12'}, page_content='12 CHAPTER 1 Understanding large language models\\n The good news is that many pretrained  LLMs, available as open source models,\\ncan be used as general-purpose tools to write, extract, and edit texts that were not\\npart of the training data. Also, LLMs can be  fine-tuned on specific tasks with rela-\\ntively smaller datasets, reducing the comp utational resources needed and improving\\nperformance.\\n We will implement the code for pretraining and use it to pretrain an LLM for educa-\\ntional purposes. All computations are executable on consumer hardware. After imple-\\nmenting the pretraining code, we will learn how to reuse openly available model weights\\nand load them into the architecture we will implement, allowing us to skip the expen-\\nsive pretraining stage when we fine-tune our LLM.\\n1.6 A closer look at the GPT architecture\\nGPT was originally introduced in the pa per “Improving Language Understanding by\\nGenerative Pre-Training” ( https:/ /mng.bz/x2qg) by Radford et al. from OpenAI.\\nGPT-3 is a scaled-up version of this model that has more parameters and was trained\\non a larger dataset. In addition, the original model offered in ChatGPT was created by\\nfine-tuning GPT-3 on a large instructio n dataset using a method from OpenAI’s\\nInstructGPT paper ( https:/ /arxiv.org/abs/2203.02155). As figure 1.6 shows, these\\nmodels are competent text completion models and can carry out other tasks such as\\nspelling correction, classification, or language translation. This is actually very remark-\\nable given that GPT models are pretrained on a relatively simple next-word prediction\\ntask, as depicted in figure 1.7.\\nThe next-word prediction task is a form of self-supervised learning, which is a form of\\nself-labeling. This means that we don’t n eed to collect labels for the training data\\nexplicitly but can use the structure of the data itself: we can use the next word in a sen-\\ntence or document as the label that the mode l is supposed to predict. Since this next-\\nword prediction task allows us to create labels “on the fly,” it is possible to use massive\\nunlabeled text datasets to train LLMs.\\n Compared to the original transformer architecture we covered in section 1.4, the\\ngeneral GPT architecture is relatively simp le. Essentially, it’s just the decoder part\\nwithout the encoder (figure 1. 8). Since decoder-style mode ls like GPT generate text\\nby predicting text one word at a time, they are considered a type of autoregressive\\nmodel. Autoregressive models incorporate their previous outputs as inputs for future\\nThe model is simply trained to\\npredict the next   word\\nFigure 1.7 In the next-word predict ion pretraining task for GPT \\nmodels, the system learns to predict the upcoming word in a \\nsentence by looking at the words that have come before it. This \\napproach helps the model understand how words and phrases \\ntypically fit together in language, forming a foundation that can \\nbe applied to various other tasks.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 34, 'page_label': '13'}, page_content='131.6 A closer look at the GPT architecture\\npredictions. Consequently, in GPT, each new word is chosen based on the sequence\\nthat precedes it, which improves the coherence of the resulting text.\\n Architectures such as GPT-3 are also significantly larger than the original transformer\\nmodel. For instance, the original transformer repeated the encoder and decoder blocks\\nsix times. GPT-3 has 96 transformer layers and 175 billion parameters in total.\\nGPT-3 was introduced in 2020, which, by the standards of deep learning and large lan-\\nguage model development, is considered a long time ago. However, more recent archi-\\ntectures, such as Meta’s Llama models, are still based on the same underlying concepts,\\nintroducing only minor modifications. Hence, understanding GPT remains as relevant\\nas ever, so I focus on implementing the prominent architecture behind GPT while pro-\\nviding pointers to specific tweaks employed by alternative LLMs.\\n Although the original transformer model, consisting of encoder and decoder blocks,\\nwas explicitly designed for language translation, GPT models—despite their larger yet\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nIteration 1 Iteration 2 Iteration 3Creates the next\\nword based on\\nthe input text\\nThe output of the\\nprevious round\\nserves as input to\\nthe next round.\\nFigure 1.8 The GPT architecture employs only the decoder porti on of the original transformer. It is designed for \\nunidirectional, left-to-right processing, making it well suited for text generation and next-word prediction tasks to \\ngenerate text in an iterative fashion, one word at a time.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 35, 'page_label': '14'}, page_content='14 CHAPTER 1 Understanding large language models\\nsimpler decoder-only architecture aimed at next-word prediction—are also capable of\\nperforming translation tasks. This capability was initially unexpected to researchers, as\\nit emerged from a model primarily trained on a next-word prediction task, which is a\\ntask that did not specifically target translation.\\n The ability to perform tasks that the mode l wasn’t explicitly trained to perform is\\ncalled an emergent behavior . This capability isn’t explicitly taught during training but\\nemerges as a natural consequence of the mode l’s exposure to vast quantities of multi-\\nlingual data in diverse contexts. The fact that GPT models can “learn” the translation\\npatterns between languages and perform tran slation tasks even though they weren’t\\nspecifically trained for it demonstrates th e benefits and capabilities of these large-\\nscale, generative language models. We can perform diverse tasks without using diverse\\nmodels for each.\\n1.7 Building a large language model\\nNow that we’ve laid the groundwork for understanding LLMs, let’s code one from\\nscratch. We will take the fundamental idea behind GPT as a blueprint and tackle this\\nin three stages, as outlined in figure 1.9.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 1.9 The three main stages of coding an LLM ar e implementing the LLM architecture and data preparation \\nprocess (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation \\nmodel to become a personal assistant or text classifier (stage 3).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 36, 'page_label': '15'}, page_content='15Summary\\nIn stage 1, we will learn about the fundamental data preprocessing steps and code the\\nattention mechanism at the heart of every LL M. Next, in stage 2, we will learn how to\\ncode and pretrain a GPT-like LLM capable of generating new texts. We will also go\\nover the fundamentals of evaluating LLMs, which is essential for developing capable\\nNLP systems. \\n Pretraining an LLM from scratch is a significant endeavor, demanding thousands\\nto millions of dollars in computing costs for GPT-like models. Therefore, the focus of\\nstage 2 is on implementing training for educational purposes using a small dataset. In\\naddition, I also provide code examples for loading openly available model weights.\\n Finally, in stage 3, we will take a pretrained LLM and fine-tune it to follow instruc-\\ntions such as answering queries or classi fying texts—the most common tasks in many\\nreal-world applications and research.\\n I hope you are looking forward to embarking on this exciting journey!\\nSummary\\n\\uf0a1 LLMs have transformed the field of natu ral language processing, which previ-\\nously mostly relied on explicit rule-bas ed systems and simpler statistical meth-\\nods. The advent of LLMs introduced new deep learning-driven approaches\\nthat led to advancements in understanding, generating, and translating human\\nlanguage.\\n\\uf0a1 Modern LLMs are trained in two main steps: \\n– First, they are pretrained on a large corpus of unlabeled text by using the\\nprediction of the next word in a sentence as a label.\\n– Then, they are fine-tuned on a smaller, labeled target dataset to follow\\ninstructions or perform classification tasks.\\n\\uf0a1 LLMs are based on the transformer arch itecture. The key idea of the trans-\\nformer architecture is an attention mechanism that gives the LLM selective\\naccess to the whole input sequence wh en generating the output one word at\\nat i m e .\\n\\uf0a1 The original transformer ar chitecture consists of an encoder for parsing text\\nand a decoder for generating text. \\n\\uf0a1 LLMs for generating text and followin g instructions, such as GPT-3 and\\nChatGPT, only implement decoder modules, simplifying the architecture.\\n\\uf0a1 Large datasets consisting of billions of words are essential for pretraining\\nLLMs.\\n\\uf0a1 While the general pretraining task for GP T-like models is to predict the next\\nword in a sentence, these LLMs exhibit emergent properties, such as capabili-\\nties to classify, translate, or summarize texts.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 37, 'page_label': '16'}, page_content='16 CHAPTER 1 Understanding large language models\\n\\uf0a1 Once an LLM is pretrained, the result ing foundation model can be fine-tuned\\nmore efficiently for various downstream tasks. \\n\\uf0a1 LLMs fine-tuned on custom  datasets can outperform  general LLMs on specific\\ntasks.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 38, 'page_label': '17'}, page_content='17\\nWorking with text data\\nSo far, we’ve covered the general structur e of large language models (LLMs) and\\nlearned that they are pretrained on vast amounts of text. Specifically, our focus was on\\ndecoder-only LLMs based on the transformer architecture, which underlies the mod-\\nels used in ChatGPT and other popular GPT-like LLMs.\\n During the pretraining stage, LLMs pr ocess text one word at a time. Training\\nLLMs with millions to billions of parame ters using a next-word prediction task\\nyields models with impressive capabilities. These models can then be further fine-\\ntuned to follow general instructions or pe rform specific target tasks. But before we\\ncan implement and train LLMs, we need to prepare the training dataset, as illus-\\ntrated in figure 2.1.\\nThis chapter covers\\n\\uf0a1 Preparing text for large language model training\\n\\uf0a1 Splitting text into word and subword tokens\\n\\uf0a1 Byte pair encoding as a more advanced way of \\ntokenizing text\\n\\uf0a1 Sampling training examples with a sliding window \\napproach\\n\\uf0a1 Converting tokens into vectors that feed into a \\nlarge language model\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 39, 'page_label': '18'}, page_content='18 CHAPTER 2 Working with text data\\nYou’ll learn how to prepare input text for tr aining LLMs. This involves splitting text\\ninto individual word and subword tokens, which can then be encoded into vector rep-\\nresentations for the LLM. You’ll also learn about advanced tokenization schemes like\\nbyte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll imple-\\nment a sampling and data-loading strategy to produce the input-output pairs neces-\\nsary for training LLMs.\\n2.1 Understanding word embeddings\\nDeep neural network models, including LLMs, cannot process raw text directly. Since\\ntext is categorical, it isn’t compatible with the mathematical operations used to imple-\\nment and train neural networks. Therefor e, we need a way to represent words as\\ncontinuous-valued vectors. \\nNOTE Readers unfamiliar with  vectors and tensors in  a computational con-\\ntext can learn more in appendix A, section A.2.2.\\nThe concept of converting data into a vector format is often referred to as embedding.\\nUsing a specific neural network layer or another pretrained neural network model, we\\ncan embed different data types—for example, video, audio, and text, as illustrated in\\nfigure 2.2. However, it’s important to note that different data formats require distinct\\nembedding models. For example, an embedding model designed for text would not\\nbe suitable for embedding audio or video data.\\nImplements the data\\nsampling pipeline\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁnetuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 2.1 The three main stages of coding an LLM. Thi s chapter focuses on step 1 of stage 1: implementing the \\ndata sample pipeline.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 40, 'page_label': '19'}, page_content='192.1 Understanding word embeddings\\nAt its core, an embedding is a mapping from discrete objects, such as words, images,\\nor even entire documents, to points in a continuous vector space—the primary pur-\\npose of embeddings is to convert nonnumeric data into a format that neural networks\\ncan process.\\n While word embeddings are the most co mmon form of text embedding, there are\\nalso embeddings for sentences, paragraphs , or whole documents. Sentence or para-\\ngraph embeddings are popular choices for  retrieval-augmented generation.  Retrieval-\\naugmented generation combines generation (like producing text) with retrieval (like\\nsearching an external knowledge base) to pull relevant information when generating\\ntext, which is a technique that  is beyond the scope of this  book. Since our goal is to\\ntrain GPT-like LLMs, which learn to generate text one word at a time, we will focus on\\nword embeddings.\\n Several algorithms and frameworks have been developed to generate word embed-\\ndings. One of the earlier and most popular examples is the Word2Vec approach.\\nWord2Vec trained neural netw ork architecture to generate word embeddings by pre-\\ndicting the context of a word given the ta r g e t  w o r d  o r  v i c e  v e r s a .  T h e  m a i n  i d e a\\nbehind Word2Vec is that word s that appear in similar cont exts tend to have similar\\nmeanings. Consequently, when projected in to two-dimensional word embeddings for\\nvisualization purposes, similar terms are clustered together, as shown in figure 2.3.\\n Word embeddings can have varying dime nsions, from one to thousands. A higher\\ndimensionality might capture more nuanced relationships but at the cost of computa-\\ntional efficiency.\\nVideo\\nsample\\nAudio\\nsample\\nText\\nsample\\nVideo embedding model\\nAudio embedding model\\nText embedding model\\nVideo embedding vector\\nAudio embedding vector\\nText embedding vector\\n1.23 -0.31 0.89\\n-0.15 0.45 2.11\\n1.78 0.18 -2.10\\nEmbedding model converts raw\\ninput into a vector representation\\nUnlabeled\\ninput data\\nVector representation\\nof the input\\nFigure 2.2 Deep learning models cannot process data formats like video, audio, and text in their raw \\nform. Thus, we use an embedding model to transform this raw data into a dense vector representation \\nthat deep learning architectures can easily understand and process. Specifically, this figure illustrates \\nthe process of converting raw data into a three-dimensional numerical vector.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 41, 'page_label': '20'}, page_content='20 CHAPTER 2 Working with text data\\nWhile we can use pretrained models such as Word2Vec to generate embeddings for\\nmachine learning models, LLMs commonly produce their own embeddings that are\\npart of the input layer and are updated du ring training. The advantage of optimizing\\nthe embeddings as part of the LLM training  instead of using Word2Vec is that the\\nembeddings are optimized to the specific ta sk and data at hand. We will implement\\nsuch embedding layers later in this chapter. (LLMs can also create contextualized out-\\nput embeddings, as we discuss in chapter 3.)\\n Unfortunately, high-dimensional embedd ings present a challenge for visualiza-\\ntion because our sensory perception and common graphical representations are\\ninherently limited to three dimensions or fewer, which is why figure 2.3 shows two-\\ndimensional embeddings in a two-dimensio nal scatterplot. However, when working\\nwith LLMs, we typically us e embeddings with a much hi gher dimensionality. For\\nboth GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality\\nof the model’s hidden states) varies based on the specific model variant and size. It\\nis a tradeoff between performance and effi ciency. The smallest GPT-2 models (117M\\nand 125M parameters) use an embedding si ze of 768 dimensions to provide con-\\ncrete examples. The largest GPT-3 model (175B parameters) uses an embedding\\nsize of 12,288 dimensions. \\n Next, we will walk through the requir ed steps for preparing the embeddings used\\nby an LLM, which include splitting text into words, converting words into tokens, and\\nturning tokens into embedding vectors.\\nFirst dimension\\nSecond\\ndimension\\neagle\\nduck\\ngoose\\nsquirrel\\nlong\\nlonger\\nlongest\\nGermany Berlin\\nEngland London\\nVector embedding of\\nthe word squirrel\\nVector embeddings of\\ndifferent types of birds\\nFigure 2.3 If word embeddings are two-dimensional, we can plot them in a two-\\ndimensional scatterplot for visualization purposes as shown here. When using word \\nembedding techniques, such as Word2Vec, words corresponding to similar concepts \\noften appear close to each other in the embedding space. For instance, different types \\nof birds appear closer to each other in the embedding space than in countries and cities.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 42, 'page_label': '21'}, page_content='212.2 Tokenizing text\\n2.2 Tokenizing text\\nLet’s discuss how we split input text into individual to kens, a required preprocessing\\nstep for creating embeddings for an LLM. These tokens are either individual words or\\nspecial characters, including punctuation characters, as shown in figure 2.4.\\nT h e  t e x t  w e  w i l l  t o k e n i z e  f o r  L L M  t r a i n i n g  i s  “ T h e  V e r d i c t , ”  a  s h o r t  s t o r y  b y  E d i t h\\nWharton, which has been released into the public domain and is thus permitted to be\\nused for LLM training tasks. The text is available on Wikisource at https:/ /en.wikisource\\n.org/wiki/The_Verdict, and you can copy and paste it into a text file, which I copied\\ninto a text file \"the-verdict.txt\".\\n Alternatively, you can find this \"the-verdict.txt\" file in this book’s GitHub\\nrepository at https:/ /mng.bz/Adng. You can download the file with the following\\nPython code:\\n \\n \\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text: This is an example\\nOutput text\\nPostprocessing steps\\nToken IDs: 40134 2052 133 389\\n.\\n12\\nThis section covers the\\nconcept of splitting\\ntext into tokens\\nFigure 2.4 A view of the text processing steps in the context of an LLM. Here, we split an \\ninput text into individual tokens, which are either words or special characters, such as \\npunctuation characters.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 43, 'page_label': '22'}, page_content='22 CHAPTER 2 Working with text data\\nimport urllib.request\\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\\n       \"the-verdict.txt\")\\nfile_path = \"the-verdict.txt\"\\nurllib.request.urlretrieve(url, file_path)\\nNext, we can load the the-verdict.txt file using Python’s standard file reading utilities. \\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nprint(\"Total number of character:\", len(raw_text))\\nprint(raw_text[:99])\\nThe print command prints the total number of characters followed by the first 100\\ncharacters of this file for illustration purposes:\\nTotal number of character: 20479\\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow \\nenough--so it was no \\nOur goal is to tokenize this 20,479-character short story into individual words and spe-\\ncial characters that we can then turn into embeddings for LLM training.\\nNOTE It’s common to process millions of articles and hundreds of thousands\\nof books—many gigabytes of text—whe n working with LLMs. However, for\\neducational purposes, it’s sufficient to work with smaller text samples like a\\nsingle book to illustrate the main idea s behind the text pr ocessing steps and\\nto make it possible to run it in a reasonable time on consumer hardware.\\nHow can we best split this text to obtain a list of tokens? For this, we go on a small\\nexcursion and use Python’s regular expression library re for illustration purposes.\\n(You don’t have to learn or memorize any regular expression syntax since we will later\\ntransition to a prebuilt tokenizer.)\\n Using some simple example text, we can use the re.split command with the fol-\\nlowing syntax to split a text on whitespace characters:\\nimport re\\ntext = \"Hello, world. This, is a test.\"\\nresult = re.split(r\\'(\\\\s)\\', text)\\nprint(result)\\nThe result is a list of individual words, whitespaces, and punctuation characters:\\n[\\'Hello,\\', \\' \\', \\'world.\\', \\' \\', \\'This,\\', \\' \\', \\'is\\', \\' \\', \\'a\\', \\' \\', \\'test.\\']\\nListing 2.1 Reading in a short story as text sample into Python\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 44, 'page_label': '23'}, page_content='232.2 Tokenizing text\\nThis simple tokenization sc heme mostly works for separa ting the example text into\\nindividual words; however, so me words are still connected  to punctuation characters\\nthat we want to have as separate list entries. We also refrain from making all text lower-\\ncase because capitalization helps LLMs di stinguish between proper nouns and com-\\nmon nouns, understand sentence structure,  and learn to generate text with proper\\ncapitalization.\\n Let’s modify the regular expression splits on whitespaces ( \\\\s), commas, and peri-\\nods ([,.]):\\nresult = re.split(r\\'([,.]|\\\\s)\\', text)\\nprint(result)\\nWe can see that the words and punctuation characters are now separate list entries just\\nas we wanted:\\n[\\'Hello\\', \\',\\', \\'\\', \\' \\', \\'world\\', \\'.\\', \\'\\', \\' \\', \\'This\\', \\',\\', \\'\\', \\' \\', \\'is\\',\\n\\' \\', \\'a\\', \\' \\', \\'test\\', \\'.\\', \\'\\']\\nA small remaining problem is that the list still includes whitespace characters. Option-\\nally, we can remove these redundant characters safely as follows:\\nresult = [item for item in result if item.strip()]\\nprint(result)\\nThe resulting whitespace-free output looks like as follows:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'This\\', \\',\\', \\'is\\', \\'a\\', \\'test\\', \\'.\\']\\nNOTE When developing a simple tokeni zer, whether we should encode\\nwhitespaces as separate characters or just remove them depends on our appli-\\ncation and its requirements. Removing  whitespaces reduces the memory and\\ncomputing requirements. However, keep ing whitespaces can be useful if we\\ntrain models that are sensitive to the ex act structure of the text (for example,\\nPython code, which is sensitive to indentation and spacing). Here, we remove\\nwhitespaces for simplicity and brevity of the tokenized outputs. Later, we will\\nswitch to a tokenization scheme that includes whitespaces.\\nThe tokenization scheme we devised here wo rks well on the simple  sample text. Let’s\\nmodify it a bit further so that it can also handle other types of punctuation, such as ques-\\ntion marks, quotation marks, and the double-dashes we have seen earlier in the first 100\\ncharacters of Edith Wharton’s short story, along with additional special characters:\\ntext = \"Hello, world. Is this-- a test?\"\\nresult = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\nresult = [item.strip() for item in result if item.strip()]\\nprint(result)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 45, 'page_label': '24'}, page_content='24 CHAPTER 2 Working with text data\\nThe resulting output is:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'Is\\', \\'this\\', \\'--\\', \\'a\\', \\'test\\', \\'?\\']\\nAs we can see based on the results summarized in figure 2.5, our tokenization scheme\\ncan now handle the various special characters in the text successfully.\\nNow that we have a basic to kenizer working, let’s apply it to Edith Wharton’s entire\\nshort story:\\npreprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', raw_text)\\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\\nprint(len(preprocessed))\\nThis print statement outputs 4690, which is the number of tokens in this text (without\\nwhitespaces). Let’s print the first 30 tokens for a quick visual check:\\nprint(preprocessed[:30])\\nThe resulting output shows that our tokeni zer appears to be ha ndling the text well\\nsince all words and special characters are neatly separated:\\n[\\'I\\', \\'HAD\\', \\'always\\', \\'thought\\', \\'Jack\\', \\'Gisburn\\', \\'rather\\', \\'a\\',\\n\\'cheap\\', \\'genius\\', \\'--\\', \\'though\\', \\'a\\', \\'good\\', \\'fellow\\', \\'enough\\',\\n\\'--\\', \\'so\\', \\'it\\', \\'was\\', \\'no\\', \\'great\\', \\'surprise\\', \\'to\\', \\'me\\', \\'to\\',\\n\\'hear\\', \\'that\\', \\',\\', \\'in\\']\\n2.3 Converting tokens into token IDs\\nNext, let’s convert these tokens from a Pyth on string to an integer representation to\\nproduce the token IDs. This conversion is an  intermediate step before converting the\\ntoken IDs into embedding vectors.\\n To map the previously generated tokens into token IDs, we have to build a vocabu-\\nlary first. This vocabulary defines how we map each unique word and special character\\nto a unique integer, as shown in figure 2.6.\\nInput text Hello, world. Is this-- a test?\\nHello , world . Is this -- a test ?Tokenized text\\nFigure 2.5 The tokenization scheme we implemented so far splits \\ntext into individual words and punctuation characters. In this specific \\nexample, the sample text gets split into 10 individual tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 46, 'page_label': '25'}, page_content='252.3 Converting tokens into token IDs\\nNow that we have tokenized Edith Wharton’s short story and assigned it to a Python\\nvariable called preprocessed, let’s create a list of all unique tokens and sort them\\nalphabetically to determine the vocabulary size:\\nall_words = sorted(set(preprocessed))\\nvocab_size = len(all_words)\\nprint(vocab_size)\\nAfter determining that the vocabulary size is 1,130 via this code, we create the vocabu-\\nlary and print its first 51 entries for illustration purposes.\\nvocab = {token:integer for integer,token in enumerate(all_words)}\\nfor i, item in enumerate(vocab.items()):\\n    print(item)\\n    if i >= 50:\\n        break\\nListing 2.2 Creating a vocabulary\\nComplete training dataset\\nThe quick brown fox jumps\\nover the lazy dog\\nTokenized training dataset\\nThe quick brown\\nVocabulary\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n1. Tokenization breaks\\ndown the input text\\ninto individual tokens.\\n2. Each unique token is\\nadded to the vocabulary\\nin alphabetical order.\\njumps\\nlazy\\nover\\n3\\n4\\n5\\nquick 6\\nthe 7\\nThe vocabulary\\ncontains all unique\\ntokens in the training\\nset and is usually\\nsorted alphabetically.\\nEach unique token is\\nmapped to a unique\\ninteger called token ID.\\nUnique tokens\\nToken IDs\\nThe training set consists\\nof only one sentence for\\nillustration purposes.\\nInput text\\nFigure 2.6 We build a vocabulary by tokenizing the ent ire text in a training dataset into individual \\ntokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. \\nThe unique tokens are then aggregated into a vocabulary that defines a mapping from each unique \\ntoken to a unique integer value. The depicted vocabulary is purposefully small and contains no \\npunctuation or special characters for simplicity.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 47, 'page_label': '26'}, page_content='26 CHAPTER 2 Working with text data\\nThe output is\\n(\\'!\\', 0)\\n(\\'\"\\', 1)\\n(\"\\'\", 2)\\n...\\n(\\'Her\\', 49)\\n(\\'Hermia\\', 50)\\nAs we can see, the dictionary contains individual tokens associated with unique inte-\\nger labels. Our next goal is to apply this vocabulary to convert new text into token IDs\\n(figure 2.7).\\nWhen we want to convert the outputs of an LLM from numbers back into text, we need a\\nway to turn token IDs into text. For this, we can create an inverse version of the vocabu-\\nlary that maps token IDs back to the corresponding text tokens. \\nTokenization breaks down the\\ntraining set into individual tokens.\\nNew tokenized sample\\ntext is mapped to\\ntoken IDs using an\\nexisting vocabulary.\\nFigure 2.7 Starting with a new text sample, we tokenize the text and use the vocabulary to convert \\nthe text tokens into token IDs. The vocabulary is built from the entire training set and can be applied \\nto the training set itself and any new text samples. The depicted vocabulary contains no punctuation \\nor special characters for simplicity.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 48, 'page_label': '27'}, page_content='272.3 Converting tokens into token IDs\\n Let’s implement a complete tokenizer class in Python with an encode method that\\nsplits text into tokens and carries out the string-to-integer mapping to produce token\\nIDs via the vocabulary. In addition, we’ll implement a decode method that carries out\\nthe reverse integer-to-string mapping to convert the token IDs back into text. The fol-\\nlowing listing shows the code for this tokenizer implementation.\\nclass SimpleTokenizerV1:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab           \\n        self.int_to_str = {i:s for s,i in vocab.items()}       \\n    \\n    def encode(self, text):        \\n        preprocessed = re.split(r\\'([,.?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):        \\n        text = \" \".join([self.int_to_str[i] for i in ids]) \\n        \\n        text = re.sub(r\\'\\\\s+([,.?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nUsing the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer\\nobjects via an existing vocabulary, which we  can then use to encode and decode text,\\nas illustrated in figure 2.8.\\n Let’s instantiate a new tokenizer object from the SimpleTokenizerV1 class and\\ntokenize a passage from Edith Wharton’s short story to try it out in practice:\\ntokenizer = SimpleTokenizerV1(vocab)\\ntext = \"\"\"\"It\\'s the last he painted, you know,\" \\n       Mrs. Gisburn said with pardonable pride.\"\"\"\\nids = tokenizer.encode(text)\\nprint(ids)\\nThe preceding code prints the following token IDs:\\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, \\n754, 793, 7]\\nNext, let’s see whether we can turn these token IDs back into text using the decode\\nmethod:\\nprint(tokenizer.decode(ids))\\nListing 2.3 Implementing a simple text tokenizer\\nStores the vocabulary as a class attribute for\\naccess in the encode and decode methods\\nCreates an inverse\\nvocabulary that maps\\ntoken IDs back to the\\noriginal text tokens\\nProcesses \\ninput text \\ninto token \\nIDs\\nConverts token IDs \\nback into text\\nRemoves spaces \\nbefore the specified \\npunctuation\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 49, 'page_label': '28'}, page_content='28 CHAPTER 2 Working with text data\\nThis outputs:\\n\\'\" It\\\\\\' s the last he painted, you know,\" Mrs. Gisburn said with \\npardonable pride.\\'\\nBased on this output, we can see that the decode method successfully converted the\\ntoken IDs back into the original text.\\n So far, so good. We implemented a tokenizer capable of tokenizing and detokeniz-\\ning text based on a snippet from the training set. Let’s now apply it to a new text sam-\\nple not contained in the training set:\\ntext = \"Hello, do you like tea?\"\\nprint(tokenizer.encode(text))\\nExecuting this code will result in the following error:\\nKeyError: \\'Hello\\'\\nThe problem is that the word “Hello” was no t used in the “The Verdict” short story.\\nHence, it is not contained in the vocabula ry. This highlights the need to consider\\nlarge and diverse training sets to extend the vocabulary when working on LLMs.\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample text Tokenized sample text\\nThe brown dog\\nToken IDs\\n7 0 1\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample textTokenized sample text\\nThe brown dog\\nToken IDs\\n7 0 1\\nCalling tokenizer.encode(text)on sample text\\nCalling tokenizer.decode(ids)on token IDs\\nVocabulary\\nInverse\\nvocabulary\\nFigure 2.8 Tokenizer implementations share two common methods: an encode method and a decode \\nmethod. The encode method takes in the sample text, splits it into individual tokens, and converts the \\ntokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back \\ninto text tokens, and concatenates the text tokens into natural text.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 50, 'page_label': '29'}, page_content='292.4 Adding special context tokens\\n Next, we will test the tokenizer further on text that contains unknown words and\\ndiscuss additional special to kens that can be used to provide further context for an\\nLLM during training.\\n2.4 Adding special context tokens\\nWe need to modify the tokenizer to handle  unknown words. We also need to address\\nthe usage and addition of special context tokens that can enhance a model’s under-\\nstanding of context or other relevant in formation in the text. These special tokens\\ncan include markers for unknown words an d document boundaries, for example. In\\nparticular, we will modify th e vocabulary and tokenizer, SimpleTokenizerV2, to sup-\\nport two new tokens, <|unk|> and <|endoftext|>, as illustrated in figure 2.9.\\nWe can modify the tokenizer to use an <|unk|> token if it encounters a word that is\\nnot part of the vocabulary. Furthermore,  we add a token between unrelated texts.\\nFor example, when training  GPT-like LLMs on multiple independent documents or\\nbooks, it is common to insert a token before each document or book that follows a\\nprevious text source, as illustrated in figure 2.10. This helps the LLM understand\\nthat although these text sources are concat enated for training, they are, in fact,\\nunrelated.\\nSample text Tokenized sample text\\nThe brown dog\\nToken IDs\\nplayfully\\n7 0 1 783\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n<|unk|>\\n<|endoftext|>\\n783\\n784\\nExisting vocabulary\\nExtend vocabulary\\nwith additional\\nspecial tokens\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nFigure 2.9 We add special tokens to a vocabulary to deal with certain contexts. For instance, \\nwe add an <|unk|> token to represent new and unknown words that were not part of the training \\ndata and thus not part of the existing vocabulary. Furthermore, we add an <|endoftext|> \\ntoken that we can use to separate two unrelated text sources. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 51, 'page_label': '30'}, page_content='30 CHAPTER 2 Working with text data\\nLet’s now modify the vocabulary to include these two special tokens, <unk> and\\n<|endoftext|>, by adding them to our list of all unique words:\\nall_tokens = sorted(list(set(preprocessed)))\\nall_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\\nprint(len(vocab.items()))\\nBased on the output of this print statement,  the new vocabulary size is 1,132 (the pre-\\nvious vocabulary size was 1,130).\\n As an additional quick check, let’s print the last five entries of the updated vocabulary:\\nfor i, item in enumerate(list(vocab.items())[-5:]):\\n    print(item)\\nThe code prints\\n(\\'younger\\', 1127)\\n(\\'your\\', 1128)\\n(\\'yourself\\', 1129)\\n(\\'<|endoftext|>\\', 1130)\\n(\\'<|unk|>\\', 1131)\\nThe <|endoftext|> tokens are\\nprepended to each subsequent text\\nsource.\\nIndependent text source\\nText concatenated from all\\nindependent sources\\n“… the underdog\\nteam ﬁnally clinched\\nthe championship in\\na thrilling overtime\\nvictory.”\\n“…<|endoftext|>\\nElara and Finn lived\\nwith kindness and\\nwisdom, enjoying\\ntheir days happily\\never after.”\\n“…<|endoftext|>\\nThe Dow Jones\\nIndustrial Average\\nclosed up 250 points\\ntoday, marking its\\nhighest gain in the\\npast three months.”\\n“…<|endoftext|>\\nAmelia smiled,\\nknowing her journey\\nhad forever changed\\nher heart.”\\n“… in a thrilling overtime victory. … days happily ever after.<|endoftext|> <|endoftext|>\\n… marking its highest gain in the past three months. … journey had forever<|endoftext|>\\nchanged her heart.”\\nFigure 2.10 When working with multiple independent text source, we add <|endoftext|> \\ntokens between these texts. These <|endoftext|> tokens act as markers, signaling the \\nstart or end of a particular segment, allowing for more effective processing and understanding \\nby the LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 52, 'page_label': '31'}, page_content='312.4 Adding special context tokens\\nBased on the code output, we can confirm that the two new special tokens were\\nindeed successfully incorporated into the vocabulary. Next, we adjust the tokenizer\\nfrom code listing 2.3 accordingly as shown in the following listing.\\nclass SimpleTokenizerV2:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab\\n        self.int_to_str = { i:s for s,i in vocab.items()}\\n    \\n    def encode(self, text):\\n        preprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        preprocessed = [item if item in self.str_to_int           \\n                        else \"<|unk|>\" for item in preprocessed]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):\\n        text = \" \".join([self.int_to_str[i] for i in ids])\\n        text = re.sub(r\\'\\\\s+([,.:;?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nCompared to the SimpleTokenizerV1 we implemented in listing 2.3, the new Simple-\\nTokenizerV2 replaces unknown words with <|unk|> tokens. \\n Let’s now try this new tokenizer out in practice. For this, we will use a simple text\\nsample that we concatenate from two independent and unrelated sentences:\\ntext1 = \"Hello, do you like tea?\"\\ntext2 = \"In the sunlit terraces of the palace.\"\\ntext = \" <|endoftext|> \".join((text1, text2))\\nprint(text)\\nThe output is\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe palace.\\nNext, let’s tokenize the sample text using the SimpleTokenizerV2 on the vocab we\\npreviously created in listing 2.2:\\ntokenizer = SimpleTokenizerV2(vocab)\\nprint(tokenizer.encode(text))\\nListing 2.4 A simple text tokenizer that handles unknown words\\nReplaces\\nunknown words\\nby <|unk|>\\ntokens\\nReplaces spaces \\nbefore the specified \\npunctuations\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 53, 'page_label': '32'}, page_content='32 CHAPTER 2 Working with text data\\nThis prints the following token IDs:\\n[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\\nWe can see that the list of token IDs contains 1130 for the <|endoftext|> separator\\ntoken as well as two 1131 tokens, which are used for unknown words. \\n Let’s detokenize the text for a quick sanity check:\\nprint(tokenizer.decode(tokenizer.encode(text)))\\nThe output is\\n<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe <|unk|>.\\nBased on comparing this detokenized text wi th the original inpu t text, we know that\\nthe training dataset, Edith Wharton’s short story “The Verdict,” does not contain the\\nwords “Hello” and “palace.”\\n Depending on the LLM, some researchers also consider additional special tokens\\nsuch as the following:\\n\\uf0a1 [BOS] (beginning of sequence) —This token marks the start of a text. It signifies to\\nthe LLM where a piece of content begins.\\n\\uf0a1 [EOS] (end of sequence) —This token is positioned at the end of a text and\\nis especially useful when concatenating multiple unrelated texts, similar to\\n<|endoftext|>. For instance, when combining two different Wikipedia arti-\\ncles or books, the [EOS] token indicates where one ends and the next begins.\\n\\uf0a1 [PAD] (padding)—When training LLMs with batc h sizes larger than one, the\\nbatch might contain texts of varying leng ths. To ensure all texts have the same\\nlength, the shorter texts are extended or “padded” using the [PAD] token, up to\\nthe length of the longest text in the batch.\\nThe tokenizer used for GPT models does not need any of these tokens; it only uses an\\n<|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token.\\n<|endoftext|> i s  a l s o  u s e d  f o r  p a d d i n g .  H o w e v e r ,  a s  w e ’ l l  e x p l o r e  i n  s u b s e q u e n t\\nchapters, when training on batched inputs, we typically use a mask, meaning we don’t\\nattend to padded tokens. Thus, the specific token chosen for padding becomes incon-\\nsequential.\\n Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token\\nfor out-of-vocabulary words. Instead, GPT models use a byte pair encoding  tokenizer,\\nwhich breaks words down into subword units, which we will discuss next.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 54, 'page_label': '33'}, page_content='332.5 Byte pair encoding\\n2.5 Byte pair encoding\\nLet’s look at a more sophisticated tokenization scheme based on a concept called byte\\npair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,\\nand the original model used in ChatGPT.\\n Since implementing BPE can be relatively  complicated, we will use an existing\\nPython open source library called tiktoken (https:/ /github.com/openai/tiktoken), which\\nimplements the BPE algorithm very efficientl y based on source code in Rust. Similar\\nto other Python libraries, we can inst all the tiktoken library via Python’s \\npip installer\\nfrom the terminal:\\npip install tiktoken\\nThe code we will use is based on tiktoken 0.7.0. You can use the following code to\\ncheck the version you currently have installed:\\nfrom importlib.metadata import version\\nimport tiktoken\\nprint(\"tiktoken version:\", version(\"tiktoken\"))\\nOnce installed, we can instantiate the BPE tokenizer from tiktoken as follows:\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nThe usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented pre-\\nviously via an encode method:\\ntext = (\\n    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\\n     \"of someunknownPlace.\"\\n)\\nintegers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\\nprint(integers)\\nThe code prints the following token IDs: \\n[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,\\n 8812, 2114, 286, 617, 34680, 27271, 13]\\nWe can then convert the token IDs back into text using the decode method, similar to\\nour SimpleTokenizerV2:\\nstrings = tokenizer.decode(integers)\\nprint(strings)\\nThe code prints\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of\\n someunknownPlace.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 55, 'page_label': '34'}, page_content='34 CHAPTER 2 Working with text data\\nWe can make two noteworthy observations based on the token IDs and decoded text.\\nFirst, the <|endoftext|> token is assigned a relati vely large token ID, namely, 50256.\\nIn fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and\\nthe original model used in ChatGPT, has a total vocabulary size of 50,257, with\\n<|endoftext|> being assigned the largest token ID.\\n Second, the BPE tokenizer encodes and decodes unknown words, such as\\nsomeunknownPlace, correctly. The BPE tokenizer can handle any unknown word. How\\ndoes it achieve this without using <|unk|> tokens?\\n The algorithm underlying BPE breaks do wn words that aren’t in its predefined\\nvocabulary into smaller subword units or even individual characters, enabling it to\\nhandle out-of-vocabulary words. So, thanks  to the BPE algorithm, if the tokenizer\\nencounters an unfamiliar word during toke nization, it can represent it as a sequence\\nof subword tokens or characters, as illustrated in figure 2.11.\\nThe ability to break down unknown words into individual characters ensures that\\nthe tokenizer and, consequently, the LLM that  is trained with it can process any text,\\neven if it contains words that were  not present in its training data.\\nA detailed discussion and implementation of BPE is out of the scope of this book, but\\nin short, it builds its vocabulary by iter atively merging frequent  characters into sub-\\nwords and frequent subwords into words. For example, BPE starts with adding all indi-\\nvidual single characters to its vocabulary (“a, ” “b,” etc.). In the next stage, it merges\\ncharacter combinations that frequently oc cur together into su bwords. For example,\\n“d” and “e” may be merged into the subword “de,” which is common in many English\\nExercise 2.1 Byte pair encoding of unknown words \\nTry the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\\nprint the individual token IDs. Then, call the decode function on each of the resulting\\nintegers in this list to reproduce the mapp ing shown in figure 2.11. Lastly, call the\\ndecode method on the token IDs to check whether it can reconstruct the original\\ninput, “Akwirw ier.”\\n\"Akwirw ier\"\\n\"Ak\" \"w\" \"ir\" \"w\" \" \" \"ier\"\\n33901 86 343 86 220 959\\nText sample with\\nunknown words\\nUnknown words are\\ntokenized into individual\\ncharacters or subwords.\\nTokens:\\nToken IDs:\\nFigure 2.11 BPE tokenizers \\nbreak down unknown words \\ninto subwords and individual \\ncharacters. This way, a BPE \\ntokenizer can parse any word \\nand doesn’t need to replace \\nunknown words with special \\ntokens, such as \\n<|unk|>.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 56, 'page_label': '35'}, page_content='352.6 Data sampling with a sliding window\\nwords like “define,” “depend,” “made,” and “hidden.” The merges are determined by\\na frequency cutoff.\\n2.6 Data sampling with a sliding window\\nThe next step in creating the embeddings for the LLM is to generate the input–target\\npairs required for training an LLM. What do these input–target pairs look like? As we\\nalready learned, LLMs are pretrained by predicting the next word in a text, as depicted\\nin figure 2.12.\\nLet’s implement a data loader that fetches the input–target pairs in figure 2.12 from\\nthe training dataset using a sliding window approach. To get started, we will tokenize\\nthe whole “The Verdict” short story using the BPE tokenizer:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nenc_text = tokenizer.encode(raw_text)\\nprint(len(enc_text))\\nExecuting this code will return 5145, the total number of tokens in the training set,\\nafter applying the BPE tokenizer.\\n Next, we remove the first 50 tokens from the dataset for demonstration purposes,\\nas it results in a slightly more interesting text passage in the next steps:\\nenc_sample = enc_text[50:]\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nInput the\\nLLM receives\\nTarget to\\npredict\\nText\\nsample: LLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   timeLLMs   learn   to   predict   one   word   at   a   time\\nThe LLM can’t\\naccess words past\\nthe target.\\nFigure 2.12 Given a text sample, extract i nput blocks as subsamples that serve as \\ninput to the LLM, and the LLM’s prediction task during training is to predict the next \\nword that follows the input block. During training, we mask out all words that are past \\nthe target. Note that the text shown in this figure must undergo tokenization before \\nthe LLM can process it; however, this figure omits the tokenization step for clarity.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 57, 'page_label': '36'}, page_content='36 CHAPTER 2 Working with text data\\nOne of the easiest and most intuitive ways to create the input–target pairs for the next-\\nword prediction task is to create two variables, x and y, where x contains the input\\ntokens and y contains the targets, which are the inputs shifted by 1:\\ncontext_size = 4        \\nx = enc_sample[:context_size]\\ny = enc_sample[1:context_size+1]\\nprint(f\"x: {x}\")\\nprint(f\"y:      {y}\")\\nRunning the previous code prints the following output:\\nx: [290, 4920, 2241, 287]\\ny:      [4920, 2241, 287, 257]\\nBy processing the inputs along with the ta rgets, which are the inputs shifted by one\\nposition, we can create the next-word prediction tasks (see figure 2.12), as follows:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(context, \"---->\", desired)\\nThe code prints\\n[290] ----> 4920\\n[290, 4920] ----> 2241\\n[290, 4920, 2241] ----> 287\\n[290, 4920, 2241, 287] ----> 257\\nEverything left of the arrow ( ---->) refers to the input an LLM would receive, and\\nthe token ID on the right side of the arro w represents the target token ID that the\\nLLM is supposed to predict. Let’s repeat the previous code but convert the token IDs\\ninto text:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\\nThe following outputs show how the input and outputs look in text format:\\n and ---->  established\\n and established ---->  himself\\n and established himself ---->  in\\n and established himself in ---->  a\\nWe’ve now created the input–target pairs that we can use for LLM training.\\n There’s only one more task before we can turn the tokens into embeddings: imple-\\nmenting an efficient data loader that iter ates over the input dataset and returns the\\nThe context size determines \\nhow many tokens are included \\nin the input.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 58, 'page_label': '37'}, page_content='372.6 Data sampling with a sliding window\\ninputs and targets as PyTorch tensors, whic h can be thought of as multidimensional\\narrays. In particular, we are interested in returning two tensors: an input tensor con-\\ntaining the text that the LLM sees and a targ et tensor that includes the targets for the\\nLLM to predict, as depicted in figure 2.13. While the figure shows the tokens in string\\nformat for illustration purposes, the code implementation will operate on token IDs\\ndirectly since the encode method of the BPE tokenizer performs both tokenization\\nand conversion into token IDs as a single step.\\nNOTE For the efficient data loader implementation, we will use PyTorch’s\\nbuilt-in Dataset and DataLoader classes. For additi onal information and\\nguidance on installing PyTorch, please see section A.2.1.3 in appendix A.\\nThe code for the dataset class is shown in the following listing.\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nclass GPTDatasetV1(Dataset):\\n    def __init__(self, txt, tokenizer, max_length, stride):\\n        self.input_ids = []\\n        self.target_ids = []\\n        token_ids = tokenizer.encode(txt)   \\nListing 2.5 A dataset for batched inputs and targets\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nx = tensor([[  \"In\",      \"the\",     \"heart\",  \"of\"   ],\\n[  \"the\" ,    \"city\",    \"stood\",  \"the\"  ],\\n[  \"old\",     \"library\", \",\",      \"a\"    ],\\n[ … ]])\\ny = tensor([[  \"the\",     \"heart\",   \"of\",     \"the\"  ],\\n[  \"city\",    \"stood\",   \"the\",    \"old\"  ],\\n[  \"library\", “,\", “a\", “relic\"],\\n[ … ]])\\nSample text\\nTensor\\ncontaining\\nthe inputs\\nTensor\\ncontaining\\nthe targets\\nFigure 2.13 To implement efficient data loaders, we collect the inputs in a tensor, x, where each row \\nrepresents one input context. A second tensor, y, contains the corresponding prediction targets (next \\nwords), which are created by shifting the input by one position.\\nTokenizes the \\nentire text\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 59, 'page_label': '38'}, page_content='38 CHAPTER 2 Working with text data\\n        for i in range(0, len(token_ids) - max_length, stride):    \\n            input_chunk = token_ids[i:i + max_length]\\n            target_chunk = token_ids[i + 1: i + max_length + 1]\\n            self.input_ids.append(torch.tensor(input_chunk))\\n            self.target_ids.append(torch.tensor(target_chunk))\\n    def __len__(self):   \\n        return len(self.input_ids)\\n    def __getitem__(self, idx):        \\n        return self.input_ids[idx], self.target_ids[idx]\\nThe GPTDatasetV1 class is based on the PyTorch Dataset class and defines how indi-\\nvidual rows are fetched from the dataset, where each row consists of a number of\\ntoken IDs (based on a max_length) assigned to an input_chunk tensor. The target_\\nchunk tensor contains the corresponding targets. I recommend reading on to see what\\nthe data returned from this dataset look s like when we combine the dataset with a\\nPyTorch DataLoader—this will bring addition al intuition and clarity.\\nNOTE If you are new to the structure of PyTorch Dataset classes, such as\\nshown in listing 2.5, refer to section A.6 in appendix A, which explains the\\ngeneral structure and usage of PyTorch Dataset and DataLoader classes.\\nThe following code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch\\nDataLoader.\\ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\\n                         stride=128, shuffle=True, drop_last=True,\\n                         num_workers=0):\\n    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \\n    dataloader = DataLoader(\\n        dataset,\\n        batch_size=batch_size,\\n        shuffle=shuffle,\\n        drop_last=drop_last,    \\n        num_workers=num_workers    \\n    )\\n    return dataloader\\nListing 2.6 A data loader to generate batches with input-with pairs\\nUses a sliding window to chunk\\nthe book into overlapping\\nsequences of max_length\\nReturns the total number \\nof rows in the dataset\\nReturns a single row \\nfrom the dataset\\nInitializes the \\ntokenizer\\nCreates \\ndataset\\ndrop_last=True drops the last \\nbatch if it is shorter than the \\nspecified batch_size to prevent \\nloss spikes during training.\\nThe number of CPU processes \\nto use for preprocessing\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 60, 'page_label': '39'}, page_content='392.6 Data sampling with a sliding window\\nLet’s test the dataloader with a batch size of 1 for an LLM with a context size of 4 to\\ndevelop an intuition of how the GPTDatasetV1 class from listing 2.5 and the create_\\ndataloader_v1 function from listing 2.6 work together:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\\ndata_iter = iter(dataloader)     \\nfirst_batch = next(data_iter)\\nprint(first_batch)\\nExecuting the preceding code prints the following:\\n[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\\nThe first_batch variable contains two tensors: the first tensor stores the input token\\nIDs, and the second tensor stores  the target token IDs. Since the max_length is set to\\n4, each of the two tensors contains four token IDs. Note that an input size of 4 is quite\\nsmall and only chosen for simplicity. It is common to train LLMs with input sizes of at\\nleast 256.\\n To understand the meaning of stride=1, let’s fetch another batch from this dataset:\\nsecond_batch = next(data_iter)\\nprint(second_batch)\\nThe second batch has the following contents:\\n[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\\nIf we compare the first and se cond batches, we can see th at the second batch’s token\\nIDs are shifted by one position (for example, the second ID in the first batch’s input is\\n367, which is the first ID of the second batch’s input). The stride setting dictates the\\nnumber of positions the inputs shift ac ross batches, emulating a sliding window\\napproach, as demonstrated in figure 2.14.\\nBatch sizes of 1, such as we have sampled fr om the data loader so far, are useful for\\nillustration purposes. If you have previous experience with deep learning, you may\\nknow that small batch sizes require less me mory during training but lead to more\\nExercise 2.2 Data loaders with different strides and context sizes\\nTo develop more intuition for how the data loader works, try to run it with different\\nsettings such as max_length=2 and stride=2, and max_length=8 and stride=2.\\nConverts dataloader into a Python \\niterator to fetch the next entry via \\nPython’s built-in next() function\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 61, 'page_label': '40'}, page_content='40 CHAPTER 2 Working with text data\\nnoisy model updates. Just like in regular deep learning, the batch size is a tradeoff and\\na hyperparameter to experiment with when training LLMs.\\n Let’s look briefly at how we can use the data loader to sample with a batch size\\ngreater than 1:\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=4, stride=4,\\n    shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Inputs:\\\\n\", inputs)\\nprint(\"\\\\nTargets:\\\\n\", targets)\\nThis prints\\nInputs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nSample text\\nA stride of 1 moves the input ﬁeld by 1 position\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nA stride of 4 moves the input ﬁeld by 4 positions\\n\"In the heart of\"\\n\"the heart of the\"\\nInputs of batch 1:\\nInputs of batch 2:\\n\"In the heart of\"\\n“the city stood the\"\\nInputs of batch 1:\\nInputs of batch 2:\\nFigure 2.14 When creating multiple batches from the input dataset, we slide an \\ninput window across the text. If the stride is set to 1, we shift the input window by \\none position when creating the next batch. If we set the stride equal to the input \\nwindow size, we can prevent overlaps between the batches.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 62, 'page_label': '41'}, page_content='412.7 Creating token embeddings\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nTargets:\\n tensor([[  367,  2885,  1464,  1807],\\n        [ 3619,   402,   271, 10899],\\n        [ 2138,   257,  7026, 15632],\\n        [  438,  2016,   257,   922],\\n        [ 5891,  1576,   438,   568],\\n        [  340,   373,   645,  1049],\\n        [ 5975,   284,   502,   284],\\n        [ 3285,   326,    11,   287]])\\nNote that we increase the stride to 4 to utilize the data set fully (we don’t skip a single\\nword). This avoids any overlap between th e batches since more overlap could lead to\\nincreased overfitting.\\n2.7 Creating token embeddings\\nThe last step in preparing the input text fo r LLM training is to convert the token IDs\\ninto embedding vectors, as shown in figure 2.15. As a preliminary step, we must initialize\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nOutput text\\nPostprocessing steps\\nToken IDs:\\nThis is an example .\\n40134 2052 133 389 12\\nCreating input\\ntoken embeddings\\nFigure 2.15 Preparation involves tokenizing text , converting text tokens to token IDs, and \\nconverting token IDs into embedding vectors. Here, we consider the previously created token \\nIDs to create the token embedding vectors.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 63, 'page_label': '42'}, page_content='42 CHAPTER 2 Working with text data\\nthese embedding weights with random values. This initialization serves as the starting\\npoint for the LLM’s learning process. In chapter 5, we will optimize the embedding\\nweights as part of the LLM training.\\n A continuous vector representation, or  embedding, is necessary since GPT-like\\nLLMs are deep neural networks trained with the backpropagation algorithm. \\nNOTE If you are unfamiliar with how neur al networks are trained with back-\\npropagation, please read section B.4 in appendix A.\\nLet’s see how the token ID to embedding vector conversion works with a hands-on\\nexample. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\\ninput_ids = torch.tensor([2, 3, 5, 1])\\nFor the sake of simplicity, suppose we have a small vocabulary of only 6 words (instead\\nof the 50,257 words in the BPE tokenizer vocabulary), and we want to create embed-\\ndings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\\nvocab_size = 6\\noutput_dim = 3\\nUsing the vocab_size and output_dim, we can instantiate an embedding layer in\\nPyTorch, setting the random seed to 123 for reproducibility purposes:\\ntorch.manual_seed(123)\\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nprint(embedding_layer.weight)\\nThe print statement prints the embedding layer’s underlying weight matrix:\\nParameter containing:\\ntensor([[ 0.3374, -0.1778, -0.1690],\\n        [ 0.9178,  1.5810,  1.3010],\\n        [ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-1.1589,  0.3255, -0.6315],\\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\\nThe weight matrix of the embedding layer contains small, random values. These val-\\nues are optimized during LLM training as pa rt of the LLM optimization itself. More-\\nover, we can see that the weight matrix has six rows and three columns. There is one row\\nfor each of the six possible tokens in the vocabulary, and there is one column for each of\\nthe three embedding dimensions.\\n Now, let’s apply it to a token ID to obtain the embedding vector: \\nprint(embedding_layer(torch.tensor([3])))\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 64, 'page_label': '43'}, page_content='432.8 Encoding word positions\\nThe returned embedding vector is\\ntensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\\nIf we compare the embedding vector for token ID 3 to the previous embedding\\nmatrix, we see that it is identical to the fo urth row (Python starts with a zero index, so\\nit’s the row corresponding to index 3). In other words, the embedding layer is essen-\\ntially a lookup operation that retrieves rows from the embedding layer’s weight matrix\\nvia a token ID.\\nNOTE For those who are familiar with one-hot encoding, the embedding\\nlayer approach described here is essentially just a more efficient way of imple-\\nmenting one-hot encoding followed by ma trix multiplication in a fully con-\\nnected layer, which is illustrated in  the supplementary code on GitHub at\\nhttps:/ /mng.bz/ZEB5. Because the embedding layer is just a more efficient\\nimplementation equivalent to the on e-hot encoding and matrix-multiplica-\\ntion approach, it can be seen as a neur al network layer that can be optimized\\nvia backpropagation.\\nWe’ve seen how to convert a single token ID into a three-dimensional embedding vec-\\ntor. Let’s now apply that to all four input IDs (torch.tensor([2, 3, 5, 1])):\\nprint(embedding_layer(input_ids))\\nThe print output reveals that this results in a 4 × 3 matrix:\\ntensor([[ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-2.8400, -0.7849, -1.4096],\\n        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\\nEach row in this output matrix is obtai ned via a lookup operation from the embed-\\nding weight matrix, as illustrated in figure 2.16.\\n Having now created embedd ing vectors from token IDs, next we’ll add a small\\nmodification to these embedd ing vectors to encode posi tional information about a\\ntoken within a text.\\n2.8 Encoding word positions\\nIn principle, token embeddings are a suit able input for an LLM. However, a minor\\nshortcoming of LLMs is that their self-a ttention mechanism (see chapter 3) doesn’t\\nhave a notion of position or order for the tokens within a sequence. The way the pre-\\nviously introduced embedding layer works is that the same token ID always gets\\nmapped to the same vector representation, regardless of where the token ID is posi-\\ntioned in the input sequence, as shown in figure 2.17.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 65, 'page_label': '44'}, page_content='44 CHAPTER 2 Working with text data\\n0.3374 −0.1778 − 0.1690\\n0.9178 1.5810 1.3010\\n1.2753 − 0.2010 − 0.1606\\n− 0.4015 0.9666 − 1.1481\\n− 1.1589 0.3255 − 0.6315\\n− 2.8400 − 0.7849 − 1.4096\\nWeight matrix of the\\nembedding layer\\n2\\n3\\n5\\n1\\n1.2753 − 0.2010 − 0.1606\\n− 0.4015 0.9666 − 1.1481\\n− 2.8400 − 0.7849 − 1.4096\\n0.9178 1.5810 1.3010\\n2\\n1.2753 − 0.2010 − 0.1606\\n2\\n3\\n5\\n1\\n5\\nToken IDs to embed\\nEmbedded token IDs\\nEmbedding vector of\\nthe ﬁrst token ID\\nEmbedding vector of\\nthe third token ID\\n1.2753 − 0.2010 − 0.1606\\n− 2.8400 − 0.7849 − 1.4096 fox\\njumps\\nover\\ndogfox\\njumps\\nover\\ndog\\nInput text\\n− 2.8400 − 0.7849 − 1.4096\\nFigure 2.16 Embedding layers perform a lookup operation, retrieving the embedding \\nvector corresponding to the token ID from the embedding layer’s weight matrix. For \\ninstance, the embedding vector of the token ID 5 is the sixth row of the embedding \\nlayer weight matrix (it is the sixth instead of the fifth row because Python starts \\ncounting at 0). We assume that the token IDs were produced by the small vocabulary \\nfrom section 2.3. 0.3374 − 0.1778 − 0.1690\\n0.9178 1.5810 1.3010\\n1.2753 − 0.2010 − 0.1606\\n− 0.4015 0.9666 − 1.1481\\n− 1.1589 0.3255 − 0.6315\\n− 2.8400 − 0.7849 − 1.4096\\n2\\n3\\n5\\n2\\n1.2753 − 0.2010 − 0.1606\\n− 0.4015 0.9666 − 1.1481\\n− 2.8400 − 0.7849 − 1.4096\\n1.2753 − 0.2010 − 0.1606\\n2\\n1.2753 − 0.2010 − 0.1606\\n2\\n3\\n5\\n2\\nfox\\njumps\\nover\\nfoxfox\\njumps\\nover\\nfox\\n2\\n1.2753 − 0.2010− 0.1606\\nWeight matrix of the\\nembedding layer\\nToken IDs to embed\\nThe same token IDs\\nresult in the same\\nembedding vectors\\n1.2753 − 0.2010 − 0.1606\\nFigure 2.17 The embedding layer converts a token ID into the same vector \\nrepresentation regardless of where it is located in the input sequence. For \\nexample, the token ID 5, whether it’s in the first or fourth position in the \\ntoken ID input vector, will result in the same embedding vector.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 66, 'page_label': '45'}, page_content='452.8 Encoding word positions\\nIn principle, the deterministic, position-independent embedding of the token ID is\\ngood for reproducibility purposes. Howeve r, since the self-attention mechanism of\\nLLMs itself is also position-agnostic, it is helpful to in ject additional position informa-\\ntion into the LLM.\\n To achieve this, we can use two broad categories of position-aware embeddings: rela-\\ntive positional embeddings an d absolute position al embeddings. Absolute positional\\nembeddings are directly associated with specific positions in a sequence. For each posi-\\ntion in the input sequence, a unique embedding is added to the token’s embedding to\\nconvey its exact location. For instance, the first token will have a specific positional\\nembedding, the second token another distinct embedding, and so on, as illustrated in\\nfigure 2.18.\\nInstead of focusing on the abso lute position of a token, the emphasis of relative posi-\\ntional embeddings is on the relative position or distance between tokens. This means\\nthe model learns the relationships in terms of “how far apart” rather than “at which\\nexact position.” The advantage here is that the model can generalize better to sequences\\nof varying lengths, even if it hasn’t seen such lengths during training.\\n Both types of positional  embeddings aim to augment the capacity of LLMs to\\nunderstand the order and relationships betw een tokens, ensuring more accurate and\\ncontext-aware predictions. The choice be tween them often depends on the specific\\napplication and the nature of the data being processed.\\n OpenAI’s GPT models use absolute posi tional embeddings that are optimized\\nduring the training process rather than be ing fixed or predefined like the positional\\nencodings in the original transformer model. This optimization process is part of the\\nmodel training itself. For now, let’s create the initial positional embeddings to create the\\nLLM inputs.\\nInput embeddings:\\nPositional embeddings:\\nToken embeddings:\\n++ + +\\n2.1 2.2 2.3 3.1 3.2 3.3 4.1 4.2 4.3 5.1 5.2 5.3\\n1.1 1.2 1.3 2.1 2.2 2.3 3.1 3.2 3.3 4.1 4.2 4.3\\n111 111 111 111\\nEmbedding of the ﬁrst token Embedding of the third token\\nFigure 2.18 Positional embeddings are added to the token embedding vector to create the \\ninput embeddings for an LLM. The positional vectors have the same dimension as the original \\ntoken embeddings. The token embeddings are shown with value 1 for simplicity.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 67, 'page_label': '46'}, page_content='46 CHAPTER 2 Working with text data\\n Previously, we focused on very small embedding sizes for simplicity. Now, let’s con-\\nsider more realistic and useful embedding sizes and encode the input tokens into a\\n256-dimensional vector representation, which is smaller than what the original GPT-3\\nmodel used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\\nfor experimentation. Furthermore, we assume that the to ken IDs were created by the\\nBPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\\nvocab_size = 50257\\noutput_dim = 256\\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nUsing the previous token_embedding_layer, if we sample data from the data loader,\\nwe embed each token in each batch into a 256-dimensional vector. If we have a batch\\nsize of 8 with four tokens each, the result will be an 8 × 4 × 256 tensor.\\n Let’s instantiate the data loader (see section 2.6) first:\\nmax_length = 4\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=max_length,\\n   stride=max_length, shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Token IDs:\\\\n\", inputs)\\nprint(\"\\\\nInputs shape:\\\\n\", inputs.shape)\\nThis code prints\\nToken IDs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nInputs shape:\\n torch.Size([8, 4])\\nAs we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch\\nconsists of eight text samples with four tokens each.\\n Let’s now use the embedding layer to embed these token IDs into 256-dimensional\\nvectors:\\ntoken_embeddings = token_embedding_layer(inputs)\\nprint(token_embeddings.shape)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 68, 'page_label': '47'}, page_content='472.8 Encoding word positions\\nThe print function call returns\\ntorch.Size([8, 4, 256])\\nThe 8 × 4 × 256–dimensional tensor output shows that each token ID is now embed-\\nded as a 256-dimensional vector.\\n For a GPT model’s absolute embedding approach, we just need to create another\\nembedding layer that has the sa me embedding dimension as the token_embedding_\\nlayer:\\ncontext_length = max_length\\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\\nprint(pos_embeddings.shape)\\nThe input to the pos_embeddings is usually a placeholder vector torch.arange(con-\\ntext_length), which contains a sequence of numb ers 0, 1, ..., up to the maximum\\ninput length –1. The context_length is a variable that represents the supported input\\nsize of the LLM. Here, we choose it similar to the maximum length of the input text.\\nIn practice, input text can be longer than the supported context length, in which case\\nwe have to truncate the text.\\n The output of the print statement is\\ntorch.Size([4, 256])\\nAs we can see, the positional embedding tensor consists of four 256-dimensional vec-\\ntors. We can now add these directly to the token embeddings, where PyTorch will add\\nthe 4 × 256–dimensional \\npos_embeddings tensor to each 4 × 256–dimensional token\\nembedding tensor in each of the eight batches:\\ninput_embeddings = token_embeddings + pos_embeddings\\nprint(input_embeddings.shape)\\nThe print output is\\ntorch.Size([8, 4, 256])\\nThe input_embeddings we created, as summarized in figure 2.19, are the embedded\\ninput examples that can now be processed by the main LLM modules, which we will\\nbegin implementing in the next chapter.\\n \\n \\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 69, 'page_label': '48'}, page_content='48 CHAPTER 2 Working with text data\\nSummary\\n\\uf0a1 LLMs require textual data to be converted into numerical vectors, known as\\nembeddings, since they can’t process raw text. Embeddings transform discrete\\ndata (like words or images) into contin uous vector spaces, making them com-\\npatible with neural network operations. \\n\\uf0a1 As the first step, raw text is broken into tokens, which can be words or characters.\\nThen, the tokens are converted into integer representations, termed token IDs.\\n\\uf0a1 Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance\\nthe model’s understanding and handle various contexts, such as unknown\\nwords or marking the boundary between unrelated texts.\\nGPT-like\\ndecoder-only\\ntransformer\\nOutput text\\nPostprocessing steps\\nInput text:\\nInput embeddings:\\nThis is an example.\\nTokenized text: This is an example\\nToken IDs: 40134 2052 133 389\\n.\\n12\\nToken embeddings:\\nPositional embeddings:\\n+\\nThe input embedding pipeline\\nFigure 2.19 As part of the input processing pipeline, input text is first broken \\nup into individual tokens. These tokens are then converted into token IDs using a \\nvocabulary. The token IDs are converted into embedding vectors to which positional \\nembeddings of a similar size are added, resulting in input embeddings that are used \\nas input for the main LLM layers.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 70, 'page_label': '49'}, page_content='49Summary\\n\\uf0a1 The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3\\ncan efficiently handle unknown words by  breaking them down into subword\\nunits or individual characters.\\n\\uf0a1 We use a sliding window approach on to kenized data to generate input–target\\npairs for LLM training.\\n\\uf0a1 Embedding layers in PyTorch function as a lookup operation, retrieving vectors\\ncorresponding to token IDs. The resulting embedding vectors provide continu-\\nous representations of tokens, which is crucial for training deep learning mod-\\nels like LLMs. \\n\\uf0a1 While token embeddings provide consiste nt vector representations for each\\ntoken, they lack a sense of the token’s position in a sequen ce. To rectify this,\\ntwo main types of positional embeddings  exist: absolute and relative. OpenAI’s\\nGPT models utilize absolute positional embeddings, which are added to the token\\nembedding vectors and are optimized during the model training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 71, 'page_label': '50'}, page_content='50\\nCoding attention\\nmechanisms\\nAt this point, you know how to prepare the input text for training LLMs by splitting\\ntext into individual word and subword tokens, which can be encoded into vector rep-\\nresentations, embeddings, for the LLM. \\n Now, we will look at an integral part of the LLM architecture itself, attention\\nmechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\\nin isolation and focus on them at a mechanistic level. Then we will code the remaining\\nThis chapter covers\\n\\uf0a1 The reasons for using attention mechanisms in \\nneural networks\\n\\uf0a1 A basic self-attention framework, progressing to \\nan enhanced self-attention mechanism \\n\\uf0a1 A causal attention module that allows LLMs to \\ngenerate one token at a time\\n\\uf0a1 Masking randomly selected attention weights with \\ndropout to reduce overfitting\\n\\uf0a1 Stacking multiple causal attention modules into a \\nmulti-head attention module\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 72, 'page_label': '51'}, page_content='51\\nparts of the LLM surrounding the self-attention mechanism to see it in action and to\\ncreate a model to generate text.\\n We will implement four different variants of attention mechanisms, as illustrated in\\nfigure 3.2. These different attention variants  build on each other, and the goal is to\\nThis chapter implements the\\nattention mechanism, an important\\nbuilding block of GPT-like LLMs\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.1 The three main stages of coding an LLM. This chapter focuses on step 2 of stage 1: implementing \\nattention mechanisms, which are an integral part of the LLM architecture.\\n1) Simpliﬁed\\nself-attention 2) Self-attention 3) Causal attention 4) Multi-head\\nattention\\nA simpliﬁed self-attention\\ntechnique to introduce the\\nbroader idea\\nSelf-attention with trainable\\nweights that forms the basis of\\nthe mechanism used in LLMs\\nA type of self-attention used in LLMs\\nthat allows a model to consider only\\nprevious and current inputs in a\\nsequence, ensuring temporal order\\nduring the text generation\\nAn extension of self-attention and\\ncausal attention that enables the\\nmodel to simultaneously attend\\nto information from different\\nrepresentation subspaces\\nFigure 3.2 The figure depicts different attention mechanisms we will code in this chapter, starting \\nwith a simplified version of self-attention before adding the trainable weights. The causal attention \\nmechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, \\nmulti-head attention organizes the attention mechanism into multiple heads, allowing the model to \\ncapture various aspects of the input data in parallel.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 73, 'page_label': '52'}, page_content='52 CHAPTER 3 Coding attention mechanisms\\narrive at a compact and effici ent implementation of multi- head attention that we can\\nthen plug into the LLM architecture we will code in the next chapter.\\n3.1 The problem with modeling long sequences\\nBefore we dive into the self-attention mechanism at the heart of LLMs, let’s consider\\nthe problem with pre-LLM ar chitectures that do not in clude attention mechanisms.\\nSuppose we want to develop a language tr anslation model that translates text from\\none language into another. As shown in figure 3.3, we can’t simply translate a text word\\nby word due to the grammatical structures in the source and target language.\\nTo address this problem, it is common to use a deep neural network with two submod-\\nules, an encoder and a decoder. The job of the encoder is to first read in and process the\\nentire text, and the decoder then produces the translated text.\\n Before the advent of transformers, recurrent neural networks  (RNNs) were the most\\npopular encoder–decoder architecture for la nguage translation. An RNN is a type of\\nneural network where outputs from previous  steps are fed as inputs to the current\\nduKannst helfenmir Satzdiesen uebersetzenzu\\nyouCan helpme sentencethis translateto\\nduKannst helfenmir Satzdiesen uebersetzenzu\\nyouCan mehelp translateto sentencethis\\nGerman input sentence to translate\\nThe word-by-word translation results\\nin a grammatically incorrect sentence\\nThe correct translation\\nCertain words in the generated translation\\nrequire access to words that appear earlier\\nor later in the original sentence.\\nFigure 3.3 When translating text from one language to  another, such as German to English, it’s not \\npossible to merely translate word by word. Instead, the translation process requires contextual \\nunderstanding and grammatical alignment. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 74, 'page_label': '53'}, page_content='533.1 The problem with modeling long sequences\\nstep, making them well-suited for sequential  data like text. If you are unfamiliar with\\nRNNs, don’t worry—you don’t need to know  the detailed workings of RNNs to fol-\\nlow this discussion; our focus here is mo re on the general concept of the encoder–\\ndecoder setup. \\n In an encoder–decoder RNN, the input text is fed into the encoder, which pro-\\ncesses it sequentially. The enco der updates its hidden state (the internal values at the\\nhidden layers) at each step, trying to ca pture the entire meaning of the input sen-\\ntence in the final hidden state, as illustrated in figure 3.4. The decoder then takes this\\nfinal hidden state to start generating the translated sentence, one word at a time. It\\nalso updates its hidden state at each step, which is  supposed to carry the context nec-\\nessary for the next-word prediction.\\nWhile we don’t need to know the inner workings of these encoder–decoder RNNs,\\nthe key idea here is that the encoder part processes the entire input text into a hid-\\nden state (memory cell). The decoder then ta kes in this hidden state to produce the\\noutput. You can think of this hidden state as an embedding vector, a concept we dis-\\ncussed in chapter 2.\\n The big limitation of encoder–decoder RNNs is that the RNN can’t directly access\\nearlier hidden states from the encoder du ring the decoding phase. Consequently, it\\nrelies solely on the current hidden state, which encapsulates all relevant information.\\nThis can lead to a loss of context, especially in complex sentences where dependen-\\ncies might span long distances.\\nduKannst mir\\nyouCan help\\nHidden states\\nOutputs\\nInputs\\nEncoder\\nDecoder\\nHidden states of a\\nneural network\\nA memory cell (hidden state)\\nmemorizing entire input\\nGerman input sentence to translate\\nThe translated English sentence\\nFigure 3.4 Before the advent of transformer  models, encoder–decoder RNNs were a popular choice \\nfor machine translation. The encoder takes a sequence of tokens from the source language as input, \\nwhere a hidden state (an intermediate neural network layer) of the encoder encodes a compressed \\nrepresentation of the entire input sequence. Then, the decoder uses its current hidden state to begin \\nthe translation, token by token.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 75, 'page_label': '54'}, page_content='54 CHAPTER 3 Coding attention mechanisms\\n Fortunately, it is not essential to understand RNNs to build an LLM. Just remem-\\nber that encoder–decoder RNNs had a shortcoming that motivated the design of\\nattention mechanisms.\\n3.2 Capturing data dependencies with attention \\nmechanisms\\nAlthough RNNs work fine for translating short sentences, they don’t work well for lon-\\nger texts as they don’t have direct access to previous words in the input. One major\\nshortcoming in this approach is that th e RNN must remember the entire encoded\\ninput in a single hidden state before passing it to the decoder (figure 3.4).\\n Hence, researchers developed the Bahdanau attention  mechanism for RNNs in\\n2014 (named after the first author of the re spective paper; for more information, see\\nappendix B), which modifies the encoder–decoder RNN such that the decoder can\\nselectively access different parts of the input sequence at each decoding step as illus-\\ntrated in figure 3.5.\\nInterestingly, only three years later, re searchers found that RNN architectures are\\nnot required for building deep neural netw orks for natural lang uage processing and\\nduKannst mir\\nyouCan help\\nHidden states\\nOutputs\\nInputs\\nWhen generating an output\\ntoken, the model has a way\\nto access to all input tokens.\\nThe dotted line width is proportional\\nto how important the input token is\\nfor the respective output token.\\nWe are focusing on\\ngenerating the second\\noutput token.\\nFigure 3.5 Using an attention mechanism, the text-generating decoder part of the network can \\naccess all input tokens selectively. This means that some input tokens are more important than others \\nfor generating a given output token. The importance is determined by the attention weights, which we \\nwill compute later. Note that this figure shows the general idea behind attention and does not depict \\nthe exact implementation of the Bahdanau mechanism, which is an RNN method outside this book’s \\nscope.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 76, 'page_label': '55'}, page_content='553.3 Attending to different parts of the input with self-attention\\nproposed the original transformer architecture (discussed in chapter 1) including a\\nself-attention mechanism inspired by the Bahdanau attention mechanism. \\n Self-attention is a mechan ism that allows each position in the input sequence to\\nconsider the relevancy of, or “attend to,”  all other positions in the same sequence\\nwhen computing the representation of a se quence. Self-attention is a key component\\nof contemporary LLMs based on the transformer architecture, such as the GPT series. \\n This chapter focuses on coding and unde rstanding this self-attention mechanism\\nused in GPT-like models, as illustrated in figu re 3.6. In the next chapter, we will code\\nthe remaining parts of the LLM.\\n3.3 Attending to different parts of the input \\nwith self-attention\\nWe’ll now cover the inner workings of the self-attention mechanism and learn how to\\ncode it from the ground up. Self-attenti on serves as the cornerstone of every LLM\\nbased on the transformer architecture. This topic may require a lot of focus and atten-\\ntion (no pun intended), but once you gr asp its fundamentals, you will have con-\\nquered one of the toughest aspects of this book and LLM implementation in general.\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text\\nPreprocessing steps\\nOutput text\\nPostprocessing steps\\nTopic of the\\nprevious\\nchapter\\nSelf-attention module\\nTopic of the\\ncurrent\\nchapter\\nThe remaining parts of the\\nLLM architecture are the\\ntopic of the next chapter\\nFigure 3.6 Self-attention is a mechanism in transformers used to compute \\nmore efficient input representations by allowing each position in a sequence to \\ninteract with and weigh the importance of all other positions within the same \\nsequence. In this chapter, we will code this self-attention mechanism from the \\nground up before we code the remaining parts of the GPT-like LLM in the \\nfollowing chapter.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 77, 'page_label': '56'}, page_content='56 CHAPTER 3 Coding attention mechanisms\\nSince self-attention can appear complex, espe cially if you are encountering it for the\\nfirst time, we will begin by examining a simp lified version of it. Then we will imple-\\nment the self-attention mechanism with trainable weights used in LLMs.\\n3.3.1 A simple self-attention mechanism without trainable weights\\nLet’s begin by implementing a simplified variant of self-attention, free from any train-\\nable weights, as summarized in  figure 3.7. The goal is to illustrate a few key concepts\\nin self-attention before adding trainable weights.\\nThe “self” in self-attention  \\nIn self-attention, the “self” refers to the mechanism’s ability to compute attention\\nweights by relating different positions within a single input sequence. It assesses and\\nlearns the relationships and dependencies between various parts of the input itself,\\nsuch as words in a sentence or pixels in an image. \\nThis is in contrast to traditional attention mechanisms, where the focus is on the rela-\\ntionships between elements of two different sequences, such as in sequence-to-\\nsequence models where the attention might be between an input sequence and an\\noutput sequence, such as the example depicted in figure 3.5.\\nThe context vector z(2) is\\ncomputed as a combination of\\nall input vectors weighted with\\nrespect to input element\\nx(2)\\nAttention weight to\\nweigh the importance\\nof input\\nx(1)\\nInput vector\\n(token embedding)\\ncorresponding to\\nthe ﬁrst token\\n0.4 0.1 0.8 0.5 0.8 0.6 0.5 0.8 0.6 0.0 0.8 0.5\\n0.4 0.6 0.5\\nFigure 3.7 The goal of self-attention is to compute a context vector for each input \\nelement that combines information from all other input elements. In this example, \\nwe compute the context vector z(2). The importance or contribution of each input \\nelement for computing z(2) is determined by the attention weights \\uf06121 to \\uf0612T. When \\ncomputing z(2), the attention weights are calculated with respect to input element \\nx(2) and all other inputs.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 78, 'page_label': '57'}, page_content='573.3 Attending to different parts of the input with self-attention\\nFigure 3.7 shows an input sequence, denoted as x, consisting of T elements repre-\\nsented as x(1) to x(T). This sequence typically represen ts text, such as a sentence, that\\nhas already been transformed into token embeddings.\\n For example, consider an input text like “Your journey starts with one step.” In this\\ncase, each element of the sequence, such as x(1), corresponds to a d-dimensional\\nembedding vector representing a specific token, like “Your.” Figure 3.7 shows these\\ninput vectors as three-dimensional embeddings.\\n In self-attention, our goal is to calculate context vectors z(i) for each element x(i)\\nin the input sequence. A context vector  can be interpreted as an enriched embedding\\nvector.\\n To illustrate this concept, let’s focus on the embedding vector of the second input\\nelement, x(2) (which corresponds to the token “journey”), and the corresponding con-\\ntext vector, z(2), shown at the bottom of figure 3. 7. This enhanced context vector, z(2),\\nis an embedding that co ntains information about x(2) and all other input elements,\\nx(1) to x(T).\\n Context vectors play a crucial role in self-attention. Their purpose is to create\\nenriched representations of each element in an input sequence (like a sentence)\\nby incorporating information from all othe r elements in the sequence (figure 3.7).\\nThis is essential in LLMs, which need to understand the relati onship and relevance\\nof words in a sentence to each other. Later, we will add trainable weights that help\\nan LLM learn to construct these context ve ctors so that they are relevant for the\\nLLM to generate the next token. But firs t, let’s implement a simplified self-atten-\\ntion mechanism to compute these weight s and the resulting context vector one\\nstep at a time. \\n Consider the following input sentence , which has already been embedded into\\nthree-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension\\nto ensure it fits on the page without line breaks:\\nimport torch\\ninputs = torch.tensor(\\n  [[0.43, 0.15, 0.89], # Your     (x^1)\\n   [0.55, 0.87, 0.66], # journey  (x^2)\\n   [0.57, 0.85, 0.64], # starts   (x^3)\\n   [0.22, 0.58, 0.33], # with     (x^4)\\n   [0.77, 0.25, 0.10], # one      (x^5)\\n   [0.05, 0.80, 0.55]] # step     (x^6)\\n)\\nThe first step of implementing self-attenti on is to compute the intermediate values ω,\\nreferred to as attention scores, as illustrate d in figure 3.8. Due to spatial constraints,\\nthe figure displays the values of the preceding inputs tensor in a truncated version;\\nfor example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the\\nwords “journey” and “starts” may appear similar by random chance.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 79, 'page_label': '58'}, page_content='58 CHAPTER 3 Coding attention mechanisms\\nFigure 3.8 illustrates how we calculate the intermediate attention scores between the\\nquery token and each input token. We dete rmine these scores by computing the dot\\nproduct of the query, x(2), with every other input token:\\nquery = inputs[1]                           \\nattn_scores_2 = torch.empty(inputs.shape[0])\\nfor i, x_i in enumerate(inputs):\\n    attn_scores_2[i] = torch.dot(x_i, query)\\nprint(attn_scores_2)\\nThe computed attention scores are\\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\\nUnderstanding dot products \\nA dot product is essentially a concise way of multiplying two vectors element-wise and\\nthen summing the products, which can be demonstrated as follows:\\nres = 0.\\nfor idx, element in enumerate(inputs[0]):\\n    res += inputs[0][idx] * query[idx]\\nprint(res)\\nprint(torch.dot(inputs[0], query))\\nThe output confirms that the sum of the element-wise multiplication gives the same\\nresults as the dot product:\\ntensor(0.9544)\\ntensor(0.9544)\\nEmbedded query token:\\nThe embedded query\\ntoken is one of the\\nembedded input tokens\\n(here, the query is the\\nsecond token).\\n0.4 0.1 0.8 0.5 0.8 0.6 0.5 0.8 0.6 0.0 0.8 0.5\\n0.5 0.8 0.6 0.9 1.4 1.4 1.0\\nInputs:\\nAttention score between\\ninput       and queryx(1) x(2)\\nAttention score between\\ninput       and query\\nx(3) x(2)\\nFigure 3.8 The overall goal is to illustrate the computation of the context vector z(2) using the \\nsecond input element, x(2) as a query. This figure shows the first intermediate step, computing the \\nattention scores \\uf077 between the query x(2) and all other input elements as a dot product. (Note that \\nthe numbers are truncated to one digit after the decimal point to reduce visual clutter.)\\nThe second input \\ntoken serves as \\nthe query.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 80, 'page_label': '59'}, page_content='593.3 Attending to different parts of the input with self-attention\\nIn the next step, as shown in figure 3.9, we normalize each of the attention scores we\\ncomputed previously. The main goal behind  the normalization is to obtain attention\\nweights that sum up to 1. This normalization is a convention that is useful for interpre-\\ntation and maintaining training stability in an LLM. Here’s a straightforward method\\nfor achieving this normalization step:\\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\\nprint(\"Attention weights:\", attn_weights_2_tmp)\\nprint(\"Sum:\", attn_weights_2_tmp.sum())\\nAs the output shows, the attention weights now sum to 1: \\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\\nSum: tensor(1.0000)\\nIn practice, it’s more common and advisable to use the softmax function for normal-\\nization. This approach is better at managing extreme values and offers more favorable\\nBeyond viewing the dot product operation as a mathematical tool that combines\\ntwo vectors to yield a scalar value, the dot product is a measure of similarity\\nbecause it quantifies how closely two vector s are aligned: a higher dot product indi-\\ncates a greater degree of alignment or similarity between the vectors. In the con-\\ntext of self-attention mechanisms, the dot  product determines the extent to which\\neach element in a sequence focuses on, or “attends to,” any other element: the\\nhigher the dot product, the higher the si milarity and attention score between two\\nelements.\\nAttention weights:\\nWe computed these attention\\nscores in the previous step.\\nWe now normalize the\\nattention scores to obtainω\\nthe attention weights α\\n0.4 0.1 0.8 0.5 0.8 0.6 0.5 0.8 0.6 0.0 0.8 0.5\\n0.5 0.8 0.6 0.9 1.4 1.4 1.0\\n0.1 0.2 0.2 0.1\\nFigure 3.9 After computing the attention scores \\uf07721 to \\uf0772T with respect to the input query x(2), the next \\nstep is to obtain the attention weights \\uf06121 to \\uf0612T by normalizing the attention scores.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 81, 'page_label': '60'}, page_content='60 CHAPTER 3 Coding attention mechanisms\\ngradient properties during training. The fo llowing is a basic implementation of the\\nsoftmax function for normalizing the attention scores:\\ndef softmax_naive(x):\\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\\nattn_weights_2_naive = softmax_naive(attn_scores_2)\\nprint(\"Attention weights:\", attn_weights_2_naive)\\nprint(\"Sum:\", attn_weights_2_naive.sum())\\nAs the output shows, the softmax function also meets the objective and normalizes the\\nattention weights such that they sum to 1:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nIn addition, the softmax func tion ensures that the attention weights are always posi-\\ntive. This makes the output interpretable as probabilities or relative importance,\\nwhere higher weights indicate greater importance.\\n Note that this naive softmax implementation ( softmax_naive) may encounter\\nnumerical instability problems, such as ov erflow and underflow, when dealing with\\nlarge or small input values. Therefore, in practice, it’s advisable to use the PyTorch\\nimplementation of softmax, which has been extensively optimized for performance:\\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\\nprint(\"Attention weights:\", attn_weights_2)\\nprint(\"Sum:\", attn_weights_2.sum())\\nIn this case, it yields the same results as our previous softmax_naive function:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nNow that we have computed the normalized  attention weights, we are ready for the\\nfinal step, as shown in figure 3.10: calculating the context vector z(2) by multiplying the\\nembedded input tokens, x(i), with the corresponding attention weights and then sum-\\nming the resulting vectors. Thus, context vector z(2) is the weighted sum of all input vec-\\ntors, obtained by multiplying each input vector by its corresponding attention weight:\\nquery = inputs[1]        \\ncontext_vec_2 = torch.zeros(query.shape)\\nfor i,x_i in enumerate(inputs):\\n    context_vec_2 += attn_weights_2[i]*x_i\\nprint(context_vec_2)\\nThe results of this computation are\\ntensor([0.4419, 0.6515, 0.5683])\\nThe second input \\ntoken is the query.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 82, 'page_label': '61'}, page_content='613.3 Attending to different parts of the input with self-attention\\nNext, we will generalize this procedure for computing context vectors to calculate all\\ncontext vectors simultaneously.\\n3.3.2 Computing attention weights for all input tokens\\nSo far, we have computed attention weig hts and the context vector for input 2, as\\nshown in the highlighted row in figure 3.11. Now let’s extend this computation to cal-\\nculate attention weights and context vectors for all inputs.\\nAttention weights:\\n0.4 0.1 0.8 0.5 0.8 0.6 0.5 0.8 0.6 0.0 0.8 0.5\\n0.1\\nContext vector: 0.4 0.6 0.5\\nInputs:\\n× 0.2 × 0.2 × 0.1 ×\\n+\\nMultiply each input vector\\nwith the corresponding\\nattention weight.\\nThis is the second context vector because\\nthe attention weights were computed\\nwith respect to the second input vector\\nin the previous steps.\\nFigure 3.10 The final step, after calculating and nor malizing the attention scores to obtain the \\nattention weights for query x(2), is to compute the context vector z(2). This context vector is a \\ncombination of all input vectors x(1) to x(T) weighted by the attention weights.\\nYour startswith one stepjourney\\nThis row contains the attention\\nweights (normalized attention\\nscores) computed previously\\nYour 0.20 0.20 0.19 0.12 0.12 0.14\\nstarts 0.13 0.23 0.23 0.12 0.11 0.15\\nwith 0.14 0.20 0.20 0.14 0.12 0.17\\none 0.15 0.19 0.19 0.13 0.18 0.12\\nstep 0.13 0.21 0.21 0.14 0.09 0.18\\njourney 0.13 0.23 0.23 0.12 0.10 0.15\\nFigure 3.11 The highlighted row shows the attention weights for the second \\ninput element as a query. Now we will generalize the computation to obtain \\nall other attention weights. (Please note that the numbers in this figure are \\ntruncated to two digits after the decimal point to reduce visual clutter. The \\nvalues in each row should add up to 1.0 or 100%.)Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 83, 'page_label': '62'}, page_content='62 CHAPTER 3 Coding attention mechanisms\\nWe follow the same three steps as before (see figure 3.12), except that we make a few\\nmodifications in the code to compute all context vectors instead of only the second\\none, z(2):\\nattn_scores = torch.empty(6, 6)\\nfor i, x_i in enumerate(inputs):\\n    for j, x_j in enumerate(inputs):\\n        attn_scores[i, j] = torch.dot(x_i, x_j)\\nprint(attn_scores)\\nThe resulting attention scores are as follows:\\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\\nEach element in the tensor represents an attention score between each pair of inputs,\\nas we saw in figure 3.11. Note that the va lues in that figure are normalized, which is\\nwhy they differ from the unnormalized atte ntion scores in the preceding tensor. We\\nwill take care of the normalization later. \\n When computing the preceding a ttention score tensor, we used for loops in\\nPython. However, for loops are generally slow, and we  can achieve the same results\\nusing matrix multiplication:\\nattn_scores = inputs @ inputs.T\\nprint(attn_scores)\\nWe can visually confirm that the results are the same as before:\\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\\n1) Compute attention scores\\n2) Compute attention weights\\n3) Compute context vectors\\nCompute the attention scores as\\ndot products between the inputs.\\nThe attention weights are a normalized\\nversion of the attention scores.\\nThe context vectors are computed as\\na weighted sum over the inputs.\\nFigure 3.12 In step 1, we add an additional for loop to compute the dot \\nproducts for all pairs of inputs.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 84, 'page_label': '63'}, page_content='633.3 Attending to different parts of the input with self-attention\\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\\nIn step 2 of figure 3.12, we normalize each row so that the values in each row sum to 1:\\nattn_weights = torch.softmax(attn_scores, dim=-1)\\nprint(attn_weights)\\nThis returns the following attention weight  tensor that matches the values shown in\\nfigure 3.10:\\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\\nIn the context of using PyTorch, the dim parameter in functions like torch.softmax\\nspecifies the dimension of the input tens or along which the function will be com-\\nputed. By setting dim=-1, we are instructing the softmax function to apply the nor-\\nmalization along the last dimension of the attn_scores tensor. If attn_scores is a\\ntwo-dimensional tensor (for example, with a shape of [rows, colu mns]), it will nor-\\nmalize across the columns so that the valu es in each row (summing over the column\\ndimension) sum up to 1.\\n We can verify that the rows indeed all sum to 1:\\nrow_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nprint(\"Row 2 sum:\", row_2_sum)\\nprint(\"All row sums:\", attn_weights.sum(dim=-1))\\nThe result is\\nRow 2 sum: 1.0\\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\\nIn the third and final step of figure 3.12, we use these attention weights to compute all\\ncontext vectors via matrix multiplication:\\nall_context_vecs = attn_weights @ inputs\\nprint(all_context_vecs)\\nIn the resulting output tensor, each row contains a three-dimensional context vector:\\ntensor([[0.4421, 0.5931, 0.5790],\\n        [0.4419, 0.6515, 0.5683],\\n        [0.4431, 0.6496, 0.5671],\\n        [0.4304, 0.6298, 0.5510],\\n        [0.4671, 0.5910, 0.5266],\\n        [0.4177, 0.6503, 0.5645]])\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 85, 'page_label': '64'}, page_content='64 CHAPTER 3 Coding attention mechanisms\\nWe can double-check that th e code is correct by comparing the second row with the\\ncontext vector z(2) that we computed in section 3.3.1:\\nprint(\"Previous 2nd context vector:\", context_vec_2)\\nBased on the result, we can see that the previously calculated context_vec_2 matches\\nthe second row in the previous tensor exactly: \\nPrevious 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\\nThis concludes the code walkthrough of a simple self-attention mechanism. Next, we\\nwill add trainable weights, enabling the LL M to learn from data and improve its per-\\nformance on specific tasks.\\n3.4 Implementing self-attentio n with trainable weights\\nOur next step will be to implement the self-attention mechanism used in the origi-\\nnal transformer architecture, the GPT models, and most other popular LLMs. This\\nself-attention mechanism is also called scaled dot-product attention . Figure 3.13 shows\\nhow this self-attention mechanism fits in to the broader context of implementing\\nan LLM. \\n1) Simpliﬁed\\nself-attention 2) Self-attention 3) Causal attention 4) Multi-head\\nattention\\nWe already implemented\\na simpliﬁed attention\\nmechanism.\\nWe will now extend the\\nself-attention mechanism\\nwith trainable weights.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM Foundation model\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.13 Previously, we coded a simplified attention mechanism to understand the basic mechanism behind \\nattention mechanisms. Now, we add trainable weights to this attention mechanism. Later, we will extend this \\nself-attention mechanism by adding a causal mask and multiple heads.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 86, 'page_label': '65'}, page_content='653.4 Implementing self-attention with trainable weights\\nAs illustrated in figure 3.13, the self-attention mechanism with trainable weights builds\\non the previous concepts: we want to compute context vectors as weighted sums over\\nthe input vectors specific to a certain input element. As you will see, there are only slight\\ndifferences compared to the basic self-attention mechanism we coded earlier.\\n The most notable difference is the introduction of weight matrices that are\\nupdated during model training. These trainable weight matrices are crucial so that\\nthe model (specifically, the attention module  inside the model) can learn to produce\\n“good” context vectors. (We will train the LLM in chapter 5.)\\n We will tackle this self-attention mechanism in the two subsections. First, we will code\\nit step by step as before. Second, we will organize the code into a compact Python class\\nthat can be imported into the LLM architecture.\\n3.4.1 Computing the attention weights step by step\\nWe will implement the self-attention mechanism step by step by introducing the\\nthree trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\\nproject the embedd ed input tokens, x(i), into query, key, and value vectors, respec-\\ntively, as illustrated in figure 3.14.\\nEarlier, we defined the second input element x(2) as the query when we computed the\\nsimplified attention weights to  compute the context vector z(2). Then we generalized\\nthis to compute all context vectors z(1) ... z (T) for the six-word input sentence “Your\\njourney starts with one step.” \\nThe second input token serves as the\\ncurrent input vector to create the query\\n0.4 0.1 0.8\\nkey value\\n0.5 0.8 0.6 0.0 0.8 0.5\\nThis is the value vector corresponding to the ﬁrst input token obtained via\\nmatrix multiplication between the weight matrix        and input token\\n0.3 0.7 0.1 0.8 0.4 1.1 0.3 1.0 0.3 0.9 0.3 0.70.4 1.4\\nFigure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we compute query \\n(q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second \\ninput, x(2), as the query input. The query vector q(2) is obtained via matrix multiplication between the input x(2) and \\nthe weight matrix Wq. Similarly, we obtain the key and value vectors via matrix multiplication involving the weight \\nmatrices Wk and Wv.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 87, 'page_label': '66'}, page_content='66 CHAPTER 3 Coding attention mechanisms\\n Similarly, we start here by computing only one context vector, z(2), for illustration\\npurposes. We will then modify this code to calculate all context vectors.\\n Let’s begin by defining a few variables:\\nx_2 = inputs[1]    \\nd_in = inputs.shape[1]     \\nd_out = 2        \\nNote that in GPT-like models, the input and output dimensions are usually the same,\\nbut to better follow the computation, we’ll use different input ( d_in=3) and output\\n(d_out=2) dimensions here.\\n Next, we initialize the three weight matrices Wq, Wk, and Wv shown in figure 3.14:\\ntorch.manual_seed(123)\\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\\nWe set requires_grad=False to reduce clutter in the outputs, but if we were to use\\nthe weight matrices for mo del training, we would set requires_grad=True to update\\nthese matrices during model training.\\n Next, we compute the query, key, and value vectors:\\nquery_2 = x_2 @ W_query \\nkey_2 = x_2 @ W_key \\nvalue_2 = x_2 @ W_value\\nprint(query_2)\\nThe output for the query results in a two- dimensional vector since we set the number\\nof columns of the corresponding weight matrix, via d_out, to 2:\\ntensor([0.4306, 1.4551])\\nWeight parameters vs. attention weights \\nIn the weight matrices W, the term “weight” is short for “weight parameters,” the val-\\nues of a neural network that are optimized during training. This is not to be confused\\nwith the attention weights. As we already saw, attention weights determine the extent\\nto which a context vector depends on the different parts of the input (i.e., to what\\nextent the network focuses on different parts of the input). \\nIn summary, weight parameters are the fundamental, learned coefficients that define\\nthe network’s connections, while attention weights are dynamic, context-specific values.\\nThe second\\ninput element\\nThe input embedding \\nsize, d=3\\nThe output embedding \\nsize, d_out=2\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 88, 'page_label': '67'}, page_content='673.4 Implementing self-attention with trainable weights\\nEven though our temporary goal is only to compute the one context vector, z(2), we still\\nrequire the key and value vect ors for all input elements as they are involved in com-\\nputing the attention weights with respect to the query q(2) (see figure 3.14).\\n We can obtain all keys and values via matrix multiplication:\\nkeys = inputs @ W_key \\nvalues = inputs @ W_value\\nprint(\"keys.shape:\", keys.shape)\\nprint(\"values.shape:\", values.shape)\\nAs we can tell from the output s, we successfully projected the six input tokens from a\\nthree-dimensional onto a two-dimensional embedding space:\\nkeys.shape: torch.Size([6, 2])\\nvalues.shape: torch.Size([6, 2])\\nThe second step is to compute the attention scores, as shown in figure 3.15.\\nFirst, let’s compute the attention score ω22:\\nkeys_2 = keys[1]            \\nattn_score_22 = query_2.dot(keys_2)\\nprint(attn_score_22)\\nThe unscaled attention score is computed\\nas a dot product between the query and\\nthe key vectors.\\nSince we want to compute the context vector\\nfor the second input token, the query is derived\\nfrom that second input token.\\n0.4 0.1 0.8\\nquery\\nkey value\\n0.5 0.8 0.6 0.0 0.8 0.5\\n0.3 0.7 0.1 0.8 0.4 1.1 0.3 1.0 0.3 0.9 0.3 0.70.4 1.4\\n0.4 1.4 0.4 1.4\\n1.2 1.8 1.5Attention\\nscore\\nFigure 3.15 The attention score computation is a dot- product computation similar to what we used in the \\nsimplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the \\ndot-product between the input elements but using the query and key obtained by transforming the inputs via the \\nrespective weight matrices.\\nRemember that Python \\nstarts indexing at 0.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 89, 'page_label': '68'}, page_content='68 CHAPTER 3 Coding attention mechanisms\\nThe result for the unnormalized attention score is\\ntensor(1.8524)\\nAgain, we can generalize this computat ion to all attention scores via matrix\\nmultiplication:\\nattn_scores_2 = query_2 @ keys.T      \\nprint(attn_scores_2)\\nAs we can see, as a quick check, the se cond element in the output matches the\\nattn_score_22 we computed previously:\\ntensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\\nNow, we want to go from the attention scores to the attention weights, as illustrated in\\nfigure 3.16. We compute the attention weig hts by scaling the attention scores and\\nusing the softmax function. However, now we  scale the attention scores by dividing\\nthem by the square root of the embedding dimension of the keys (taking the square\\nroot is mathematically the same as exponentiating by 0.5):\\nd_k = keys.shape[-1]\\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\\nprint(attn_weights_2)\\nAll attention scores \\nfor given query\\n0.4 0.1 0.8\\nquery q(2)\\nkey value\\nq(2)\\n0.5 0.8 0.6 0.0 0.8 0.5\\n0.3 0.7 0.1 0.8 0.4 1.1 0.3 1.0 0.3 0.9 0.3 0.70.4 1.4\\n0.4 1.4 0.4 1.4\\n1.2 1.8 1.5Attention\\nscore\\n0.1 0.2 0.1\\nThe unscaled attention\\nscore from the previous\\nstep.\\nThe attention weights\\nare computed using the\\nsoftmax function.\\nAttention\\nweight\\nFigure 3.16 After computing the attention scores \\uf077, the next step is to normalize these scores using the \\nsoftmax function to obtain the attention weights \\uf061.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 90, 'page_label': '69'}, page_content='693.4 Implementing self-attention with trainable weights\\nThe resulting attention weights are\\ntensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\\nNow, the final step is to compute the context vectors, as illustrated in figure 3.17.\\nSimilar to when we computed the context vector as a weighted sum over the input vec-\\ntors (see section 3.3), we now compute the context vector as a weighted sum over the\\nvalue vectors. Here, the attention weights serve as a weighting factor that weighs\\nThe rationale behind scaled-dot product attention\\nThe reason for the normalization by the embedding dimension size is to improve the\\ntraining performance by avoiding small gradients. For instance, when scaling up the\\nembedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large\\ndot products can result in very small gradients during backpropagation due to the\\nsoftmax function applied to them. As do t products increase, the softmax function\\nbehaves more like a step function, resulting in gradients nearing zero. These small\\ngradients can drastically slow down learning or cause training to stagnate.\\nThe scaling by the square root of the embedding dimension is the reason why this\\nself-attention mechanism is also called scaled-dot product attention. \\nquery\\nkey ( value\\nT\\n03. 07. 01. 08. 04. 11. 03. 10. 03. 09. 03. 07.04. 14.\\n04. 14. 04. 14.\\n12. 18. 15.\\n01. 02. 01.Attention\\nweight T\\n03. 08.\\nContext\\nvector\\nThe last step is multiplying each\\nvalue vector with its respective\\nattention weight and then summing\\nthem to obtain the context vector\\n0.5 0.8 0.6 0.0 0.8 0.50.4 0.1 0.8\\nFigure 3.17 In the final step of the self-attention comput ation, we compute the context vector by combining all \\nvalue vectors via the attention weights. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 91, 'page_label': '70'}, page_content='70 CHAPTER 3 Coding attention mechanisms\\nthe respective importance of each value vector. Also as before, we can use matrix mul-\\ntiplication to obtain the output in one step:\\ncontext_vec_2 = attn_weights_2 @ values\\nprint(context_vec_2)\\nThe contents of the resulting vector are as follows:\\ntensor([0.3061, 0.8210])\\nSo far, we’ve only computed a single context vector, z(2). Next, we will generalize the\\ncode to compute all context vectors in the input sequence, z(1) to z(T).\\n3.4.2 Implementing a compact self-attention Python class\\nAt this point, we have gone through a lo t of steps to compute the self-attention out-\\nputs. We did so mainly for illustration purpos es so we could go through one step at a\\ntime. In practice, with the LLM implementation in the next chapter in mind, it is\\nhelpful to organize this code into a Python class, as shown in the following listing.\\nimport torch.nn as nn\\nclass SelfAttention_v1(nn.Module):\\n    def __init__(self, d_in, d_out):\\n        super().__init__()\\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\\nWhy query, key, and value?\\nThe terms “key,” “query,” and “value” in the context of attention mechanisms are\\nborrowed from the domain of information retrieval and databases, where similar con-\\ncepts are used to store, search, and retrieve information.\\nA query is analogous to a search query in a database. It represents the current item\\n(e.g., a word or token in a sentence) the model focuses on or tries to understand.\\nThe query is used to probe the other part s of the input sequence to determine how\\nmuch attention to pay to them.\\nThe key is like a database key used for indexing and searching. In the attention mech-\\nanism, each item in the input sequence (e.g., each word in a sentence) has an asso-\\nciated key. These keys are used to match the query. \\nThe value in this context is similar to the value in a key-value pair in a database. It\\nrepresents the actual content or representation of the input items. Once the model\\ndetermines which keys (and thus which part s of the input) are most relevant to the\\nquery (the current focus item), it retrieves the corresponding values.\\nListing 3.1 A compact self-attention class\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 92, 'page_label': '71'}, page_content='713.4 Implementing self-attention with trainable weights\\n    def forward(self, x):\\n        keys = x @ self.W_key\\n        queries = x @ self.W_query\\n        values = x @ self.W_value\\n        attn_scores = queries @ keys.T # omega\\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        context_vec = attn_weights @ values\\n        return context_vec\\nIn this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\\nfundamental building block of PyTorch models that provides necessary functionalities\\nfor model layer creation and management. \\n The __init__ method initializes trainable weight matrices ( W_query, W_key, and\\nW_value) for queries, keys, and values, each transforming the input dimension d_in to\\nan output dimension d_out. \\n During the forward pass, using the fo rward method, we compute the attention\\nscores (attn_scores) by multiplying queries and keys, normalizing these scores using\\nsoftmax. Finally, we create a context vector by weighting the values with these normal-\\nized attention scores.\\n We can use this class as follows:\\ntorch.manual_seed(123)\\nsa_v1 = SelfAttention_v1(d_in, d_out)\\nprint(sa_v1(inputs))\\nSince inputs contains six embedding vectors, this  results in a matrix storing the six\\ncontext vectors:\\ntensor([[0.2996, 0.8053],\\n        [0.3061, 0.8210],\\n        [0.3058, 0.8203],\\n        [0.2948, 0.7939],\\n        [0.2927, 0.7891],\\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\\nAs a quick check, notice that the second row ( [0.3061, 0.8210]) matches the con-\\ntents of context_vec_2 in the previous section. Figure 3.18 summarizes the self-atten-\\ntion mechanism we just implemented.\\n Self-attention involves the trainable weight matrices Wq, Wk, and Wv. These matrices\\ntransform input data into queries, keys, and values, respectively, which are crucial com-\\nponents of the attention mechanism. As th e model is exposed to more data during\\ntraining, it adjusts these trainable weights, as we will see in upcoming chapters.\\n We can improve the SelfAttention_v1 implementation further by utilizing\\nPyTorch’s nn.Linear layers, which effectively perform matrix multiplication when\\nthe bias units are disabled. Additional ly, a significant advantage of using nn.Linear\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 93, 'page_label': '72'}, page_content='72 CHAPTER 3 Coding attention mechanisms\\ninstead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\\nhas an optimized weight initialization sc heme, contributing to more stable and\\neffective model training.\\nclass SelfAttention_v2(nn.Module):\\n    def __init__(self, d_in, d_out, qkv_bias=False):\\nListing 3.2 A self-attention class using PyTorch’s Linear layers\\nContext vector\\ncorresponding to\\nthe second input\\ntoken\\nEmbedded queries, where the\\nsecond row is the query\\nvector\\nq(2) corresponding to\\nthe second input token x(2)\\nWe multiply the inputs X\\nwith weight matrix Wv to\\nget the value matrix V.\\nThe embedding\\nvector x(2) of the\\nsecond input token\\n0.4 0.1 0.8\\n0.4 1.4\\nInputs\\nWeight\\nmatrix\\nWeight\\nmatrix\\nWeight\\nmatrix\\nQueries Keys Values\\n0.3 0.8\\nContext\\nvectors\\n0.4 1.1 0.3 1.0\\nAttention weight matrix\\ncontaining the attention\\nscores for each pair of\\ninputs\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.19 0.16 0.16 0.15 0.17 0.15\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.19 0.16 0.16 0.15 0.16 0.15\\nFigure 3.18 In self-attention, we transform the input vectors in the input matrix X with the three weight \\nmatrices, Wq, Wk, and Wv. The new compute the attention weight matrix based on the resulting queries (Q) and \\nkeys (K). Using the attention weights and values (V), we then compute the context vectors (Z). For visual clarity, \\nwe focus on a single input text with n tokens, not a batch of multiple inputs. Consequently, the three-dimensional \\ninput tensor is simplified to a two-dimensional matrix in this context. This approach allows for a more straightforward \\nvisualization and understanding of the processes involved. For consistency with later figures, the values in the \\nattention matrix do not depict the real attention weights. (The numbers in this figure are truncated to two digits \\nafter the decimal point to reduce visual clutter. The values in each row should add up to 1.0 or 100%.)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 94, 'page_label': '73'}, page_content='733.4 Implementing self-attention with trainable weights\\n        super().__init__()\\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n    def forward(self, x):\\n        keys = self.W_key(x)\\n        queries = self.W_query(x)\\n        values = self.W_value(x)\\n        attn_scores = queries @ keys.T\\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        context_vec = attn_weights @ values\\n        return context_vec\\nYou can use the SelfAttention_v2 similar to SelfAttention_v1:\\ntorch.manual_seed(789)\\nsa_v2 = SelfAttention_v2(d_in, d_out)\\nprint(sa_v2(inputs))\\nThe output is\\ntensor([[-0.0739,  0.0713],\\n        [-0.0748,  0.0703],\\n        [-0.0749,  0.0702],\\n        [-0.0760,  0.0685],\\n        [-0.0763,  0.0679],\\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\\nNote that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\nthey use different initial weight s for the weight matrices since nn.Linear uses a more\\nsophisticated weight initialization scheme.\\nExercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\\nNote that nn.Linear in SelfAttention_v2 uses a different we ight initialization\\nscheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1,\\nwhich causes both mechanisms to produce  different results. To check that both\\nimplementations, SelfAttention_v1 and SelfAttention_v2, are otherwise simi-\\nlar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-\\nAttention_v1, such that both objects then produce the same results.\\nYour task is to correctly assign the weights from an instance of SelfAttention_v2\\nto an instance of SelfAttention_v1. To do this, you need to understand the rela-\\ntionship between the weights in both versions. (Hint: nn.Linear stores the weight\\nmatrix in a transposed form. ) After the assign ment, you should observe that both\\ninstances produce the same outputs.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 95, 'page_label': '74'}, page_content='74 CHAPTER 3 Coding attention mechanisms\\nNext, we will make enhancements to the self-attention mechanism, focusing specifically\\non incorporating causal and multi-head elem ents. The causal aspect involves modify-\\ning the attention mechanism to prevent the model from accessing future information\\nin the sequence, which is crucial for tasks like language modeling, where each word\\nprediction should only depend on previous words. \\n The multi-head component involves spli tting the attention mechanism into multi-\\nple “heads.” Each head learns different aspects of the data, allowing the model to\\nsimultaneously attend to information from different representation subspaces at dif-\\nferent positions. This improves the model’s performance in complex tasks.\\n3.5 Hiding future words with causal attention\\nFor many LLM tasks, you will want the self -attention mechanism to consider only the\\ntokens that appear prior to the current posi tion when predicting the next token in a\\nsequence. Causal attention, also known as masked attention, is a specialized form of self-\\nattention. It restricts a model to only consider previous and current inputs in a sequence\\nwhen processing any given token when computing attention scores. This is in contrast\\nto the standard self-attention mechanism, which allows access to the entire input\\nsequence at once.\\n Now, we will modify the standard self-attention mechanism to create a causal\\nattention mechanism, which is essential for developing an LLM in the subsequent\\nchapters. To achieve this in  GPT-like LLMs, for each token processed, we mask out\\nthe future tokens, which come after the current token in the input text, as illus-\\ntrated in figure 3.19. We mask out the attention weights above the diagonal, and we\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.19 0.16 0.16 0.15 0.17 0.15\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.19 0.16 0.16 0.15 0.16 0.15\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\n0.55 0.44\\n1.0\\n0.38 0.30 0.31\\n0.27 0.24 0.24 0.23\\n0.21 0.19 0.19 0.18 0.19\\n0.19 0.16 0.16 0.15 0.16 0.15\\nMasked out\\nfuture tokens\\nfor the “Your”\\ntoken\\nAttention weight for input tokens\\ncorresponding to “step” and “Your”\\nFigure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for \\na given input, the LLM can’t access future tokens when computing the context vectors using the \\nattention weights. For example, for the word “journey” in the second row, we only keep the attention \\nweights for the words before (“Your”) and in the current position (“journey”).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 96, 'page_label': '75'}, page_content='753.5 Hiding future words with causal attention\\nnormalize the nonmasked attention weights such that the attention weights sum to 1 in\\neach row. Later, we will implement this masking and normalization procedure in code.\\n3.5.1 Applying a causal attention mask\\nOur next step is to implement the causal attention mask in code. To implement the\\nsteps to apply a causal attention mask to ob tain the masked attention weights, as sum-\\nmarized in figure 3.20, let’s work with the attention scores and weights from the previ-\\nous section to code the causal attention mechanism. \\nIn the first step, we compute the attentio n weights using the softmax function as we\\nhave done previously:\\nqueries = sa_v2.W_query(inputs)    \\nkeys = sa_v2.W_key(inputs) \\nattn_scores = queries @ keys.T\\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\\nprint(attn_weights)\\nThis results in the following attention weights:\\ntensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<SoftmaxBackward0>)\\nWe can implement the second step using PyTorch’s tril function to create a mask\\nwhere the values above the diagonal are zero:\\ncontext_length = attn_scores.shape[0]\\nmask_simple = torch.tril(torch.ones(context_length, context_length))\\nprint(mask_simple)\\nAttention scores\\n(unnormalized)\\nAttention weights\\n(normalized)\\n1) Apply\\nsoftmax\\n2) Mask with 0’s\\nabove diagonal\\nMasked attention scores\\n(unnormalized)\\nMasked attention weights\\n(normalized)\\n3) Normalize\\nrows\\n“Normalized” means that the\\nvalues in each row sum to 1\\nFigure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the \\nsoftmax function to the attention scores, zeroing out the elements above the diagonal and normalizing \\nthe resulting matrix. Reuses the query and key weight matrices \\nof the SelfAttention_v2 object from the \\nprevious section for convenience\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 97, 'page_label': '76'}, page_content='76 CHAPTER 3 Coding attention mechanisms\\nThe resulting mask is\\ntensor([[1., 0., 0., 0., 0., 0.],\\n        [1., 1., 0., 0., 0., 0.],\\n        [1., 1., 1., 0., 0., 0.],\\n        [1., 1., 1., 1., 0., 0.],\\n        [1., 1., 1., 1., 1., 0.],\\n        [1., 1., 1., 1., 1., 1.]])\\nNow, we can multiply this mask with the attention weights to zero-out the values above\\nthe diagonal:\\nmasked_simple = attn_weights*mask_simple\\nprint(masked_simple)\\nAs we can see, the elements above the diagonal are successfully zeroed out:\\ntensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<MulBackward0>)\\nThe third step is to renormalize the attent ion weights to sum up to 1 again in each\\nrow. We can achieve this by dividing each element in each row by the sum in each row:\\nrow_sums = masked_simple.sum(dim=-1, keepdim=True)\\nmasked_simple_norm = masked_simple / row_sums\\nprint(masked_simple_norm)\\nThe result is an attention weight matrix where the attention weights above the diago-\\nnal are zeroed-out, and the rows sum to 1:\\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<DivBackward0>)\\nInformation leakage\\nWhen we apply a mask and then renormalize the attention weights, it might initially\\nappear that information from future tokens (which we intend to mask) could still influ-\\nence the current token because their values are part of the softmax calculation. How-\\never, the key insight is that when we renormalize the attention weights after masking,\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 98, 'page_label': '77'}, page_content='773.5 Hiding future words with causal attention\\nWhile we could wrap up our implementation of  causal attention at this point, we can\\nstill improve it. Let’s take a mathematical property of the softmax function and imple-\\nment the computation of the masked attention weights more efficiently in fewer steps,\\nas shown in figure 3.21.\\nThe softmax function converts its inputs in to a probability dist ribution. When nega-\\ntive infinity values (-∞) are present in a row, the softmax function treats them as zero\\nprobability. (Mathematically, this is because e –∞ approaches 0.)\\n We can implement this more efficient ma sking “trick” by creating a mask with 1s\\nabove the diagonal and then replacing these 1s with negative infinity (-inf) values:\\nmask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\\nprint(masked)\\nThis results in the following mask:\\ntensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\\n       grad_fn=<MaskedFillBackward0>)\\nwhat we’re essentially doing is recalculating the softmax over a smaller subset (since\\nmasked positions don’t contribute to the softmax value).\\nThe mathematical elegance of softmax is that despite initially including all positions\\nin the denominator, after masking and renormalizing, the effect of the masked posi-\\ntions is nullified—they don’t contribute to the softmax score in any meaningful way.\\nIn simpler terms, after masking and renorma lization, the distribution of attention\\nweights is as if it was calculated only among the unmasked positions to begin with.\\nThis ensures there’s no information leakage from future (or otherwise masked)\\ntokens as we intended.\\n1) Mask with −∞\\nabove diagonal\\n2) Apply\\nsoftmax\\nAttention scores\\n(unnormalized)\\nMasked attention scores\\n(unnormalized)\\nMasked attention weights\\n(normalized)\\nFigure 3.21 A more efficient way to obtain the masked attention weight matrix in \\ncausal attention is to mask the attention scores with negative infinity values before \\napplying the softmax function.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 99, 'page_label': '78'}, page_content='78 CHAPTER 3 Coding attention mechanisms\\nNow all we need to do is apply the softmax  function to these masked results, and we\\nare done:\\nattn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\\nprint(attn_weights)\\nAs we can see based on the output, the values in each row sum to 1, and no further\\nnormalization is necessary:\\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\\n       grad_fn=<SoftmaxBackward0>)\\nWe could now use the modified attention weights to compute the context vectors via\\ncontext_vec = attn_weights @ values, as in section 3.4. However, we will first cover\\nanother minor tweak to the causal attentio n mechanism that is useful for reducing\\noverfitting when training LLMs.\\n3.5.2 Masking additional atte ntion weights with dropout\\nDropout in deep learning is a technique wher e randomly selected hidden layer units\\nare ignored during training, effectively “dropping” them out. This method helps pre-\\nvent overfitting by ensuring that a model does not become overly reliant on any spe-\\ncific set of hidden layer units. It’s import ant to emphasize that dropout is only used\\nduring training and is disabled afterward.\\n In the transformer architecture, includin g models like GPT, dropout in the atten-\\ntion mechanism is typically applied at two specific times: after calculating the atten-\\ntion weights or after applying the attention weights to the value vectors. Here we will\\napply the dropout mask after computing the attention weights, as illustrated in fig-\\nure 3.22, because it’s the more common variant in practice.\\n In the following code example, we use a dropout rate of 50%, which means mask-\\ning out half of the attention weights. (When we train the GPT model in later chapters,\\nwe will use a lower dropout rate, such as 0.1 or 0.2.) We apply PyTorch’s dropout\\nimplementation first to a 6 × 6 tensor consisting of 1s for simplicity:\\ntorch.manual_seed(123)\\ndropout = torch.nn.Dropout(0.5)   \\nexample = torch.ones(6, 6)     \\nprint(dropout(example))\\nWe choose a \\ndropout rate of 50%.\\nHere, we create a \\nmatrix of 1s.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 100, 'page_label': '79'}, page_content='793.5 Hiding future words with causal attention\\nAs we can see, approximately half of the values are zeroed out:\\ntensor([[2., 2., 0., 2., 2., 0.],\\n        [0., 0., 0., 2., 0., 2.],\\n        [2., 2., 2., 2., 0., 2.],\\n        [0., 2., 2., 0., 0., 2.],\\n        [0., 2., 0., 2., 0., 2.],\\n        [0., 2., 2., 2., 2., 0.]])\\nWhen applying dropout to an attention weight matrix with a rate of 50%, half of the\\nelements in the matrix are randomly set to  zero. To compensate for the reduction in\\nactive elements, the values of the remaining elements in the matrix are scaled up by a\\nfactor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the atten-\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\n0.55 0.44\\n1.0\\n0.38 0.30 0.31\\n0.27 0.24 0.24 0.23\\n0.21 0.19 0.19 0.18 0.19\\n0.19 0.16 0.16 0.15 0.16 0.15\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\nDropout mask\\nwith random\\npositions to be\\ndropped\\nThe dropout mask\\napplied to the\\nattention scores will\\nzero out certain\\nattention scores\\nAttention weight for input\\ntokens corresponding to\\n“step” and “Your”\\n1.0\\n0.38 0.30 0.31\\n0.24 0.24\\n0.19 0.18\\n0.16 0.16 0.15 0.16\\nFigure 3.22 Using the causal attention mask (upper left), we apply an additional \\ndropout mask (upper right) to zero out additional attention weights to reduce overfitting \\nduring training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 101, 'page_label': '80'}, page_content='80 CHAPTER 3 Coding attention mechanisms\\ntion weights, ensuring that the average influence of the attention mechanism remains\\nconsistent during both the training and inference phases.\\n Now let’s apply dropout to the attention weight matrix itself:\\ntorch.manual_seed(123)\\nprint(dropout(attn_weights))\\nThe resulting attention weight matrix now has additional elements zeroed out and the\\nremaining 1s rescaled:\\ntensor([[2.0000, 0.0000, 0 .0000, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\\n        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\\n        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\\n        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\\n       grad_fn=<MulBackward0>\\nNote that the resulting drop out outputs may look differen t depending on your oper-\\nating system; you can read more about this  inconsistency here on the PyTorch issue\\ntracker at https:/ /github.com/pytorch/pytorch/issues/121595.\\n Having gained an understanding of causal attention and dropout masking, we can\\nnow develop a concise Python class. This cl ass is designed to facilitate the efficient\\napplication of these two techniques.\\n3.5.3 Implementing a compac t causal attention class\\nWe will now incorporate the causal atte ntion and dropout modifications into the\\nSelfAttention Python class we developed in section 3.4. This class will then serve as a\\ntemplate for developing multi-head attention, which is the final attention class we will\\nimplement.\\n But before we begin, let’s ensure that the code can handle batches consisting of\\nmore than one input so that the CausalAttention class supports the batch outputs\\nproduced by the data loader we implemented in chapter 2.\\n For simplicity, to simulate such batch inputs, we duplicate the input text example:\\nbatch = torch.stack((inputs, inputs), dim=0)\\nprint(batch.shape)               \\nThis results in a three-dimensional tensor consisting of two input texts with six tokens\\neach, where each token is a three-dimensional embedding vector:\\ntorch.Size([2, 6, 3])\\nThe following CausalAttention class is similar to the SelfAttention class we imple-\\nmented earlier, except that we added the dropout and causal mask components.\\n \\nTwo inputs with six tokens each; each \\ntoken has embedding dimension 3.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 102, 'page_label': '81'}, page_content='813.5 Hiding future words with causal attention\\nclass CausalAttention(nn.Module):\\n    def __init__(self, d_in, d_out, context_length,\\n                dropout, qkv_bias=False):\\n        super().__init__()\\n        self.d_out = d_out\\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.dropout = nn.Dropout(dropout)           \\n        self.register_buffer(\\n           \\'mask\\',\\n           torch.triu(torch.ones(context_length, context_length),\\n           diagonal=1)\\n        )            \\n    def forward(self, x):\\n        b, num_tokens, d_in = x.shape                  \\n        keys = self.W_key(x)\\n        queries = self.W_query(x)\\n        values = self.W_value(x)\\n        attn_scores = queries @ keys.transpose(1, 2)   \\n        attn_scores.masked_fill_(                   \\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1\\n        )\\n        attn_weights = self.dropout(attn_weights)\\n        context_vec = attn_weights @ values\\n        return context_vec\\nWhile all added code lines should be fa miliar at this point, we now added a self\\n.register_buffer() ca l l in t he __init__ method. The use of register_buffer in\\nPyTorch is not strictly necessary for all use cases but offers several advantages here. For\\ninstance, when we use the CausalAttention class in our LLM, buffers are automati-\\ncally moved to the appropriate device (CPU or GPU) along with our model, which will\\nbe relevant when training our LLM. This  means we don’t need to manually ensure\\nthese tensors are on the same device as yo ur model parameters, avoiding device mis-\\nmatch errors.\\n We can use the CausalAttention class as follows, similar to SelfAttention\\npreviously:\\ntorch.manual_seed(123)\\ncontext_length = batch.shape[1]\\nca = CausalAttention(d_in, d_out, context_length, 0.0)\\ncontext_vecs = ca(batch)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nListing 3.3 A compact causal attention class\\nCompared \\nto the previous \\nSelfAttention_v1 \\nclass, we added a \\ndropout layer.\\nThe register_buffer call is also a new addition \\n(more information is provided in the following text).\\nWe transpose \\ndimensions 1 and 2, \\nkeeping the batch \\ndimension at the first \\nposition (0).\\nIn PyTorch, operations\\nwith a trailing underscore\\nare performed in-place,\\navoiding unnecessary\\nmemory copies.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 103, 'page_label': '82'}, page_content='82 CHAPTER 3 Coding attention mechanisms\\nThe resulting context vector is a three-di mensional tensor where each token is now\\nrepresented by a two-dimensional embedding:\\ncontext_vecs.shape: torch.Size([2, 6, 2])\\nFigure 3.23 summarizes what we have accomp lished so far. We have focused on the\\nconcept and implementation of  causal attention in neural networks. Next, we will\\nexpand on this concept and implement a multi-head attention module that imple-\\nments several causal attent ion mechanisms in parallel.\\n3.6 Extending single-head attention to multi-head \\nattention\\nOur final step will be to extend the previously implemented causal attention class over\\nmultiple heads. This is also called multi-head attention. \\n The term “multi-head” refers to dividi ng the attention mechanism into multiple\\n“heads,” each operating independently. In this context, a single causal attention mod-\\nule can be considered single-head attention,  where there is only one set of attention\\nweights processing the input sequentially.\\n We will tackle this expansion from causal  attention to multi-head attention. First,\\nwe will intuitively build a multi-head at tention module by stacking multiple Causal-\\nAttention modules. Then we will then impl ement the same multi-head attention\\nmodule in a more complicated but more computationally efficient way.\\n3.6.1 Stacking multiple single-head attention layers\\nIn practical terms, implemen ting multi-head attention involves creating multiple\\ninstances of the self-attention mechanism (see figure 3.18), each with its own weights,\\nand then combining their outputs. Using multiple instances of  the self-attention\\nmechanism can be computationally intensive, but it’s crucial for the kind of complex\\npattern recognition that models like transformer-based LLMs are known for. \\n1) Simpliﬁed\\nself-attention 3) Causal attention2) Self-attention 4) Multi-head\\nattention\\nIn the previous section,\\nwe implemented a\\nself-attention mechanism\\nwith trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 104, 'page_label': '83'}, page_content='833.6 Extending single-head attention to multi-head attention\\n Figure 3.24 illustrates the structure of  a multi-head attention module, which con-\\nsists of multiple single-head attention modules, as previously depicted in figure 3.18,\\nstacked on top of each other.\\nAs mentioned before, the main idea behind multi-head attention is to run the attention\\nmechanism multiple times (in parallel) with different, learned linear projections—the\\nresults of multiplying the input data (like the query, key, and value vectors in attention\\nmechanisms) by a weight matrix. In code, we can achieve this by implementing a sim-\\nple \\nMultiHeadAttentionWrapper class that stacks multiple  instances of our previously\\nimplemented CausalAttention module.\\n \\n \\n \\n0.7 0.2 0.1\\nInputs\\nFor multi-head attention with\\ntwo heads, we obtain two\\nattention weight matrices,\\nincluding causal and dropout\\nmasks.\\nQueries Keys Values\\n-0.7 -0.1\\nContext\\nvectors\\nWeight\\nmatrix\\nWeight\\nmatrix\\nWeight\\nmatrix\\nThe embedded input tokens\\nremain unchanged.\\nInstead of one query matrix , weQ\\nhave two query matrices Q1 and .Q2\\nWe now have two sets of\\ncontext vectors, Z1 and .Z2\\n0.7 0.4\\n0.7 0.4-0.7 -0.1\\nCombined\\ncontext\\nvectors Z\\nThe context vector in Z2\\ncorresponding to the ﬁfth\\ninput that was highlighted\\nin the inputs .X\\nThe values of the 5th row (input)\\nare shown as an example.\\nInstead of one value weight\\nmatrix W\\nv in single-head\\nattention, use two matrices\\nW\\nv1 and .Wv2\\nFigure 3.24 The multi-head attention module includes two single-head attention modules stacked on top of \\neach other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention \\nmodule with two heads, we now have two value weight matrices: Wv1 and Wv2. The same applies to the other \\nweight matrices, WQ and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single \\ncontext vector matrix Z.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 105, 'page_label': '84'}, page_content='84 CHAPTER 3 Coding attention mechanisms\\nclass MultiHeadAttentionWrapper(nn.Module):\\n    def __init__(self, d_in, d_out, context_length,\\n                 dropout, num_heads, qkv_bias=False):\\n        super().__init__()\\n        self.heads = nn.ModuleList(\\n            [CausalAttention(\\n                 d_in, d_out, context_length, dropout, qkv_bias\\n             ) \\n             for _ in range(num_heads)]\\n        )\\n    def forward(self, x):\\n        return torch.cat([head(x) for head in self.heads], dim=-1)\\nFor example, if we use this MultiHeadAttentionWrapper class with two attention heads\\n(via num_heads=2) and CausalAttention output dimension d_out=2, we get a four-\\ndimensional context vector (d_out*num_heads=4), as depicted in figure 3.25.\\nTo illustrate this further with a concrete example, we can use the MultiHeadAttention-\\nWrapper class similar to the CausalAttention class before:\\ntorch.manual_seed(123)\\ncontext_length = batch.shape[1] # This is the number of tokens\\nd_in, d_out = 3, 2\\nListing 3.4 A wrapper class to implement multi-head attention\\nMulti-head attention\\nZ2\\nConcatenated\\ncontext vector\\nmatrices\\nZ\\nZ2Z1\\nd_in = 3 d_out = 2 d_out = 4\\nTwo attention heads produces a\\ntensor stacking two matrices that\\nrepresent the context vectors.\\nInputs\\nX\\nZ1\\nContext\\nvector\\nmatrices\\nChoosing an embedding dimension of 2 (d_out = 2)\\nfor the context vectors results in a ﬁnal embedding\\ndimension of 4 (d_out × num_heads).\\nFigure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of \\nattention heads (num_heads). If we set num_heads=2, as in this example, we obtain \\na tensor with two sets of context vector matrices. In each context vector matrix, the \\nrows represent the context vectors corresponding to the tokens, and the columns \\ncorrespond to the embedding dimension specified via d_out=4. We concatenate these \\ncontext vector matrices along the column dimension. Since we have two attention \\nheads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 106, 'page_label': '85'}, page_content='853.6 Extending single-head attention to multi-head attention\\nmha = MultiHeadAttentionWrapper(\\n    d_in, d_out, context_length, 0.0, num_heads=2\\n)\\ncontext_vecs = mha(batch)\\nprint(context_vecs)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nThis results in the following tensor representing the context vectors:\\ntensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\\n         [-0.5874,  0.0058,  0.5891,  0.3257],\\n         [-0.6300, -0.0632,  0.6202,  0.3860],\\n         [-0.5675, -0.0843,  0.5478,  0.3589],\\n         [-0.5526, -0.0981,  0.5321,  0.3428],\\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\\n         [-0.5874,  0.0058,  0.5891,  0.3257],\\n         [-0.6300, -0.0632,  0.6202,  0.3860],\\n         [-0.5675, -0.0843,  0.5478,  0.3589],\\n         [-0.5526, -0.0981,  0.5321,  0.3428],\\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\\ncontext_vecs.shape: torch.Size([2, 6, 4])\\nThe first dimension of the resulting context_vecs tensor is 2 since we have two input\\ntexts (the input texts are du plicated, which is why the co ntext vectors are exactly the\\nsame for those). The second dimension refers to the 6 tokens in each input. The third\\ndimension refers to the four-dimensional embedding of each token.\\nUp to this point, we have implemented a MultiHeadAttentionWrapper that combined\\nmultiple single-head attention modules. However, these are processed sequentially via\\n[head(x) for head in self.heads] in the forward method. We can improve this\\nimplementation by processing the heads in parallel. One way to achieve this is by com-\\nputing the outputs for all attention heads simultaneously via matrix multiplication.\\nExercise 3.2 Returning two-dimensional embedding vectors \\nChange the input arguments for the MultiHeadAttentionWrapper(...,  num_\\nheads=2) call such that the output context vectors are two-dimensional instead of\\nfour dimensional while keeping the setting num_heads=2. Hint: You don’t have to\\nmodify the class implementation; you ju st have to change one of the other input\\narguments.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 107, 'page_label': '86'}, page_content='86 CHAPTER 3 Coding attention mechanisms\\n3.6.2 Implementing multi-head attention with weight splits\\nSo far, we have created a MultiHeadAttentionWrapper to implement multi-head\\nattention by stacking multiple single-head attention modules. This was done by instan-\\ntiating and combining several CausalAttention objects.\\n Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\\nCausalAttention, we can combine these concepts into a single MultiHeadAttention\\nclass. Also, in addition to merging the MultiHeadAttentionWrapper with the Causal-\\nAttention code, we will make some other mo difications to implement multi-head\\nattention more efficiently.\\n In the MultiHeadAttentionWrapper, multiple heads are implemented by creating\\na list of CausalAttention objects ( self.heads), each representing a separate atten-\\ntion head. The CausalAttention class independently performs the attention mecha-\\nnism, and the results from each head are concatenated. In contrast, the following\\nMultiHeadAttention class integrates the multi-head functionality within a single class.\\nIt splits the input into multiple heads by reshaping the projected query, key, and value\\ntensors and then combines the results from these heads after computing attention.\\n Let’s take a look at the MultiHeadAttention class before we discuss it further.\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self, d_in, d_out, \\n                 context_length, dropout, num_heads, qkv_bias=False):\\n        super().__init__()\\n        assert (d_out % num_heads == 0), \\\\\\n            \"d_out must be divisible by num_heads\"\\n        self.d_out = d_out\\n        self.num_heads = num_heads\\n        self.head_dim = d_out // num_heads   \\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\\n        self.out_proj = nn.Linear(d_out, d_out)   \\n        self.dropout = nn.Dropout(dropout)\\n        self.register_buffer(\\n            \"mask\",\\n            torch.triu(torch.ones(context_length, context_length),\\n                       diagonal=1)\\n        )\\n    def forward(self, x):\\n        b, num_tokens, d_in = x.shape\\n        keys = self.W_key(x)        \\n        queries = self.W_query(x)   \\n        values = self.W_value(x)    \\nListing 3.5 An efficient multi-head attention class\\nReduces the projection \\ndim to match the \\ndesired output dim\\nUses a Linear \\nlayer to combine \\nhead outputs\\nTensor shape: (b, \\nnum_tokens, d_out)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 108, 'page_label': '87'}, page_content='873.6 Extending single-head attention to multi-head attention\\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \\n        queries = queries.view(                                             \\n            b, num_tokens, self.num_heads, self.head_dim                    \\n        )                                                                   \\n        keys = keys.transpose(1, 2)         \\n        queries = queries.transpose(1, 2)   \\n        values = values.transpose(1, 2)     \\n        attn_scores = queries @ keys.transpose(2, 3)  \\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   \\n  \\n        attn_scores.masked_fill_(mask_bool, -torch.inf)    \\n        attn_weights = torch.softmax(\\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\\n        attn_weights = self.dropout(attn_weights)\\n        context_vec = (attn_weights @ values).transpose(1, 2)  \\n        \\n        context_vec = context_vec.contiguous().view(\\n            b, num_tokens, self.d_out\\n        )\\n        context_vec = self.out_proj(context_vec)   \\n        return context_vec\\nEven though the reshaping ( .view) and transposing ( .transpose) of tensors inside\\nthe MultiHeadAttention class looks very mathematically complicated, the Multi-\\nHeadAttention class implements the same concept as the MultiHeadAttention-\\nWrapper earlier. \\n On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\\nmultiple single-head attention layers that we combined into a multi-head attention\\nlayer. The MultiHeadAttention class takes an integrated approach. It starts with a\\nmulti-head layer and then internally splits this layer into individual attention heads, as\\nillustrated in figure 3.26.\\n The splitting of the query, key, and value tensors is achieved through tensor reshap-\\ning and transposing operations using PyTorch’s .view and .transpose methods. The\\ninput is first transformed (via linear layers for queries, keys, and values) and then\\nreshaped to represent multiple heads. \\n The key operation is to split the d_out dimension into num_heads and head_dim,\\nwhere head_dim = d_out / num_heads. This splitting is then achieved using the .view\\nmethod: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension\\n(b, num_tokens, num_heads, head_dim).\\nWe implicitly\\nsplit the matrix\\nby adding a\\nnum_heads\\ndimension. Then\\nwe unroll the\\nlast dim: (b,\\nnum_tokens,\\nd_out) -> (b,\\nnum_tokens,\\nnum_heads,\\nhead_dim).\\nTransposes from shape (b, num_tokens, \\nnum_heads, head_dim) to (b, num_heads, \\nnum_tokens, head_dim)\\nComputes\\ndot product\\nfor each head\\nMasks \\ntruncated to \\nthe number \\nof tokens\\nUses the \\nmask to fill \\nattention \\nscores\\nTensor shape: \\n(b, num_tokens, \\nn_heads, \\nhead_dim)\\nCombines heads, where self.d_out \\n= self.num_heads * self.head_dim\\nAdds an optional \\nlinear projection\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 109, 'page_label': '88'}, page_content='88 CHAPTER 3 Coding attention mechanisms\\nThe tensors are then transposed to bring the num_heads dimension before the num_\\ntokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim). This\\ntransposition is crucial for correctly aligning the queries, keys, and values across the\\ndifferent heads and performing batched matrix multiplications efficiently.\\n To illustrate this batched matrix mult iplication, suppose we have the following\\ntensor:\\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   \\n                    [0.8993, 0.0390, 0.9268, 0.7388],\\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\\n                   [[0.0772, 0.3565, 0.1479, 0.5331],\\n                    [0.4066, 0.2318, 0.4545, 0.9737],\\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\\nWeight\\nmatrix\\nWeight\\nmatrix\\nInputs X\\nQueries\\nQueries\\nInputs Queries Queries\\nWeight\\nmatrix Queries\\nPerform two matrix multiplications to obtain\\nthe two query matrices, Q1 and .Q2\\nObtain queries with only oneQ\\nmatrix multiplication.\\nThen split queries intoQ\\nQ1 and .Q2\\nFigure 3.26 In the MultiHeadAttentionWrapper class with two attention heads, \\nwe initialized two weight matrices, Wq1 and Wq2, and computed two query matrices, Q1 \\nand Q2 (top). In the MultiheadAttention class, we initialize one larger weight matrix \\nWq, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and \\nthen split the query matrix into Q1 and Q2 (bottom). We do the same for the keys and \\nvalues, which are not shown to reduce visual clutter.\\nThe shape of this \\ntensor is (b, num_heads, \\nnum_tokens, head_dim) \\n= (1, 2, 3, 4).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 110, 'page_label': '89'}, page_content='893.6 Extending single-head attention to multi-head attention\\nNow we perform a batched matr ix multiplication between the tensor itself and a view\\nof the tensor where we transposed the last two dimensions, num_tokens and head_dim:\\nprint(a @ a.transpose(2, 3))\\nThe result is\\ntensor([[[[1.3208, 1.1631, 1.2879],\\n          [1.1631, 2.2150, 1.8424],\\n          [1.2879, 1.8424, 2.0402]],\\n         [[0.4391, 0.7003, 0.5903],\\n          [0.7003, 1.3737, 1.0620],\\n          [0.5903, 1.0620, 0.9912]]]])\\nIn this case, the matrix multiplication im plementation in PyTorch handles the four-\\ndimensional input tensor so that the matrix multiplication is carried out between the two\\nlast dimensions (num_tokens, head_dim) and then repeated for the individual heads. \\n For instance, the preceding becomes a more compact way to compute the matrix\\nmultiplication for each head separately:\\nfirst_head = a[0, 0, :, :]\\nfirst_res = first_head @ first_head.T\\nprint(\"First head:\\\\n\", first_res)\\nsecond_head = a[0, 1, :, :]\\nsecond_res = second_head @ second_head.T\\nprint(\"\\\\nSecond head:\\\\n\", second_res)\\nThe results are exactly the sa me results as those we obta ined when using the batched\\nmatrix multiplication print(a @ a.transpose(2, 3)):\\nFirst head:\\n tensor([[1.3208, 1.1631, 1.2879],\\n        [1.1631, 2.2150, 1.8424],\\n        [1.2879, 1.8424, 2.0402]])\\nSecond head:\\n tensor([[0.4391, 0.7003, 0.5903],\\n        [0.7003, 1.3737, 1.0620],\\n        [0.5903, 1.0620, 0.9912]])\\nContinuing with MultiHeadAttention, after computing the attention weights and con-\\ntext vectors, the context vectors from all heads are transposed back to the shape (b,\\nnum_tokens, num_heads, head_dim). These vectors are then reshaped (flattened) into\\nthe shape (b, num_tokens, d_out), effectively combining the outputs from all heads.\\n Additionally, we added an output projection layer ( self.out_proj) to Multi-\\nHeadAttention after combining the heads, which is not present in the Causal-\\nAttention class. This output projection layer is not strictly necessary (see appendix B for\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 111, 'page_label': '90'}, page_content='90 CHAPTER 3 Coding attention mechanisms\\nmore details), but it is commonly used in  many LLM architectures, which is why I\\nadded it here for completeness.\\n Even though the MultiHeadAttention class looks more complicated than the\\nMultiHeadAttentionWrapper due to the additional re shaping and transposition of\\ntensors, it is more efficient. The reason is that we only need one matrix multiplication\\nto compute the keys, for instance, keys = self.W_key(x) (the same is true for the que-\\nries and values). In the MultiHeadAttentionWrapper, we needed to repeat this matrix\\nmultiplication, which is computationally on e of the most expensive steps, for each\\nattention head.\\n The MultiHeadAttention class can be used similar to the SelfAttention and\\nCausalAttention classes we implemented earlier:\\ntorch.manual_seed(123)\\nbatch_size, context_length, d_in = batch.shape\\nd_out = 2\\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\\ncontext_vecs = mha(batch)\\nprint(context_vecs)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nThe results show that the output dime nsion is directly controlled by the d_out\\nargument:\\ntensor([[[0.3190, 0.4858],\\n         [0.2943, 0.3897],\\n         [0.2856, 0.3593],\\n         [0.2693, 0.3873],\\n         [0.2639, 0.3928],\\n         [0.2575, 0.4028]],\\n        [[0.3190, 0.4858],\\n         [0.2943, 0.3897],\\n         [0.2856, 0.3593],\\n         [0.2693, 0.3873],\\n         [0.2639, 0.3928],\\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\\ncontext_vecs.shape: torch.Size([2, 6, 2])\\nWe have now implemented the MultiHeadAttention class that we will use when we\\nimplement and train the LLM. Note that wh ile the code is fully functional, I used\\nrelatively small embedding sizes and number s of attention heads to keep the outputs\\nreadable.\\n For comparison, the smallest GPT-2 model (117 million parameters) has 12 atten-\\ntion heads and a context vector embedding size of 768. The largest GPT-2 model (1.5\\nbillion parameters) has 25 attention head s and a context vector embedding size of\\n1,600. The embedding sizes of the token inputs and context embeddings are the same\\nin GPT models (d_in = d_out).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 112, 'page_label': '91'}, page_content='91Summary\\nSummary\\n\\uf0a1 Attention mechanisms transform input el ements into enhanced context vector\\nrepresentations that incorporate information about all inputs.\\n\\uf0a1 A self-attention mech anism computes the context ve ctor representation as a\\nweighted sum over the inputs.\\n\\uf0a1 In a simplified at tention mechanism, the attent ion weights are computed via\\ndot products.\\n\\uf0a1 A dot product is a concise way of multiplying two vectors element-wise and then\\nsumming the products.\\n\\uf0a1 Matrix multiplications, while not strictly required, help us implement computa-\\ntions more efficiently and compactly by replacing nested for loops. \\n\\uf0a1 In self-attention mechanisms used in LLMs, also called scaled-dot product\\nattention, we include trainable weight matrices to compute intermediate trans-\\nformations of the inputs: queries, values, and keys.\\n\\uf0a1 When working with LLMs that read and generate text from left to right, we add\\na causal attention mask to prevent the LLM from accessing future tokens.\\n\\uf0a1 In addition to causal attention masks to  zero-out attention weights, we can add\\na dropout mask to reduce overfitting in LLMs.\\n\\uf0a1 The attention modules in transformer-based LLMs involve multiple instances of\\ncausal attention, which is called multi-head attention.\\n\\uf0a1 We can create a multi-head attention mo dule by stacking mu ltiple instances of\\ncausal attention modules.\\n\\uf0a1 A more efficient way of creating multi-head attention modules involves batched\\nmatrix multiplications.\\nExercise 3.3 Initializing GPT-2 size attention modules\\nUsing the MultiHeadAttention class, initialize a multi- head attention module that\\nhas the same number of attention heads as the smallest GPT-2 model (12 attention\\nheads). Also ensure that you use the respective input and output embedding sizes\\nsimilar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a\\ncontext length of 1,024 tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 113, 'page_label': '92'}, page_content='92\\nImplementing\\na GPT model from\\nscratch to generate text\\nYou’ve already learned and coded the multi-head attention  mechanism, one of the\\ncore components of LLMs. Now, we will co de the other building blocks of an LLM\\nand assemble them into a GPT-like model that we will train in  the next chapter to\\ngenerate human-like text.\\n T h e  L L M  a r c h i t e c t u r e  r e f e r e n c e d  i n  f i g u r e  4 . 1 ,  c o n s i s t s  o f  s e v e r a l  b u i l d i n g\\nblocks. We will begin with a top-down view  of the model architecture before cover-\\ning the individual components in more detail.\\nThis chapter covers\\n\\uf0a1 Coding a GPT-like large language model (LLM) \\nthat can be trained to generate human-like text\\n\\uf0a1 Normalizing layer activations to stabilize neural \\nnetwork training\\n\\uf0a1 Adding shortcut connections in deep neural \\nnetworks \\n\\uf0a1 Implementing transformer blocks to create GPT \\nmodels of various sizes\\n\\uf0a1 Computing the number of parameters and \\nstorage requirements of GPT models\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 114, 'page_label': '93'}, page_content='934.1 Coding an LLM architecture\\n4.1 Coding an LLM architecture\\nLLMs, such as GPT (which stands for generative pretrained transformer ), are large deep\\nneural network architectures designed to ge nerate new text one word (or token) at a\\ntime. However, despite their size, the model architecture is less complicated than you\\nmight think, since many of its components are repeated, as we will see later. Figure 4.2\\nprovides a top-down view of a GPT-like LLM, with its main components highlighted.\\n We have already covered se veral aspects of the LLM ar chitecture, such as input\\ntokenization and embedding and the masked  multi-head attention module. Now, we\\nwill implement the core structure of the GPT model, including its transformer blocks,\\nwhich we will later train to generate human-like text.\\n Previously, we used smaller embedding dimensions for simplicity, ensuring that the\\nconcepts and examples could comfortably fit on a single page. Now, we are scaling up\\nto the size of a small GPT-2 model, specifically the smallest version with 124 million\\nparameters, as described in “Language Models Are Unsupervised Multitask Learners,”\\nby Radford et al. ( https:/ /mng.bz/yoBq) .  N o t e  t h a t  w h i l e  t h e  o r i g i n a l  r e p o r t  m e n -\\ntions 117 million parameters, this was later corrected. In chapter 6, we will focus on\\nloading pretrained weights into our implemen tation and adapting it for larger GPT-2\\nmodels with 345, 762, and 1,542 million parameters. \\n In the context of deep learning and LLM s like GPT, the term “parameters” refers\\nto the trainable weights of the model. Thes e weights are essentially the internal vari-\\nables of the model that are adjusted and optimized during the training process to\\nminimize a specific loss func tion. This optimization allo ws the model to learn from\\nthe training data.\\nA building block of an LLM\\nthat we previously implemented\\nin the previous chapter\\nIn this chapter, we will\\nnow implement the\\nother parts of the LLM\\nIn the next chapter, we\\nadd the training loop\\nand pretrain the LLM\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 4.1 The three main stages of coding an LLM. This chapter focuses on step 3 of stage 1: implementing the \\nLLM architecture.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 115, 'page_label': '94'}, page_content='94 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nFor example, in a neural network layer that is represented by a 2,048 × 2,048–dimensional\\nmatrix (or tensor) of weights, each element of this matrix is a parameter. Since there\\nare 2,048 rows and 2,048 columns, the total number of parameters in this layer is 2,048\\nmultiplied by 2,048, which equals 4,194,304 parameters.\\nGPT-2 vs. GPT-3 \\nNote that we are focusing on GPT-2 because OpenAI has made the weights of the\\npretrained model publicly available, which we will load into our implementation in\\nchapter 6. GPT-3 is fundamentally the same  in terms of model architecture, except\\nthat it is scaled up from 1.5 billion param eters in GPT-2 to 175 billion parameters\\nin GPT-3, and it is trained on more data. As of this writing, the weights for GPT-3\\nare not publicly available. GPT-2 is also a better choice for learning how to imple-\\nment LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a\\nGPU cluster for training and inference. According to Lambda Labs (https:/ /lambdalabs\\n.com/), it would take 355 years to train GPT-3 on a single V100 datacenter GPU\\nand 665 years on a consumer RTX 8000 GPU.\\nOutput layers\\nTokenized text\\nEmbedding layers\\nTransformer block\\nMasked multi-head\\nattention\\nEmbedding layers and\\ntokenization were\\ncovered in hapter 2.c\\nWe implemented the\\nattention module in\\nthe previous chapter.\\nIn this chapter, we\\nimplement a GPT model\\nincluding all of its\\nsubcomponents.\\nGPT\\nmodel\\nTransformer blocks are\\na key component of\\nGPT-like LLMs.\\nThe goal is to generate new\\ntext one word at a time.\\n“Every eﬀort moves you ”forward\\n“Every eﬀort moves you”\\nFigure 4.2 A GPT model. In addition to the embedding layers, it consists of one or more \\ntransformer blocks containing the masked multi-head attention module we previously \\nimplemented.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 116, 'page_label': '95'}, page_content='954.1 Coding an LLM architecture\\nWe specify the configuration of the small GPT-2 model via the following Python dictio-\\nnary, which we will use in the code examples later:\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,     # Vocabulary size\\n    \"context_length\": 1024,  # Context length\\n    \"emb_dim\": 768,          # Embedding dimension\\n    \"n_heads\": 12,           # Number of attention heads\\n    \"n_layers\": 12,          # Number of layers\\n    \"drop_rate\": 0.1,        # Dropout rate\\n    \"qkv_bias\": False        # Query-Key-Value bias\\n}\\nIn the GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to\\nprevent long lines of code:\\n\\uf0a1 vocab_size refers to a vocabulary of 50,257 words, as used by the BPE tokenizer\\n(see chapter 2).\\n\\uf0a1 context_length denotes the maximum number of input tokens the model can\\nhandle via the positional embeddings (see chapter 2).\\n\\uf0a1 emb_dim represents the embedding size, transforming each token into a 768-\\ndimensional vector.\\n\\uf0a1 n_heads indicates the count of attention heads in the multi-head attention\\nmechanism (see chapter 3).\\n\\uf0a1 n_layers specifies the number of transfor mer blocks in the model, which we\\nwill cover in the upcoming discussion.\\n\\uf0a1 drop_rate indicates the intensity of the dropout mechanism (0.1 implies a 10%\\nrandom drop out of hidden units) to prevent overfitting (see chapter 3).\\n\\uf0a1 qkv_bias determines whether to include a bias vector in the Linear layers of\\nthe multi-head attention for query, key, and value computations. We will initially\\ndisable this, following the norms of modern LLMs, but we will revisit it in chap-\\nter 6 when we load pretrained GPT-2 weights from OpenAI into our model (see\\nchapter 6).\\nUsing this configuration, we will impl ement a GPT placehol der architecture (Dummy-\\nGPTModel), as shown in figure 4.3. This will provide us with a big-picture view of how\\neverything fits together and what other comp onents we need to code to assemble the\\nfull GPT model architecture.\\n The numbered boxes in figure 4.3 illustra te the order in which we tackle the indi-\\nvidual concepts required to code the final GPT architecture. We will start with step 1,\\na placeholder GPT backbone we will call DummyGPTModel.\\n \\n \\n \\n \\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 117, 'page_label': '96'}, page_content='96 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nimport torch\\nimport torch.nn as nn\\nclass DummyGPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\\n        self.trf_blocks = nn.Sequential(              \\n            *[DummyTransformerBlock(cfg)              \\n              for _ in range(cfg[\"n_layers\"])]        \\n        )                                             \\n        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])    \\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logits\\nListing 4.1 A placeholder GPT model architecture class\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nWe developed a GPT\\nplaceholder model to see\\nthe overall structure of\\nthe model.\\nNext, we will\\nimplement building\\nblocks 2 5.–\\nThen we will combine building\\nblocks 2 5, including the–\\nmulti-head attention\\nmodule from chapter 3,\\ninto a transformer block.\\nFinally, we will use multiple\\ntransformer blocks to\\nimplement the untrained\\nGPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.3 The order in which we code the GPT architecture. We start with the GPT \\nbackbone, a placeholder architecture, before getting to the individual core pieces and \\neventually assembling them in a transformer block for the final GPT architecture.\\nUses a placeholder \\nfor TransformerBlock\\nUses a \\nplaceholder for \\nLayerNorm\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 118, 'page_label': '97'}, page_content='974.1 Coding an LLM architecture\\nclass DummyTransformerBlock(nn.Module):   \\n    def __init__(self, cfg):\\n        super().__init__()\\n    def forward(self, x):    \\n        return x\\nclass DummyLayerNorm(nn.Module):          \\n    def __init__(self, normalized_shape, eps=1e-5):   \\n        super().__init__()\\n    def forward(self, x):\\n        return x\\nThe DummyGPTModel class in this code defines a simplified version of a GPT-like\\nmodel using PyTorch’s neural network module (nn.Module). The model architecture\\nin the DummyGPTModel class consists of token and positional embeddings, dropout,\\na series of transformer blocks ( DummyTransformerBlock), a final layer normalization\\n(DummyLayerNorm), and a linear output layer ( out_head). The configuration is\\npassed in via a Python dictionary, for instance, the GPT_CONFIG_124M dictionary we\\ncreated earlier.\\n The forward method describes the data flow through the model: it computes token\\nand positional embeddings for the input indices, applies dropout, processes the data\\nthrough the transformer blocks, applies no rmalization, and finally produces logits\\nwith the linear output layer.\\n The code in listing 4.1 is already functi onal. However, for now, note that we use\\nplaceholders (DummyLayerNorm and DummyTransformerBlock) for the transformer block\\nand layer normalization, which we will develop later.\\n Next, we will prepare the input data and initialize a new GPT model to illustrate\\nits usage. Building on our coding of the tokenizer (see chapter 2), let’s now con-\\nsider a high-level overview of how data fl ows in and out of a GPT model, as shown in\\nfigure 4.4.\\n To implement these steps, we tokenize a batch consisting of two text inputs for the\\nGPT model using the tiktoken tokenizer from chapter 2:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nbatch = []\\ntxt1 = \"Every effort moves you\"\\ntxt2 = \"Every day holds a\"\\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\\nbatch = torch.stack(batch, dim=0)\\nprint(batch)\\nA simple placeholder class that will be \\nreplaced by a real TransformerBlock later\\nThis block does nothing and \\njust returns its input.\\nA simple placeholder class that will be \\nreplaced by a real LayerNorm later\\nThe parameters here \\nare just to mimic the \\nLayerNorm interface.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 119, 'page_label': '98'}, page_content='98 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nThe resulting token IDs for the two texts are as follows:\\ntensor([[6109,  3626,  6100,   345],   \\n        [6109,  1110,  6622,   257]])  \\nNext, we initialize a new 124-million-parameter DummyGPTModel instance and feed it\\nthe tokenized batch: \\ntorch.manual_seed(123)\\nmodel = DummyGPTModel(GPT_CONFIG_124M)\\nlogits = model(batch)\\nprint(\"Output shape:\", logits.shape)\\nprint(logits)\\nGPT model\\nInput text:\\nToken embeddings:\\nTokenized text:\\nOutput text\\nPostprocessing steps\\nToken IDs:\\nWe tokenize the input\\ntext and convert it into\\ntoken embeddings.\\nOutputs:\\nEvery effort moves you\\nyoumoveseffortEvery\\n345610036266109\\n2.4 2.4 -2.6 -1.3 2.0 1.8 -1.6 2.1\\n-1.2 0.3 -0.1 0.4 0.5 1.6 0.0 1.6\\nThe LLM returns one 768-\\ndimensional output vector\\nfor each 768-dimensional\\ninput token embedding.\\nFor the smallest GPT-2 model,\\neach embedding vector consists\\nof 768 dimensions (only the ﬁrst\\n2 dimensions are shown).\\neffort moves you forward\\nThe goal is to generate\\nthe next word, “forward.”\\nThe number of input tokens matches\\nthe number of output tokens; hence,\\nthe ﬁrst token (“Every”) will not be\\ncontained in the output.\\nFigure 4.4 A big-picture overview showing how the input data is tokenized, embedded, and fed to the GPT model. \\nNote that in our DummyGPTClass coded earlier, the token embedding is handled inside the GPT model. In LLMs, \\nthe embedded input token dimension typically matches the output dimension. The output embeddings here \\nrepresent the context vectors (see chapter 3).\\nThe first row corresponds to the first text, and \\nthe second row corresponds to the second text.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 120, 'page_label': '99'}, page_content='994.2 Normalizing activations with layer normalization\\nThe model outputs, which are commonly referred to as logits, are as follows:\\nOutput shape: torch.Size([2, 4, 50257])\\ntensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\\n         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\\n         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\\n         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\\n        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\\n         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\\n         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\\n         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\\n       grad_fn=<UnsafeViewBackward0>)\\nThe output tensor has two rows corresponding to the two text samples. Each text sam-\\nple consists of four tokens; each token is  a 50,257-dimensional vector, which matches\\nthe size of the tokenizer’s vocabulary.\\n The embedding has 50,257 dimensions because each of these dimensions refers to\\na unique token in the vocabulary. When we implement the postprocessing code, we\\nwill convert these 50,257-dimensional vectors back into token IDs, which we can then\\ndecode into words.\\n Now that we have taken a top-down look at the GPT architecture and its inputs and\\noutputs, we will code the individual placehol ders, starting with the real layer normal-\\nization class that will replace the DummyLayerNorm in the previous code.\\n4.2 Normalizing activations with layer normalization\\nTraining deep neural networks with many  layers can sometimes prove challenging\\ndue to problems like vanishing or exploding gradients. These problems lead to unsta-\\nble training dynamics and make it difficul t for the network to effectively adjust its\\nweights, which means the learning proce ss struggles to find a set of parameters\\n(weights) for the neural network that minimizes the loss function. In other words, the\\nnetwork has difficulty learning the underlyi ng patterns in the data to a degree that\\nwould allow it to make accurate predictions or decisions. \\nNOTE If you are new to neural network tr aining and the concepts of gradi-\\nents, a brief introduction to these concepts can be found in section A.4 in\\nappendix A. However, a deep mathematical understanding of gradients is not\\nrequired to follow the contents of this book.\\nLet’s now implement layer normalization to improve the stability and efficiency of neu-\\nral network training. The main idea behind layer normalization is to adjust the activa-\\ntions (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also\\nknown as unit variance. This adjustment speeds up the convergence to effective\\nweights and ensures consistent, reliable training. In GPT-2 and modern transformer\\narchitectures, layer normalization is typica lly applied before and after the multi-head\\nattention module, and, as we have seen with the \\nDummyLayerNorm placeholder, before\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 121, 'page_label': '100'}, page_content='100 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nthe final output layer. Figure 4.5 provides a visual overview of how layer normalization\\nfunctions.\\nWe can recreate the example shown in figure 4.5 via the following code, where we\\nimplement a neural network layer with five inputs and six outputs that we apply to two\\ninput examples:\\ntorch.manual_seed(123)\\nbatch_example = torch.randn(2, 5)    \\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\\nout = layer(batch_example)\\nprint(out)\\nThis prints the following tensor, where the fi rst row lists the layer outputs for the first\\ninput and the second row lists the layer outputs for the second row:\\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\\n       grad_fn=<ReluBackward0>)\\nThe neural network layer we have coded consists of a Linear layer followed by a non-\\nlinear activation function, ReLU (short for rectified linear unit), which is a standard\\nactivation function in neural netw orks. If you are unfamiliar with ReLU, it simply\\nthresholds negative inputs to 0, ensuring that a layer ou tputs only positive values,\\nwhich explains why the resulting layer outp ut does not contain any negative values.\\nLater, we will use another, more sophisticated activation function in GPT.\\n0.22 0.34 0.00 0.22 0.00 0.00\\n-0.11 0.12 -0.36 -0.24 -1.19\\nLayer inputs, where\\nthe ﬁve values represent\\na single training example\\nLayer\\noutputs\\nApply layer normalization\\n0.61 1.41 -0.87 0.58 -0.87 -0.87\\nMean = 0.13\\nVariance = 0.39\\nMean = 0.00\\nVariance = 1.00\\nZero-centered\\nmean and unit\\nvariance after\\nnormalization\\nFigure 4.5 An illustration of layer normalization where the six outputs of the layer, also \\ncalled activations, are normalized such that they have a 0 mean and a variance of 1.\\nCreates two training \\nexamples with five \\ndimensions (features) each\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 122, 'page_label': '101'}, page_content='1014.2 Normalizing activations with layer normalization\\n Before we apply layer normalization to these outputs, let’s examine the mean and\\nvariance:\\nmean = out.mean(dim=-1, keepdim=True)\\nvar = out.var(dim=-1, keepdim=True)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe output is\\nMean:\\n  tensor([[0.1324],\\n          [0.2170]], grad_fn=<MeanBackward1>)\\nVariance:\\n  tensor([[0.0231],\\n          [0.0398]], grad_fn=<VarBackward0>)\\nThe first row in the mean tensor here contains the mean value for the first input row,\\nand the second output row contains the mean for the second input row. \\n Using keepdim=True in operations like mean or variance calculation ensures that the\\noutput tensor retains the same number of dimensions as the input tensor, even though\\nthe operation reduces the tensor along the dimension specified via dim. For instance,\\nwithout keepdim=True, the returned mean tensor would be a two-dimensional vector\\n[0.1324, 0.2170] instead of a 2 × 1–dimensional matrix [[0.1324], [0.2170]].\\n The dim parameter specifies the dimension along which the calculation of the statis-\\ntic (here, mean or variance) should be performed in a tensor. As figure 4.6 explains, for\\n0.22   0.34   0.00   0.22   0.00   0.00\\n0.21   0.23   0.00   0.51   0.32   0.00\\n0.13\\n0.21\\ndim=1 dim=-1or calculates mean across the\\ncolumn dimension to obtain one mean per row\\nMean\\nInput 1\\nInput 2\\n0.22   0.34   0.00   0.22   0.00   0.00\\n0.21   0.23   0.00   0.51   0.32   0.00\\nInput 1\\nInput 2\\n0.21 0.29 0.00 0.37 0.16 0.00Mean\\ndim=0calculates mean across the row\\ndimension to obtain one mean per column\\nFigure 4.6 An illustration of the dim parameter when calculating the mean \\nof a tensor. For instance, if we have a two-dimensional tensor (matrix) with \\ndimensions [rows, columns], using dim=0 will perform the operation \\nacross rows (vertically, as shown at the bottom), resulting in an output that \\naggregates the data for each column. Using dim=1 or dim=-1 will perform \\nthe operation across columns (horizontally, as shown at the top), resulting in \\nan output aggregating the data for each row. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 123, 'page_label': '102'}, page_content='102 CHAPTER 4 Implementing a GPT model from scratch to generate text\\na two-dimensional tensor (like a matrix), using dim=-1 for operations such as mean or\\nvariance calculation is the same as using dim=1. This is because -1 refers to the tensor’s\\nlast dimension, which corresponds to the columns in a two-dimensional tensor. Later,\\nwhen adding layer normalization to the GPT model, which produces three-dimensional\\ntensors with the shape [batch_size, num_tokens, embedding_size], we can still use\\ndim=-1 for normalization across the last dimension, avoiding a change from dim=1 to\\ndim=2.\\n Next, let’s apply layer normalization to the layer outputs we obtained earlier. The\\noperation consists of subtracting the mean and dividing by the square root of the vari-\\nance (also known as the standard deviation):\\nout_norm = (out - mean) / torch.sqrt(var)\\nmean = out_norm.mean(dim=-1, keepdim=True)\\nvar = out_norm.var(dim=-1, keepdim=True)\\nprint(\"Normalized layer outputs:\\\\n\", out_norm)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nAs we can see based on the results, the no rmalized layer outputs, which now also con-\\ntain negative values, have 0 mean and a variance of 1:\\nNormalized layer outputs:\\n tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\\n       grad_fn=<DivBackward0>)\\nMean:\\n tensor([[-5.9605e-08],\\n        [1.9868e-08]], grad_fn=<MeanBackward1>)\\nVariance:\\n tensor([[1.],\\n        [1.]], grad_fn=<VarBackward0>)\\nNote that the value –5.9605e-08 in the out put tensor is the scientific notation for\\n–5.9605 × 10-8, which is –0.000000059605 in decimal form. This value is very close to 0,\\nbut it is not exactly 0 due to small numeri cal errors that can accumulate because of\\nthe finite precision with which computers represent numbers. \\n To improve readability, we can also turn off the scientific notation when printing\\ntensor values by setting sci_mode to False:\\ntorch.set_printoptions(sci_mode=False)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe output is\\nMean:\\n tensor([[    0.0000],\\n        [    0.0000]], grad_fn=<MeanBackward1>)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 124, 'page_label': '103'}, page_content='1034.2 Normalizing activations with layer normalization\\nVariance:\\n tensor([[1.],\\n        [1.]], grad_fn=<VarBackward0>)\\nSo far, we have coded and applied layer norm alization in a step-by-step process. Let’s\\nnow encapsulate this process in a PyTorch module that we can use in the GPT model\\nlater.\\nclass LayerNorm(nn.Module):\\n    def __init__(self, emb_dim):\\n        super().__init__()\\n        self.eps = 1e-5\\n        self.scale = nn.Parameter(torch.ones(emb_dim))\\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\\n    def forward(self, x):\\n        mean = x.mean(dim=-1, keepdim=True)\\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\\n        return self.scale * norm_x + self.shift\\nThis specific implementation of layer normalization operates on the last dimension of\\nthe input tensor x, which represents the embedding dimension ( emb_dim). The vari-\\nable eps is a small constant (epsilon) added to the variance to prevent division by zero\\nduring normalization. The scale and shift are two trainable parameters (of the\\nsame dimension as the input) that the LLM automatically adjusts during training if it\\nis determined that doing so would improv e the model’s performance on its training\\ntask. This allows the model to learn appropri ate scaling and shifting that best suit the\\ndata it is processing.\\nLet’s now try the LayerNorm module in practice and apply it to the batch input:\\nListing 4.2 A layer normalization class\\nBiased variance \\nIn our variance calculation method, we use an implementation detail by setting\\nunbiased=False. For those curious about what this means, in the variance calcula-\\ntion, we divide by the number of inputs n in the variance formula. This approach does\\nnot apply Bessel’s correction, which typically uses n – 1 instead of n in the denomi-\\nnator to adjust for bias in sample variance estimation. This decision results in a so-\\ncalled biased estimate of the variance. For LLMs, where the embedding dimension n\\nis significantly large, the difference between using n and n – 1 is practically negligible.\\nI chose this approach to ensure compatibility with the GPT-2 model’s normalization\\nlayers and because it reflects TensorFlow’s default behavior, which was used to\\nimplement the original GPT- 2 model. Using a similar se tting ensures our method is\\ncompatible with the pretrained weights we will load in chapter 6. Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 125, 'page_label': '104'}, page_content='104 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nln = LayerNorm(emb_dim=5)\\nout_ln = ln(batch_example)\\nmean = out_ln.mean(dim=-1, keepdim=True)\\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\\nprint(\"Mean:\\\\n\", mean)\\nprint(\"Variance:\\\\n\", var)\\nThe results show that the la yer normalization code works as expected and normalizes\\nthe values of each of the two inputs such that they have a mean of 0 and a variance of 1:\\nMean:\\n tensor([[    -0.0000],\\n        [     0.0000]], grad_fn=<MeanBackward1>)\\nVariance:\\n tensor([[1.0000],\\n        [1.0000]], grad_fn=<VarBackward0>)\\nWe have now covered two of the building blocks we will need to implement the GPT\\narchitecture, as shown in figure 4.7. Next, we will look at the GELU activation func-\\ntion, which is one of the acti vation functions used in LLMs,  instead of the traditional\\nReLU function we used previously.\\nLayer normalization vs. batch normalization\\nIf you are familiar with batch normalization, a common and traditional normalization\\nmethod for neural networks, you may wonder how it compares to layer normalization.\\nUnlike batch normalization, which normalizes across the batch dimension, layer nor-\\nmalization normalizes across the feature dimension. LLMs often require significant\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we will implement\\ncomponents 3 and 4.\\nWe implemented a GPT\\nplaceholder model to see\\nthe overall structure of\\nthe model.\\nWe implemented\\nlayer normalization.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.7 The building blocks necessary to build the GPT architecture. So far, we \\nhave completed the GPT backbone and layer normalization. Next, we will focus on \\nGELU activation and the feed forward network.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 126, 'page_label': '105'}, page_content='1054.3 Implementing a feed forward network with GELU activations\\n4.3 Implementing a feed forward network \\nwith GELU activations\\nNext, we will implement a small neural netw ork submodule used as part of the trans-\\nformer block in LLMs. We begin by implementing the GELU activation function,\\nwhich plays a crucial role in this neural network submodule. \\nNOTE For additional information on implementing neural networks in\\nPyTorch, see section A.5 in appendix A.\\nHistorically, the ReLU activa tion function has been commonly used in deep learning\\ndue to its simplicity and effectiveness ac ross various neural ne twork architectures.\\nHowever, in LLMs, several ot her activation functions are employed beyond the tradi-\\ntional ReLU. Two notable examples are GELU (Gaussian error linear unit) and SwiGLU\\n(Swish-gated linear unit).\\n GELU and SwiGLU are more complex an d smooth activation functions incorpo-\\nrating Gaussian and sigmoid-gated linear units, respectively. They offer improved per-\\nformance for deep learning models, unlike the simpler ReLU.\\n The GELU activation function can be im plemented in several ways; the exact ver-\\nsion is defined as GELU(x) = x ⋅Φ(x), where Φ(x) is the cumulative distribution func-\\ntion of the standard Gaussian distributi on. In practice, however, it’s common to\\nimplement a computationally cheaper approx imation (the original GPT-2 model was\\nalso trained with this approximation, which was found via curve fitting):\\nIn code, we can implement this function as a PyTorch module.\\nclass GELU(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n    def forward(self, x):\\n        return 0.5 * x * (1 + torch.tanh(\\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \\n            (x + 0.044715 * torch.pow(x, 3))\\n        ))\\ncomputational resources, and the available hardware or the specific use case can\\ndictate the batch size during training or inference. Since layer normalization normal-\\nizes each input independently of the batch size, it offers more flexibility and stability\\nin these scenarios. This is  particularly beneficial for distributed training or when\\ndeploying models in environments where resources are constrained.\\nListing 4.3 An implementation of the GELU activation function\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 127, 'page_label': '106'}, page_content='106 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nNext, to get an idea of what this GELU function looks like and how it compares to the\\nReLU function, let’s plot these functions side by side:\\nimport matplotlib.pyplot as plt\\ngelu, relu = GELU(), nn.ReLU()\\nx = torch.linspace(-3, 3, 100)    \\ny_gelu, y_relu = gelu(x), relu(x)\\nplt.figure(figsize=(8, 3))\\nfor i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\\n    plt.subplot(1, 2, i)\\n    plt.plot(x, y)\\n    plt.title(f\"{label} activation function\")\\n    plt.xlabel(\"x\")\\n    plt.ylabel(f\"{label}(x)\")\\n    plt.grid(True)\\nplt.tight_layout()\\nplt.show()\\nAs we can see in the resulting plot in figu re 4.8, ReLU (right) is a piecewise linear\\nfunction that outputs the input directly if it is positive; otherw ise, it outputs zero.\\nGELU (left) is a smooth, nonlinear function that approximates ReLU but with a non-\\nzero gradient for almost all negative values (except at approximately x = –0.75).\\nThe smoothness of GELU can lead to better op timization properties during training,\\nas it allows for more nuan ced adjustments to the model’ s parameters. In contrast,\\nReLU has a sharp corner at zero (figure 4.18, right), which can sometimes make opti-\\nmization harder, especially in networks that are very deep or have complex architec-\\ntures. Moreover, unlike ReLU, which outp uts zero for any negative input, GELU\\nallows for a small, non-zero output for nega tive values. This characteristic means that\\nduring the training process, neurons that receive negative input can still contribute to\\nthe learning process, albeit to a lesser extent than positive inputs.\\nCreates 100 sample \\ndata points in the \\nrange –3 to 3\\nFigure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis shows the function \\ninputs and the y-axis shows the function outputs.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 128, 'page_label': '107'}, page_content='1074.3 Implementing a feed forward network with GELU activations\\n Next, let’s use the GELU function to implement the small neural network module,\\nFeedForward, that we will be using in the LLM’s transformer block later.\\nclass FeedForward(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.layers = nn.Sequential(\\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\\n            GELU(),\\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\\n        )\\n    def forward(self, x):\\n        return self.layers(x)\\nAs we can see, the FeedForward module is a small neural network consisting of two\\nLinear layers and a GELU activation function. In th e 124-million-parameter GPT\\nmodel, it receives the input batches with to kens that have an embedding size of 768\\neach via the GPT_CONFIG_124M dictionary where GPT_CONFIG_ 124M[\"emb_dim\"] = 768.\\nFigure 4.9 shows how the embedding size is  manipulated inside this small feed for-\\nward neural network when we pass it some inputs.\\nListing 4.4 A feed forward neural network module\\nLinear layer\\nGELU activation\\nLinear layer\\nInput:    (2, 3, 768)\\nOutput: (2, 3, 3072)\\nInput:    (2, 3, 3072)\\nOutput: (2, 3, 3072)\\nInput:    (2, 3, 3072)\\nOutput: (2, 3, 768)\\nThe ﬁrst linear layer\\nincreases the embedding\\ndimension by a factor of 4.\\nInput tensor with\\nshape (2, 3, 768)\\nThe three values represent\\nthe batch size (2), number\\nof tokens (3), and embedding\\nsize (768).\\nOutput tensor with\\nshape (2, 3, 768)\\nThe second linear layer\\ndecreases the embedding\\ndimension by a factor of 4.\\nFigure 4.9 An overview of the connections between the layers of the \\nfeed forward neural network. This neural network can accommodate \\nvariable batch sizes and numbers of tokens in the input. However, the \\nembedding size for each token is determined and fixed when initializing \\nthe weights.Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 129, 'page_label': '108'}, page_content='108 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nFollowing the example in figure  4.9, let’s initialize a new FeedForward module with a\\ntoken embedding size of 768 and feed it a batch input with two samples and three\\ntokens each:\\nffn = FeedForward(GPT_CONFIG_124M)\\nx = torch.rand(2, 3, 768)         \\nout = ffn(x)\\nprint(out.shape)\\nAs we can see, the shape of the output tensor is the same as that of the input tensor:\\ntorch.Size([2, 3, 768])\\nThe FeedForward module plays a crucial role in enhancing the model’s ability to learn\\nfrom and generalize the da ta. Although the input and output dimensions of this\\nmodule are the same, it internally expand s the embedding dimension into a higher-\\ndimensional space through the first linear layer, as illustrated in figure 4.10. This expan-\\nsion is followed by a nonlinear GELU activation and then a contraction back to the orig-\\ninal dimension with the second linear tran sformation. Such a design allows for the\\nexploration of a richer representation space.\\nMoreover, the uniformity in input and outp ut dimensions simplifies the architecture\\nby enabling the stacking of multiple layers , as we will do later, without the need to\\nadjust dimensions between them, thus making the model more scalable.\\nCreates sample input \\nwith batch dimension 2\\n.. .\\n.. .\\n.. .\\nInputs\\nLinear layer 1\\nOutputs\\nLinear layer 2\\nThe inputs are projected into\\na four-times larger space via\\nthe ﬁrst linear layer.\\nThe second linear layer shrinks the\\noutputs by a factor of 4, so that they\\nmatch the original input dimensions.\\nFigure 4.10 An illustration of the expansion and contraction of the layer outputs in the feed \\nforward neural network. First, the inputs expand by a factor of 4 from 768 to 3,072 values. Then, \\nthe second layer compresses the 3,072 values back into a 768-dimensional representation.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 130, 'page_label': '109'}, page_content='1094.4 Adding shortcut connections\\n As figure 4.11 shows, we have now implemented most of the LLM’s building blocks.\\nNext, we will go over the concept of shortc ut connections that we insert between dif-\\nferent layers of a neural network, whic h are important for improving the training\\nperformance in deep neural network architectures.\\n4.4 Adding shortcut connections\\nLet’s discuss the concept behind shortcut connections , also known as skip or residual\\nconnections. Originally, shor tcut connections were prop osed for deep networks in\\ncomputer vision (specifically , in residual networks) to mi tigate the challenge of van-\\nishing gradients. The vanishing gradient pr oblem refers to the issue where gradients\\n(which guide weight updates during training) become progressively smaller as they\\npropagate backward through the layers, making  it difficult to effectively train earlier\\nlayers.\\n Figure 4.12 shows that a shortcut conne ction creates an alternative, shorter path\\nfor the gradient to flow through the network by skipping one or more layers, which is\\nachieved by adding the output of one layer to the output of a later layer. This is why\\nthese connections are also known as skip conn ections. They play a crucial role in pre-\\nserving the flow of gradients during the backward pass in training. \\n In the following list, we implement the neural network in figure 4.12 to see how\\nwe can add shortcut connections in the \\nforward method.\\n \\n \\n \\n \\n \\n \\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we implement shortcut\\nconnections so that we can\\nassemble the transformer block.\\nWe implemented building\\nblocks 2-4, which we need for\\nimplementing a GPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.11 The building blocks necessary to build  the GPT architecture. The black checkmarks \\nindicating those we have already covered.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 131, 'page_label': '110'}, page_content='110 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nclass ExampleDeepNeuralNetwork(nn.Module):\\n    def __init__(self, layer_sizes, use_shortcut):\\n        super().__init__()\\n        self.use_shortcut = use_shortcut\\n        self.layers = nn.ModuleList([      \\nListing 4.5 A neural network to illustrate shortcut connections\\nGELU\\nLinear\\nGELU\\nLinear\\nGELU\\nLinear\\n[1.0,  0.0,  -1.0][1.0,  0.0,  -1.0]\\nGELU\\nLinear\\nGELU\\nLinear\\nLayer 5\\nGELU\\nLinear\\nGELU\\nLinear\\nGELU\\nLinear\\n[1.0,  0.0,  -1.0][1.0,  0.0,  -1.0][1.0,  0.0,  -1.0][1.0,  0.0,  -1.0][1.0,  0.0,  -1.0]\\nGELU\\nLinear\\nGELU\\nLinear\\nDeep neural network Deep neural network with\\nshortcut connections\\nGradient: 0.0002\\nLayer 4\\nGradient: 0.0001\\nLayer 3\\nGradient: 0.0007\\nLayer 2\\nGradient: 0.0013\\nLayer 1\\nGradient: 0.0050\\nLayer 5\\nGradient: 1.32\\nLayer 4\\nGradient: 0.26\\nLayer 3\\nGradient: 0.32\\nLayer 2\\nGradient: 0.20\\nLayer 1\\nGradient: 0.22\\nShortcut connection\\nadds input values to\\nthe outputs of layer 1\\nIn very deep networks, the\\ngradient values in early layers\\nbecome vanishingly small\\nThe shortcut connections\\nhelp with maintaining\\nrelatively large gradient\\nvalues even in early layers\\nFigure 4.12 A comparison between a deep neural network consisting of five layers without (left) and with \\nshortcut connections (right). Shortcut connections involve adding the inputs of a layer to its outputs, effectively \\ncreating an alternate path that bypasses certain layers. The gradients denote the mean absolute gradient at each \\nlayer, which we compute in listing 4.5.\\nImplements \\nfive layers\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 132, 'page_label': '111'}, page_content='1114.4 Adding shortcut connections\\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \\n                          GELU()),\\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \\n                          GELU())\\n        ])\\n    def forward(self, x):\\n        for layer in self.layers:\\n            layer_output = layer(x)        \\n            if self.use_shortcut and x.shape == layer_output.shape:   \\n                x = x + layer_output\\n            else:\\n                x = layer_output\\n        return x\\nThe code implements a deep neural network with five layers, each consisting of a\\nLinear layer and a GELU activation function. In the forward pass, we iteratively pass the\\ninput through the layers and optionally add the shortcut connections if the self.use_\\nshortcut attribute is set to True. \\n Let’s use this code to initialize a ne ural network without shortcut connections.\\nEach layer will be initialized such that it  accepts an example with three input values\\nand returns three output values. The last layer returns a single output value:\\nlayer_sizes = [3, 3, 3, 3, 3, 1]  \\nsample_input = torch.tensor([[1., 0., -1.]])\\ntorch.manual_seed(123)                           \\nmodel_without_shortcut = ExampleDeepNeuralNetwork(\\n    layer_sizes, use_shortcut=False\\n)\\nNext, we implement a function that com putes the gradients in the model’s back-\\nward pass:\\ndef print_gradients(model, x):\\n    output = model(x)            \\n    target = torch.tensor([[0.]])\\n    loss = nn.MSELoss()\\n    loss = loss(output, target)   \\n    loss.backward()         \\nCompute the \\noutput of the \\ncurrent layer\\nCheck if\\nshortcut can\\nbe applied\\nSpecifies random seed \\nfor the initial weights \\nfor reproducibility\\nForward pass\\nCalculates loss based \\non how close the target \\nand output are\\nBackward pass to \\ncalculate the gradients\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 133, 'page_label': '112'}, page_content='112 CHAPTER 4 Implementing a GPT model from scratch to generate text\\n    for name, param in model.named_parameters():\\n        if \\'weight\\' in name:\\n            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\\nThis code specifies a loss function that computes how close the model output and a\\nuser-specified target (here, for simplici ty, the value 0) are.  Then, when calling\\nloss.backward(), PyTorch computes the loss gradient for each layer in the model. We\\ncan iterate through the weight parameters via model.named_parameters(). Suppose we\\nhave a 3 × 3 weight parameter matrix for a given layer. In that case, this layer will have\\n3 × 3 gradient values, and we print the mean absolute gradient of these 3 × 3 gradient\\nvalues to obtain a single gr adient value per layer to compare the gradients between\\nlayers more easily.\\n In short, the .backward() method is a convenient method in PyTorch that com-\\nputes loss gradient s, which are required during mo del training, without implement-\\ning the math for the gradient calculatio n ourselves, thereby making working with\\ndeep neural networks much more accessible. \\nNOTE If you are unfamiliar with the concept of gradients and neural network\\ntraining, I recommend reading sections A.4 and A.7 in appendix A.\\nLet’s now use the print_gradients function and apply it to the model without skip\\nconnections:\\nprint_gradients(model_without_shortcut, sample_input)\\nThe output is\\nlayers.0.0.weight has gradient mean of 0.00020173587836325169\\nlayers.1.0.weight has gradient mean of 0.0001201116101583466\\nlayers.2.0.weight has gradient mean of 0.0007152041653171182\\nlayers.3.0.weight has gradient mean of 0.001398873864673078\\nlayers.4.0.weight has gradient mean of 0.005049646366387606\\nThe output of the print_gradients function shows, the gradients become smaller\\nas we progress from the last layer ( layers.4) to the first layer ( layers.0), which is\\na phenomenon called the vanishing gradient problem.\\n Let’s now instantiate a model with skip connections and see how it compares:\\ntorch.manual_seed(123)\\nmodel_with_shortcut = ExampleDeepNeuralNetwork(\\n    layer_sizes, use_shortcut=True\\n)\\nprint_gradients(model_with_shortcut, sample_input)\\nThe output is\\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 134, 'page_label': '113'}, page_content='1134.5 Connecting attention and linear layers in a transformer block\\nlayers.0.0.weight has gradient mean of 0.22169792652130127\\nlayers.1.0.weight has gradient mean of 0.20694105327129364\\nlayers.2.0.weight has gradient mean of 0.32896995544433594\\nlayers.3.0.weight has gradient mean of 0.2665732502937317\\nlayers.4.0.weight has gradient mean of 1.3258541822433472\\nThe last layer (layers.4) still has a larger gradient than the other layers. However,\\nthe gradient value stabilizes as we progress toward the first layer ( layers.0) and\\ndoesn’t shrink to a vanishingly small value. \\n In conclusion, shortcut connections are important for overcoming the limitations\\nposed by the vanishing gradient problem in  deep neural networ ks. Shortcut connec-\\ntions are a core building block of very larg e models such as LLMs, and they will help\\nfacilitate more effective training by ensuri ng consistent gradient flow across layers\\nwhen we train the GPT model in the next chapter. \\n Next, we’ll connect all of the previously covered concepts (layer normalization,\\nGELU activations, feed forward module, an d shortcut co nnections) in a transformer\\nblock, which is the final building block we need to code the GPT architecture.\\n4.5 Connecting attentio n and linear layers \\nin a transformer block\\nNow, let’s implement the transformer block, a fundamental building block of GPT and\\nother LLM architectures. This block, which is repeated a dozen times in the 124-million-\\nparameter GPT-2 architecture, combines several concepts we have previously covered:\\nmulti-head attention, layer normalization,  dropout, feed forward layers, and GELU\\nactivations. Later, we will connect this transformer block to the remaining parts of the\\nGPT architecture.\\n Figure 4.13 shows a tran sformer block that combines several components, includ-\\ning the masked multi-head attention module (see chapter 3) and the FeedForward\\nmodule we previously implemented (see section 4.3). When a transformer block pro-\\ncesses an input sequence, each element in th e sequence (for example, a word or sub-\\nword token) is represented by a fixed-size vector (in this case, 768 dimensions). The\\noperations within the transformer block, including multi-head attention and feed for-\\nward layers, are designed to transform these vectors in a way that preserves their\\ndimensionality.\\n The idea is that the self-attention mechanism in the multi-head attention block iden-\\ntifies and analyzes relationships between elements in the input sequence. In contrast,\\nthe feed forward network modifies the data individually at each position. This combina-\\ntion not only enables a more nuanced unde rstanding and processing of the input but\\nalso enhances the model’s overall capacity for handling complex data patterns.\\n \\n \\n \\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 135, 'page_label': '114'}, page_content='114 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nShortcut connection\\nLinear layer\\nGELU activation\\nLinear layer\\n[[0.2961, ..., 0.4604],\\n[0.2238, ..., 0.7598],\\n[0.6945, ..., 0.5963],\\n[0.0890, ..., 0.5833]]\\nEvery\\neffort\\nmoves\\nyou\\nEach row is a 768-dimensional\\nvector representing an embedded\\ninput token.\\nA view into the “ eedf\\nforward” block\\nThe transformer\\nblock\\n[[-0.0256, ...,  0.6890],\\n[-0.0178, ...,  0.7431],\\n[ 0.4558, ...,  0.7814],\\n[ 0.0702, ...,  0.7134]]\\nOutputs have the same\\nform and dimensions\\nas the inputs.\\nThis tensor represents an\\nembedded text sample\\nthat serves as input to the\\ntransformer block.\\nDropout\\nDropout\\nThe input tokens to be\\nembedded\\nFigure 4.13 An illustration of a transformer block. Input tokens have been embedded into 768-\\ndimensional vectors. Each row corresponds to one token’s vector representation. The outputs of the \\ntransformer block are vectors of the same dimension as the input, which can then be fed into \\nsubsequent layers in an LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 136, 'page_label': '115'}, page_content='1154.5 Connecting attention and linear layers in a transformer block\\nWe can create the TransformerBlock in code.\\nfrom chapter03 import MultiHeadAttention\\nclass TransformerBlock(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.att = MultiHeadAttention(\\n            d_in=cfg[\"emb_dim\"],\\n            d_out=cfg[\"emb_dim\"],\\n            context_length=cfg[\"context_length\"],\\n            num_heads=cfg[\"n_heads\"], \\n            dropout=cfg[\"drop_rate\"],\\n            qkv_bias=cfg[\"qkv_bias\"])\\n        self.ff = FeedForward(cfg)\\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\\n    def forward(self, x):\\n                           \\n        shortcut = x\\n        x = self.norm1(x)\\n        x = self.att(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut     \\n        shortcut = x        \\n        x = self.norm2(x)\\n        x = self.ff(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut     \\n        return x\\nThe given code defines a TransformerBlock class in PyTorch that includes a multi-head\\nattention mechanism ( MultiHeadAttention) and a feed forward network ( Feed-\\nForward), both configured based on a provided configuration dictionary ( cfg), such\\nas GPT_CONFIG_124M.\\n Layer normalization (LayerNorm) is applied before each of these two components,\\nand dropout is applied after them to regularize the model and prevent overfitting. This\\nis also known as Pre-LayerNorm. Older architectures, such as the original transformer\\nmodel, applied layer normalization after the self-attention and feed forward networks\\ninstead, known as Post-LayerNorm, which often leads to worse training dynamics.\\nListing 4.6 The transformer block component of GPT\\nShortcut connection \\nfor attention block\\nAdd the original \\ninput back\\nShortcut connection \\nfor feed forward block\\nAdds the original \\ninput back\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 137, 'page_label': '116'}, page_content='116 CHAPTER 4 Implementing a GPT model from scratch to generate text\\n The class also implements the forward pa ss, where each component is followed by\\na shortcut connection that adds the input of the block to its output. This critical fea-\\nture helps gradients flow through the ne twork during training and improves the\\nlearning of deep models (see section 4.4).\\n Using the GPT_CONFIG_124M dictionary we defined earlie r, let’s instantiate a trans-\\nformer block and feed it some sample data:\\ntorch.manual_seed(123)\\nx = torch.rand(2, 4, 768)                  \\nblock = TransformerBlock(GPT_CONFIG_124M)\\noutput = block(x)\\nprint(\"Input shape:\", x.shape)\\nprint(\"Output shape:\", output.shape)\\nThe output is\\nInput shape: torch.Size([2, 4, 768])\\nOutput shape: torch.Size([2, 4, 768])\\nAs we can see, the transformer block maintains the input dimensions in its output, indi-\\ncating that the transformer architecture processes sequences of data without altering\\ntheir shape throughout the network.\\n The preservation of shape throughout th e transformer block ar chitecture is not\\nincidental but a crucia l aspect of its design. This desi gn enables its e ffective applica-\\ntion across a wide range of sequence-to-se quence tasks, where each output vector\\ndirectly corresponds to an input vector, maintaining a one-to-one relationship. How-\\never, the output is a context vector that  encapsulates information from the entire\\ninput sequence (see chapter 3). This means that while the physical dimensions of the\\nsequence (length and feature size) remain unchanged as it passes through the trans-\\nformer block, the content of each output vector is re-encoded to integrate contextual\\ninformation from across the entire input sequence.\\n With the transformer block implemented,  we now have all the building blocks\\nneeded to implement the GPT architecture. As illustrated in figure 4.14, the trans-\\nformer block combines layer normalization, the feed forward network, GELU activa-\\ntions, and shortcut connections. As we will eventually see, this transformer block will\\nmake up the main component of the GPT architecture.\\n \\n \\n \\n \\nCreates sample input of shape \\n[batch_size, num_tokens, emb_dim]\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 138, 'page_label': '117'}, page_content='1174.6 Coding the GPT model\\n4.6 Coding the GPT model\\nWe started this chapter with a big-pictur e overview of a GPT architecture that we\\ncalled DummyGPTModel. In this DummyGPTModel code implementation, we showed the\\ninput and outputs to the GPT model, but its building blocks remained a black box\\nusing a DummyTransformerBlock and DummyLayerNorm class as placeholders.\\n Let’s now replace the DummyTransformerBlock and DummyLayerNorm placeholders\\nwith the real TransformerBlock and LayerNorm classes we coded previously to assem-\\nble a fully working version of the original 124-million-parameter version of GPT-2. In\\nchapter 5, we will pretrain a GPT-2 model, and in chapter 6, we will load in the pre-\\ntrained weights from OpenAI.\\n Before we assemble the GPT-2 model in co de, let’s look at its overall structure, as\\nshown in figure 4.15, which includes all the concepts we have covered so far. As we can\\nsee, the transformer block is repeated ma ny times throughout a GPT model architec-\\nture. In the case of the 124-million-parame ter GPT-2 model, it’s  repeated 12 times,\\nwhich we specify via the \\nn_layers entry in the GPT_CONFIG_124M dictionary. This\\ntransform block is repeated 48 times in the largest GPT-2 model with 1,542 million\\nparameters.\\n The output from the final transformer block then goes through a final layer normal-\\nization step before reaching the linear output layer. This layer maps the transformer’s\\noutput to a high-dimensional space (in this case, 50,257 dimensions, corresponding to\\nthe model’s vocabulary size) to predict the next token in the sequence.\\n Let’s now code the architecture in figure 4.15.\\n \\n \\n \\n \\nWe have completed building\\nblocks 6, which we need to1–\\nimplement a GPT model.\\n3) GELU\\nactivation\\n4) Feed forward\\nnetwork\\n2) Layer\\nnormalization\\n5) Shortcut\\nconnections\\nNext, we will assemble\\nthese building blocks to\\ncreate a GPT model.\\n1) GPT\\nbackbone\\n6) Transformer\\nblock\\n7) Final GPT\\narchitecture\\nFigure 4.14 The building blocks necessary to build the GPT architecture. The black \\nchecks indicate the blocks we have completed.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 139, 'page_label': '118'}, page_content='118 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nGPT\\nmodel\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nEvery effort moves you\\n[[-0.0055, ..., -0.4747],\\n[ 0.2663, ..., -0.4224],\\n[ 1.1146, ...,  0.0276],\\n[-0.8239, ..., -0.3993]]\\nFinal LayerNorm\\nLinear output layer\\nThe transformer block\\nis repeated 12 times.\\n12 The GPT code implementation\\nincludes a token embedding\\nand positional embedding layer\\n(see chapter 2).\\nThe last linear layer embeds\\neach token vector into a 50,257-\\ndimensional embedding, where\\n50,257 is the size of the\\nvocabulary.\\nA 4 50,257 dimensional× –\\ntensor\\nThe goal is for these embeddings to\\nbe converted back into text such\\nthat the last row represents the\\nword the model is supposed to\\ngenerate (here, the word “forward”).\\nDropout\\nPositional embedding layer\\nFigure 4.15 An overview of the GPT model architect ure showing the flow of data through the GPT model. \\nStarting from the bottom, tokenized text is first converted into token embeddings, which are then augmented \\nwith positional embeddings. This combined information forms a tensor that is passed through a series of \\ntransformer blocks shown in the center (each containing multi-head attention and feed forward neural network \\nlayers with dropout and layer normalization), which are stacked on top of each other and repeated 12 times.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 140, 'page_label': '119'}, page_content='1194.6 Coding the GPT model\\nclass GPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\\n        \\n        self.trf_blocks = nn.Sequential(\\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\\n       \\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n                                            \\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logits\\nThanks to the TransformerBlock class, the GPTModel class is relatively small and\\ncompact. \\n The __init__ constructor of this GPTModel class initializes the token and posi-\\ntional embedding layers using the configur ations passed in via a Python dictionary,\\ncfg. These embedding layers are responsible for converting input token indices into\\ndense vectors and adding positional information (see chapter 2). \\n Next, the __init__ method creates a se quential stack of TransformerBlock mod-\\nules equal to the number of layers specified in cfg. Following the transformer blocks,\\na LayerNorm layer is applied, standardizing the outputs from the transformer blocks to\\nstabilize the learning process. Finally, a linear output head without bias is defined,\\nwhich projects the transforme r’s output into the vocabulary space of the tokenizer to\\ngenerate logits for each token in the vocabulary. \\n The forward method takes a batch of in put token indices, computes their embed-\\ndings, applies the positional embeddings, passes the sequence through the transformer\\nblocks, normalizes the final output, and then computes the logits, representing the next\\ntoken’s unnormalized probabilities. We will convert these logits into tokens and text\\noutputs in the next section. \\nListing 4.7 The GPT model architecture implementation\\nThe device setting will allow \\nus to train the model on a CPU \\nor GPU, depending on which \\ndevice the input data sits on. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 141, 'page_label': '120'}, page_content='120 CHAPTER 4 Implementing a GPT model from scratch to generate text\\n Let’s now initialize the 124-million-parameter GPT model using the GPT_CONFIG_\\n124M dictionary we pass into the cfg parameter and feed it with the batch text input\\nwe previously created:\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nout = model(batch)\\nprint(\"Input batch:\\\\n\", batch)\\nprint(\"\\\\nOutput shape:\", out.shape)\\nprint(out)\\nThis code prints the contents of the input batch followed by the output tensor:\\nInput batch:\\n tensor([[6109,  3626,  6100,   345],     \\n         [6109,  1110,  6622,   257]])    \\nOutput shape: torch.Size([2, 4, 50257])\\ntensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\\n         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\\n         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\\n         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\\n        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\\n         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\\n         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\\n         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\\n       grad_fn=<UnsafeViewBackward0>)\\nAs we can see, the output tensor has the shape [2, 4, 50257], since we passed in two\\ninput texts with four tokens each. The last dimension, 50257, corresponds to the\\nvocabulary size of the tokenizer. Later, we will see how to convert each of these 50,257-\\ndimensional output vectors back into tokens.\\n Before we move on to coding the func tion that converts the model outputs into\\ntext, let’s spend a bit more time with the model architecture itself and analyze its size.\\nUsing the numel() method, short for “number of elem ents,” we can collect the total\\nnumber of parameters in the model’s parameter tensors:\\ntotal_params = sum(p.numel() for p in model.parameters())\\nprint(f\"Total number of parameters: {total_params:,}\")\\nThe result is\\nTotal number of parameters: 163,009,536\\nNow, a curious reader might notice a discre pancy. Earlier, we spoke of initializing\\na 124-million-parameter GPT mo del, so why is the actual number of parameters\\n163 million?\\nToken IDs of text 1\\nToken IDs of text 2\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 142, 'page_label': '121'}, page_content='1214.6 Coding the GPT model\\n The reason is a concept called weight tying, which was used in the original GPT-2\\narchitecture. It means that the original GPT-2 architectu re reuses the weights from\\nthe token embedding layer in its output layer. To understand better, let’s take a look at\\nthe shapes of the token embedding layer and linear output layer that we initialized on\\nthe model via the GPTModel earlier:\\nprint(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\\nprint(\"Output layer shape:\", model.out_head.weight.shape)\\nAs we can see from the print outputs, the weight tensors for both these layers have the\\nsame shape:\\nToken embedding layer shape: torch.Size([50257, 768])\\nOutput layer shape: torch.Size([50257, 768])\\nThe token embedding and output layers are ve ry large due to the number of rows for\\nthe 50,257 in the tokenizer’s vocabulary. Let’s remove the output layer parameter\\ncount from the total GPT-2 model count according to the weight tying:\\ntotal_params_gpt2 = (\\n    total_params - sum(p.numel()\\n    for p in model.out_head.parameters())\\n)\\nprint(f\"Number of trainable parameters \"\\n      f\"considering weight tying: {total_params_gpt2:,}\"\\n)\\nThe output is\\nNumber of trainable parameters considering weight tying: 124,412,160\\nAs we can see, the model is now only 124 million parameters large, matching the orig-\\ninal size of the GPT-2 model. \\n Weight tying reduces the overall memory footprint and computational complexity\\nof the model. However, in my experience , using separate token embedding and out-\\nput layers results in better training and model performance; he nce, we use separate\\nlayers in our GPTModel implementation. The same is true for modern LLMs. However,\\nwe will revisit and implement the weight tying concept later in chapter 6 when we load\\nthe pretrained weights from OpenAI.\\nExercise 4.1 Number of parameters in feed forward and attention modules \\nCalculate and compare the number of parameters that are contained in the feed for-\\nward module and those that are contained in the multi-head attention module.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 143, 'page_label': '122'}, page_content='122 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nLastly, let’s compute the memory requirements of the 163 million parameters in our\\nGPTModel object:\\ntotal_size_bytes = total_params * 4      \\ntotal_size_mb = total_size_bytes / (1024 * 1024)    \\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")\\nThe result is\\nTotal size of the model: 621.83 MB\\nIn conclusion, by calculating the memory requirements for the 163 million parame-\\nters in our GPTModel object and assuming each parame ter is a 32-bit float taking up 4\\nbytes, we find that the total size of the model amounts to 621.83  MB, illustrating the\\nrelatively large storage capacity required to accommodate even relatively small LLMs. \\n Now that we’ve implemented the GPTModel architecture and saw that it outputs\\nnumeric tensors of shape [batch_size, num_tokens, vocab_size], let’s write the code\\nto convert these output tensors into text.\\n4.7 Generating text\\nWe will now implement the code that conver ts the tensor outputs of the GPT model\\nback into text. Before we get started, let’s briefly review how a generative model like\\nan LLM generates text one word (or token) at a time.\\n Figure 4.16 illustrates th e step-by-step process by which a GPT model generates\\ntext given an input context, such as “Hello, I am.” With each iteration, the input con-\\ntext grows, allowing the model to genera te coherent and cont extually appropriate\\ntext. By the sixth iteration, the model has constructed a complete sentence: “Hello, I\\nam a model ready to help.” We’ve seen that our current \\nGPTModel implementation\\noutputs tensors with shape [batch_size, num_token, vocab_size]. Now the question\\nis: How does a GPT model go from these output tensors to the generated text? \\n The process by which a GPT model goes from output tensors to generated text\\ninvolves several steps, as illustrated in fi gure 4.17. These steps include decoding the\\nExercise 4.2 Initializing larger GPT models \\nWe initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.”\\nWithout making any code modifications besides updating the configuration file, use\\nthe \\nGPTModel class to implement GPT-2 medium  (using 1,024-dimensional embed-\\ndings, 24 transformer blocks, 16 multi- head attention heads), GPT-2 large (1,280-\\ndimensional embeddings, 36 transformer bl ocks, 20 multi-head attention heads),\\nand GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head\\nattention heads). As a bonus, calculate the total number of parameters in each GPT\\nmodel.\\nCalculates the total size in \\nbytes (assuming float32, 4 \\nbytes per parameter)\\nConverts to \\nmegabytes\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 144, 'page_label': '123'}, page_content='1234.7 Generating text\\noutput tensors, selecting tokens based on  a probability distri bution, and converting\\nthese tokens into human-readable text.\\n The next-token generation process detaile d in figure 4.17 illustrates a single step\\nwhere the GPT model generates the next token given its input. In each step, the model\\noutputs a matrix with vectors representing  potential next toke ns. The vector corre-\\nsponding to the next token is extracted and converted into a probability distribution via\\nthe softmax function. Within the vector containing the resulting probability scores, the\\nindex of the highest value is located, which translates to the token ID. This token ID is\\nthen decoded back into text, producing th e next token in the sequence. Finally, this\\ntoken is appended to the previous inputs, forming a new input sequence for the subse-\\nquent iteration. This step-by-step process enables the mode l to generate text sequen-\\ntially, building coherent phrases and sentences from the initial input context.\\n In practice, we repeat this process over many iterations, such as shown in figure 4.16,\\nuntil we reach a user-specified number of  generated tokens. In code, we can imple-\\nment the token-generation process as shown in the following listing.\\n \\n \\nHello , I am a modelHello , I am a modelHello , I am a modelHello , I am a modelHello , I am a modelHello , I am a model\\nThe next generated\\ntoken\\nThe input context\\nfor the model\\nThe token generated in\\nthe previous round is\\nappended to the input for\\nthe next iteration\\nThe input context\\ngrows in each\\niteration\\n1st iteration:\\n2nd iteration:\\n3rd iteration:\\n6th iteration:\\nHello , I am a\\nHello , I am a model ready\\nHello , I am a model ready to help .\\nFigure 4.16 The step-by-step process by which an LLM generates text, one \\ntoken at a time. Starting with an initial input context (“Hello, I am”), the \\nmodel predicts a subsequent token during each iteration, appending it to the \\ninput context for the next round of prediction. As shown, the first iteration \\nadds “a,” the second “model,” and the third “ready,” progressively building \\nthe sentence. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 145, 'page_label': '124'}, page_content='124 CHAPTER 4 Implementing a GPT model from scratch to generate text\\ndef generate_text_simple(model, idx,                \\n                         max_new_tokens, context_size): \\n    for _ in range(max_new_tokens):\\n        idx_cond = idx[:, -context_size:]   \\n        with torch.no_grad():\\n            logits = model(idx_cond)\\n       \\n        logits = logits[:, -1, :]                   \\n        probas = torch.softmax(logits, dim=-1)          \\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \\n        idx = torch.cat((idx, idx_next), dim=1)    \\n    return idx\\nListing 4.8 A function for the GPT model to generate text\\n[[-0.2949, ..., -0.8141],\\n[ 1.2199, ..., -0.3599],\\n[ 1.0446, ...,  0.0020],\\n[-0.4929, ..., -0.6093]]\\nGPT\\n[15496,\\n11,\\n314,\\n716]\\n[-0.4929, ..., -0.6093]]\\n[-0.4929, ..., 2.4812, ..., -0.6093]]\\n0 257 50257\\n1. Encodes text input\\ninto four token IDs\\n2. The GPT model returns a matrix consisting\\nof four vectors (rows), where each vector\\nhas 50257 dimensions (columns).\\n3. Extracts the last vector,\\nwhich corresponds to\\nthe next token that the\\nGPT model is supposed\\nto generate\\n5. Indentifies the index\\nposition of the\\nlargest value, which\\nalso represents the\\ntoken ID\\nIf the largest element\\nis at position 257, we\\nobtain token ID 257.Token ID decoded\\ninto text\\n6. Appends token to the previous\\ninputs for the next round\\n[ 0.0001, ..., 0.0200, ...,  0.0001]]\\n4. Converts logits\\ninto probability\\ndistribution\\nusing the\\nsoftmax\\nfunctionSoftmax\\n257\\nProbabilities:\\nLogits:\\n\"Hello\"\\n\",\"\\n\"I\"\\n\"am\"\\n\"a\"\\n\"a\"\\nFigure 4.17 The mechanics of text generation in a GPT model by showing a single iteration in the token \\ngeneration process. The process begins by encoding the input text into token IDs, which are then fed into the \\nGPT model. The outputs of the model are then converted back into text and appended to the original input text.\\nidx is a (batch, n_tokens) \\narray of indices in the \\ncurrent context.\\nCrops current context if it exceeds the supported context size, \\ne.g., if LLM supports only 5 tokens, and the context size is 10, \\nthen only the last 5 tokens are used as context\\nFocuses only on the last time step, \\nso that (batch, n_token, vocab_size) \\nbecomes (batch, vocab_size)\\nprobas has \\nshape (batch, \\nvocab_size).\\nidx_next has \\nshape (batch, 1).\\nAppends sampled index to the\\nrunning sequence, where idx has\\nshape (batch, n_tokens+1)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 146, 'page_label': '125'}, page_content='1254.7 Generating text\\nThis code demonstrates a simple implemen tation of a generative loop for a lan-\\nguage model using PyTorch. It iterates for a specified number of new tokens to be\\ngenerated, crops the current context to fit the model’s maximum context size, com-\\nputes predictions, and then selects the ne xt token based on the highest probability\\nprediction. \\n To code the generate_text_simple function, we use a softmax function to con-\\nvert the logits into a probability distribution  from which we identify the position with\\nthe highest value via torch.argmax. The softmax function is monotonic, meaning it\\npreserves the order of its inputs when tran sformed into outputs. So, in practice, the\\nsoftmax step is redundant since the position with the highest score in the softmax out-\\nput tensor is the same position in the logi t tensor. In other words, we could apply the\\ntorch.argmax function to the logits tensor directly and get identical results. However,\\nI provide the code for the conversion to illustrate the full process of transforming log-\\nits to probabilities, which can add additional intuition so that the model generates the\\nmost likely next token, which is known as greedy decoding.\\n When we implement the GPT training code in the next chapter, we will use addi-\\ntional sampling techniques to modify the softmax outputs such that the model doesn’t\\nalways select the most likely token. This introduces variability and creativity in the gen-\\nerated text. \\n This process of generating one token ID at a time and appending it to the context\\nusing the generate_text_simple function is further illustrated in figure 4.18. (The\\ntoken ID generation process for each iteration is detailed in figure 4.17.) We generate\\nthe token IDs in an iterative fashion. For instance, in iteration 1, the model is pro-\\nvided with the tokens corresponding to “He llo, I am,” predicts the next token (with\\nID 257, which is “a”), and appends it to th e input. This process is  repeated until the\\nmodel produces the complete sentence “He llo, I am a model ready to help” after six\\niterations.\\n Let’s now try out the \\ngenerate_text_simple function with the \"Hello, I am\" con-\\ntext as model input. First, we encode the input context into token IDs:\\nstart_context = \"Hello, I am\"\\nencoded = tokenizer.encode(start_context)\\nprint(\"encoded:\", encoded)\\nencoded_tensor = torch.tensor(encoded).unsqueeze(0)   \\nprint(\"encoded_tensor.shape:\", encoded_tensor.shape)\\nThe encoded IDs are\\nencoded: [15496, 11, 314, 716]\\nencoded_tensor.shape: torch.Size([1, 4])\\nAdds batch \\ndimension\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 147, 'page_label': '126'}, page_content='126 CHAPTER 4 Implementing a GPT model from scratch to generate text\\nNext, we put the model into .eval() mode. This disables random components like\\ndropout, which are only used during training, and use the generate_text_simple\\nfunction on the encoded input tensor:\\nmodel.eval()                 \\nout = generate_text_simple(\\n    model=model,\\n    idx=encoded_tensor, \\n    max_new_tokens=6, \\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output:\", out)\\nprint(\"Output length:\", len(out[0]))\\nThe resulting output token IDs are\\nOutput: tensor([[15496,    11,   314,   716, 27018, 24086, 47843,\\n30961, 42348,  7267]])\\nOutput length: 10\\nHello, I am a model ready to help.\\nHello\\n[15496, 11, 314, 716]\\n[15496, 11, 314, 716, 257]\\n[15496, 11, 314, 716, 257, 2746]\\nIteration\\n1\\n2\\n3\\nID\\n[257]\\n[2746]\\nPredict\\nam, I a\\nHello am, I a model\\nHello am, I a model\\n[3492]\\nready\\nAppend\\n[15496, ..., 3492, 284, 1037, 13]\\n...\\n6\\n...\\nThe initial tokens (context)\\nprovided as input to the LLM\\nThe output tokens\\nafter six iterations\\n(max_new_tokens=6)\\nThe predicted token\\nID is appended to the\\ncontext for the next round.\\nThe token IDs converted\\ninto a text representation\\nfor illustration purposes\\nFigure 4.18 The six iterations of a token prediction cycle, where the model takes a sequence of initial token IDs \\nas input, predicts the next token, and appends this token to the input sequence for the next iteration. (The token \\nIDs are also translated into their corresponding text for better understanding.) \\nDisables dropout since \\nwe are not training \\nthe model\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 148, 'page_label': '127'}, page_content='127Summary\\nUsing the .decode method of the tokenizer, we can convert the IDs back into text:\\ndecoded_text = tokenizer.decode(out.squeeze(0).tolist())\\nprint(decoded_text)\\nThe model output in text format is\\nHello, I am Featureiman Byeswickattribute argue\\nAs we can see, the model generated gibberish, which is not at all like the coherent text\\nHello, I am a model ready to help. What happened? The reason the model is unable to\\nproduce coherent text is that we haven’t trained it yet. So far, we have only implemented\\nthe GPT architecture and initialized a GPT model instance with initial random weights.\\nModel training is a large topic in itself, and we will tackle it in the next chapter.\\nSummary\\n\\uf0a1 Layer normalization stabilize s training by ensuring th at each layer’s outputs\\nhave a consistent mean and variance.\\n\\uf0a1 Shortcut connections are connections that skip one or more layers by feeding\\nthe output of one layer directly to a de eper layer, which helps mitigate the van-\\nishing gradient problem when training deep neural networks, such as LLMs.\\n\\uf0a1 Transformer blocks are a core structur al component of GPT models, combin-\\ning masked multi-head attention module s with fully connected feed forward\\nnetworks that use the GELU activation function.\\n\\uf0a1 GPT models are LLMs with many repeated  transformer blocks that have mil-\\nlions to billions of parameters.\\n\\uf0a1 GPT models come in various sizes, for example, 124, 345, 762, and 1,542 mil-\\nlion parameters, which we can implement with the same \\nGPTModel Python class.\\n\\uf0a1 The text-generation capability of a GPT-like LLM involves decoding output ten-\\nsors into human-readable text by sequ entially predicting one token at a time\\nbased on a given input context. \\n\\uf0a1 Without training, a GPT model generate s incoherent text, which underscores\\nthe importance of model training for coherent text generation.\\nExercise 4.3 Using separate dropout parameters \\nAt the beginning of this chapter, we defined a global drop_rate setting in the GPT_\\nCONFIG_124M dictionary to set the dropout rate in various places throughout the\\nGPTModel architecture. Change the code to specify a separate dropout value for the\\nvarious dropout layers throughout the model architecture. (Hint: there are three dis-\\ntinct places where we used dropout layers: the embedding layer, shortcut layer, and\\nmulti-head attention module.)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 149, 'page_label': '128'}, page_content='128\\nPretraining\\non unlabeled data\\nThus far, we have implemented the data sampling and attention mechanism and\\ncoded the LLM architecture. It is now time  to implement a training function and\\npretrain the LLM. We will learn about ba sic model evaluation techniques to mea-\\nsure the quality of the generated text, wh ich is a requirement for optimizing the\\nLLM during the training process. Moreover, we will discuss how to load pretrained\\nweights, giving our LLM a so lid starting point for fine-tuning. Figure 5.1 lays out\\nour overall plan, highlighting what we will discuss in this chapter.\\nThis chapter covers\\n\\uf0a1 Computing the training and validation set losses \\nto assess the quality of LLM-generated text \\nduring training\\n\\uf0a1 Implementing a training function and pretraining \\nthe LLM\\n\\uf0a1 Saving and loading model weights to continue \\ntraining an LLM\\n\\uf0a1 Loading pretrained weights from OpenAI\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 150, 'page_label': '129'}, page_content='1295.1 Evaluating generative text models\\n5.1 Evaluating generative text models\\nAfter briefly recapping the text generation from chapter 4, we will set up our LLM for\\ntext generation and then discuss basic ways to evaluate the quality of the generated text.\\nWe will then calculate the training and va lidation losses. Figure 5.2 shows the topics\\ncovered in this chapter, with these first three steps highlighted.\\n \\nWeight parameters \\nIn the context of LLMs and other deep learning models, weights refer to the trainable\\nparameters that the learning process ad justs. These weights are also known as\\nweight parameters or simply parameters. In frameworks like PyTorch, these weights\\nare stored in linear layers; we used these to implement the multi-head attention mod-\\nule in chapter 3 and the GPTModel in chapter 4. After initializing a layer (new_layer\\n= torch.nn.Linear(...)), we can access its weights through the .weight attri-\\nbute, new_layer.weight. Additionally, for convenience, PyTorch allows direct\\naccess to all a model’s trainable parameters, including weights and biases, through\\nthe method model.parameters(), which we will use later when implementing the\\nmodel training.\\nIn this chapter, we will\\npretrain the LLM model.\\nIn the previous chapter, we\\nimplemented a GPT-like\\nLLM architecture.\\nFor the pretraining, we will\\nimplement the training\\nloop along with model\\nevaluation metrics.\\nFinally, we load openly\\navailable pretrained\\nweights into the\\nmodel.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 5.1 The three main stages of coding an LLM. This  chapter focuses on stage 2: pretraining the LLM (step \\n4), which includes implementing the training code (step 5), evaluating the performance (step 6), and saving and \\nloading model weights (step 7).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 151, 'page_label': '130'}, page_content='130 CHAPTER 5 Pretraining on unlabeled data\\n5.1.1 Using GPT to generate text\\nLet’s set up the LLM and briefly recap the text generation process we implemented in\\nchapter 4. We begin by initializing the GPT model that we will later evaluate and train\\nusing the GPTModel class and GPT_CONFIG_124M dictionary (see chapter 4):\\nimport torch\\nfrom chapter04 import GPTModel\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,\\n    \"context_length\": 256,   \\n    \"emb_dim\": 768,\\n    \"n_heads\": 12,\\n    \"n_layers\": 12, \\n    \"drop_rate\": 0.1,      \\n    \"qkv_bias\": False\\n}\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.eval()\\nConsidering the GPT_CONFIG_124M dictionary, the only adjustment we have made com-\\npared to the previous chapter is that we have reduced the context length ( context_\\nlength) to 256 tokens. This modification reduces the computational demands of\\ntraining the model, making it possible to carry out the training on a standard laptop\\ncomputer.\\n Originally, the GPT-2 model with 124 million parameters was configured to handle\\nup to 1,024 tokens. After the training proce ss, we will update the context size setting\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nEvaluate how well\\nthe model performs\\nLoad pretrained weights from\\nOpenAI into our LLM model\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nFigure 5.2 An overview of the topics covered in this chapter. We begin by recapping text generation \\n(step 1) before moving on to discuss basic model evaluation techniques (step 2) and training and \\nvalidation losses (step 3).\\nWe shorten the \\ncontext length from \\n1,024 to 256 tokens.\\nIt’s possible and common \\nto set dropout to 0.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 152, 'page_label': '131'}, page_content=\"1315.1 Evaluating generative text models\\nand load pretrained weights to work with  a model configured for a 1,024-token con-\\ntext length.\\n Using the GPTModel instance, we adopt the generate_text_simple function from\\nchapter 4 and introduce two handy functions: text_to_token_ ids and token_ids_\\nto_text. These functions facilitate the conversi on between text and token represen-\\ntations, a technique we will ut ilize throughout this chapter. \\nFigure 5.3 illustrates a three-step text ge neration process using a GPT model. First,\\nthe tokenizer converts input text into a se ries of token IDs (see chapter 2). Second,\\nthe model receives these token IDs and generates corresponding logits, which are vec-\\ntors representing the probability distributi on for each token in the vocabulary (see\\nchapter 4). Third, these logits are converted back into token IDs, which the tokenizer\\ndecodes into human-readable text, completing the cycle from textual input to tex-\\ntual output.\\n We can implement the text generation process, as shown in the following listing.\\nimport tiktoken\\nfrom chapter04 import generate_text_simple\\ndef text_to_token_ids(text, tokenizer):\\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\\nListing 5.1 Utility functions for text to token ID conversion\\nGPTModel\\nEvery effort moves you tensor([[ 6109, 3626, 6100, 345 ]])\\nTokenizer\\ntensor([[[-0.2968, ..., -0.1714],\\n[-1.3747, ...,  0.3993],\\n[ 1.8251, ..., -0.9297],\\n[-0.0922, ..., -0.6768]]])\\neffort moves you forward\\nTokenizer\\n1. Use the tokenizer to encode input\\ntext into a token ID representation.\\n2. Given four input token IDs, the model\\nproduces 4 logit vectors (rows) where\\neach vector has 50,257 elements\\n(columns) equal to the vocabulary size.\\n3. After converting the logits to\\ntoken IDs, we use the tokenizer\\nto decode these IDs back into\\na text representation.\\ntext_to_token_ids()\\ntoken_ids_to_text()\\nFigure 5.3 Generating text involves encoding text into token IDs that the LLM processes into logit vectors. The \\nlogit vectors are then converted back into token IDs, detokenized into a text representation.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 153, 'page_label': '132'}, page_content='132 CHAPTER 5 Pretraining on unlabeled data\\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \\n    return encoded_tensor\\ndef token_ids_to_text(token_ids, tokenizer):\\n    flat = token_ids.squeeze(0)               \\n    return tokenizer.decode(flat.tolist())\\nstart_context = \"Every effort moves you\"\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(start_context, tokenizer),\\n    max_new_tokens=10,\\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nUsing this code, the model generates the following text:\\nOutput text:\\n Every effort moves you rentingetic wasn? refres RexMeCHicular stren\\nClearly, the model isn’t yet producing cohe rent text because it hasn’t undergone\\ntraining. To define what makes text “coherent” or “high quality,” we have to imple-\\nment a numerical method to evaluate th e generated content. This approach will\\nenable us to monitor and enhance the model’s performance throughout its training\\nprocess.\\n Next, we will calculate a loss metric for the generated outputs.  This loss serves as a\\nprogress and success indicator of the traini ng progress. Furthermore, in later chap-\\nters, when we fine-tune our LLM, we will review additional me thodologies for assess-\\ning model quality.\\n5.1.2 Calculating the text generation loss\\nNext, let’s explore techniques for numeri cally assessing text quality generated\\nduring training by calculating a text generation loss. We will go over this topic step by\\nstep with a practical example to make th e concepts clear and applicable, beginning\\nwith a short recap of how the data is load ed and how the text is generated via the\\ngenerate_text_simple function. \\n Figure 5.4 illustrates the overall flow from input text to LLM-generated text using a\\nfive-step procedure. This text-generation process shows what the generate_text_simple\\nfunction does internally. We need to perf orm these same initial steps before we can\\ncompute a loss that measures the generated text quality later in this section.\\n Figure 5.4 outlines the text generation process with a small seven-token vocabulary\\nto fit this image on a single page. However, our GPTModel works with a much larger\\n.unsqueeze(0) \\nadds the batch \\ndimension\\nRemoves batch \\ndimension\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 154, 'page_label': '133'}, page_content='1335.1 Evaluating generative text models\\nvocabulary consisting of 50,257 words; hence, the token IDs in the following code will\\nrange from 0 to 50,256 rather than 0 to 6. \\n Also, figure 5.4 only show s a single text example ( \"every effort moves\") for sim-\\nplicity. In the following hands-on code ex ample that implements the steps in the fig-\\nure, we will work with two input examples for the GPT model ( \"every effort moves\"\\nand \"I really like\").\\n Consider these two input examples, which have already been mapped to token IDs\\n(figure 5.4, step 1):\\ninputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\\n                       [40,    1107, 588]])   #  \"I really like\"]\\nMatching these inputs, the targets contain the token IDs we want the model to\\nproduce:\\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\\n                        [1107, 588, 11311]])  #  \" really like chocolate\"]\\nNote that the targets are the inputs but shifted one position forward, a concept we\\ncovered in chapter 2 during the implementation of the data loader. This shifting strat-\\negy is crucial for teaching the model to predict the next token in a sequence.\\n[2,\\n1,\\n4]\\nevery\\neﬀort\\nmoves\\nvocabulary = {\\n\"a\":       0,\\n\"effort\":  1,\\n\"every\":   2,\\n\"forward\": 3,\\n\"moves\":   4,\\n\"you\":     5,\\n\"zoo\":     6\\n}\\n[0.10, 0.60, 0.20, 0.05, 0.00, 0.02, 0.01]\\n[0.06, 0.07, 0.01, 0.26, 0.35, 0.13, 0.12]\\n[0.01, 0.10, 0.10, 0.20, 0.12, 0.34, 0.13]\\n0.12, 0.34, 0.13]\\neffort\\nmoves\\nyou\\ninverse_vocabulary = {\\n0: \"a\",\\n1: \"effort\",\\n2: \"every\",\\n3: \"forward\",\\n4: \"moves\",\\n5: \"you\",\\n6: \"zoo\"\\n}\\n1. Use vocabulary\\nto map the input\\ntext to token IDs.\\n2. Obtain seven-dimensional\\nprobability row vector\\nfor each input token via\\nthe function.\\nsoftmax\\n4. Obtain all predicted\\ntoken IDs as the index\\npositions with the\\nhighest probabilities.\\n0     1     2     3     4     5     6Index position:\\n3. Locate the index position\\nwith the highest probability\\nvalue in each row vector, which\\nis done via the function.\\nargmax\\n5. Map index\\npositions back\\ninto text via\\nthe inverse\\nvocabulary.\\nInput text\\nThe output text\\ngenerated by\\nthe LLM\\n1 4 5\\n[ 1, ,\\n,\\n]\\n1\\n4\\n5\\nFigure 5.4 For each of the three input tokens, shown on the left, we compute a vector containing probability \\nscores corresponding to each token in the vocabulary. The index position of the highest probability score in each \\nvector represents the most likely next token ID. These token IDs associated with the highest probability scores \\nare selected and mapped back into a text that represents the text generated by the model.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 155, 'page_label': '134'}, page_content='134 CHAPTER 5 Pretraining on unlabeled data\\n Now we feed the inputs into the model to  calculate logits vectors for the two input\\nexamples, each comprising three tokens. Then we apply the softmax function to\\ntransform these logits into probability scores (probas; figure 5.4, step 2):\\nwith torch.no_grad():    \\n    logits = model(inputs)\\nprobas = torch.softmax(logits, dim=-1)    \\nprint(probas.shape)\\nThe resulting tensor dimension of the probability score (probas) tensor is\\ntorch.Size([2, 3, 50257])\\nThe first number, 2, corresponds to the two examples (rows) in the inputs, also known\\nas batch size. The second number, 3, corre sponds to the number of tokens in each\\ninput (row). Finally, the last number co rresponds to the embedding dimensionality,\\nwhich is determined by the vocabulary size . Following the conversion from logits to\\nprobabilities via the softmax function, the generate_text_simple function then con-\\nverts the resulting probability scores back into text (figure 5.4, steps 3–5).\\n We can complete steps 3 and 4 by applying the argmax function to the probability\\nscores to obtain the corresponding token IDs:\\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\\nprint(\"Token IDs:\\\\n\", token_ids)\\nGiven that we have two input batches, ea ch containing three tokens, applying the\\nargmax function to the probability scores (figure 5.4, step 3) yields two sets of outputs,\\neach with three predicted token IDs:\\nToken IDs:\\n tensor([[[16657],      \\n         [  339],\\n         [42826]],\\n        [[49906],       \\n         [29669],\\n         [41751]]])\\nFinally, step 5 converts the token IDs back into text:\\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\\nprint(f\"Outputs batch 1:\"\\n      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\\nWhen we decode these tokens, we find that  these output tokens are quite different\\nfrom the target tokens we want the model to generate:\\nTargets batch 1:  effort moves you\\nOutputs batch 1:  Armed heNetflix\\nDisables gradient tracking \\nsince we are not training yet\\nProbability of each \\ntoken in vocabulary\\nFirst batch\\nSecond batch\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 156, 'page_label': '135'}, page_content='1355.1 Evaluating generative text models\\nThe model produces random text that is diff erent from the target text because it has\\nnot been trained yet. We now want to ev aluate the performance of the model’s gen-\\nerated text numerically via a loss (figure 5.5). Not only is this useful for measuring\\nthe quality of the generated text, but it’s also a building block for implementing the\\ntraining function, which we will use to update the model’s weight to improve the\\ngenerated text.\\nPart of the text evaluation process that we implement, as shown in figure 5.5, is to mea-\\nsure “how far” the generated tokens are from the correct  predictions (targets). The\\ntraining function we implement later will use this information to adjust the model\\nweights to generate text that is more similar to (or, ideally, matches) the target text.\\n The model training aims to increase the softmax probability in the index positions\\ncorresponding to the correct target token IDs, as illustrated in figure 5.6. This softmax\\nprobability is also used in the evaluation metric we will implement next to numerically\\nassess the model’s generated outputs: the hi gher the probability in the correct posi-\\ntions, the better.\\n Remember that figure 5.6 displays the softmax probabilities for a compact seven-\\ntoken vocabulary to fit everything into a si ngle figure. This implies that the starting\\nrandom values will hover around 1/7, which equals approximately 0.14. However, the\\nvocabulary we are using for our GPT-2 model has 50,257 tokens, so most of the initial\\nprobabilities will hover around 0.00002 (1/50,257). \\n \\n \\nImplement\\nthe loss\\ncomputation\\nto evaluate how\\nwell the model\\nperforms.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nApply the loss to the entire dataset,\\nwhich we split into a training and\\nvalidation portion\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nLoad pretrained weights from\\nOpenAI into our LLM modelFigure 5.5 An overview of the topics covered in this chapter. We have completed step 1. We are now ready to  \\nimplement the text evaluation function (step 2).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 157, 'page_label': '136'}, page_content='136 CHAPTER 5 Pretraining on unlabeled data\\nFor each of the two input texts, we can print the initial softmax probability scores cor-\\nresponding to the target tokens using the following code:\\ntext_idx = 0\\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\\nprint(\"Text 1:\", target_probas_1)\\ntext_idx = 1\\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\\nprint(\"Text 2:\", target_probas_2)\\nThe three target token ID probabilities for each batch are\\nText 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\\nText 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\\nThe goal of training an LLM is to maximize the likelihood of the correct token, which\\ninvolves increasing its probability relative to other tokens. This way, we ensure the\\nLLM consistently picks the target token—e ssentially the next word in the sentence—\\nas the next token it generates.\\n \\n \\n[0.14, 0.14, 0.13, 0.17, 0.15, 0.13, 0.14]\\n[0.15, 0.13, 0.13, 0.16, 0.14, 0.15, 0.14]\\n[0.13, 0.14, 0.15, 0.16, 0.15, 0.13, 0.14]\\n0     1     2     3     4     5     6\\nIndex position:\\n[2,\\n1,\\n4]\\nevery\\neﬀort\\nmoves\\n\"a\"\\n\"effort\"\"every\"\\n\"forward\"\\n\"you\" \"zoo\"\\n\"moves\"\\n1. The model receives\\nthree input tokens and\\ngenerates three vectors.\\n2. Each vector index position\\nin the model-generated\\ntensors corresponds to a\\nword in the vocabulary.\\n[0.14, 0.14, 0.1\\n3. An untrained model\\nproduces random\\nvectors for each token.\\n3, 0.16, 0.14, 0.15, 0.14]\\n5, 0.16, 0.15, 0.13, 0.14]\\n4. In model training, the goal is\\nto maximize the values that\\ncorrespond to the index of\\nthe token in the target vector.\\nInputs: Targets:\\n[ 1, ,\\n,\\n]\\neffort\\nmoves\\nyou\\n1 4 5\\n1\\n4\\n5\\nFigure 5.6 Before training, the model produces random  next-token probability vectors. The goal of model \\ntraining is to ensure that the probability values corresponding to the highlighted target token IDs are maximized.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 158, 'page_label': '137'}, page_content='1375.1 Evaluating generative text models\\nNext, we will calculate the loss for the probability scores  of the two example batches,\\ntarget_probas_1 and target_probas_2. The main steps are illustrated in figure 5.7.\\nSince we already applied steps 1 to 3 to obtain target_probas_1 and target_\\nprobas_2, we proceed with step 4, applying the logarithm to the probability scores:\\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\\nprint(log_probas)\\nThis results in the following values:\\ntensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\\nBackpropagation\\nHow do we maximize the softmax probab ility values corresponding to the target\\ntokens? The big picture is that we update the model weights so that the model outputs\\nhigher values for the respective token IDs we want to generate. The weight update is\\ndone via a process called backpropagation, a standard technique for training deep\\nneural networks (see sections A.3 to A.7 in appendix A for more details about back-\\npropagation and model training).\\nBackpropagation requires a loss function, which calculates the difference between\\nthe model’s predicted output (her e, the probabilities corr esponding to the target\\ntoken IDs) and the actual desired output. This loss function measures how far off the\\nmodel’s predictions are from the target values.\\nLogits\\nProbabilities\\nTarget\\nprobabilities\\nLog probabilities\\nAverage\\nlog probability\\nNegative average\\nlog probability\\n= [[[ 0.1113, -0.1057, -0.3666,  ..., ]]]\\n= [[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., ]]]\\n= [7.4541e-05, 3.1061e-05, 1.1563e-05, ..., ]\\n= [-9.5042, -10.3796, -11.3677, ..., ]\\n= -10.7940\\n= 10.7940\\nThe negative average\\nlog probability is the\\nloss we want to\\ncompute\\n1\\n2\\n3\\n4\\n5\\n6\\nFigure 5.7 Calculating the loss involves several steps. Steps 1 to 3, which we have already \\ncompleted, calculate the token probabilities corresponding to the target tensors. These \\nprobabilities are then transformed via a logarithm and averaged in steps 4 to 6.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 159, 'page_label': '138'}, page_content='138 CHAPTER 5 Pretraining on unlabeled data\\nWorking with logarithms of probability scor es is more manageable in mathematical\\noptimization than handling the scores direct ly. This topic is outside the scope of this\\nbook, but I’ve detailed it further in a lecture, which can be found in appendix B.\\n Next, we combine these log probabilities into a single score by computing the aver-\\nage (step 5 in figure 5.7):\\navg_log_probas = torch.mean(log_probas)\\nprint(avg_log_probas)\\nThe resulting average log probability score is\\ntensor(-10.7940)\\nThe goal is to get the average log probabilit y as close to 0 as possible by updating the\\nmodel’s weights as part of the training pr ocess. However, in de ep learning, the com-\\nmon practice isn’t to push the average log probability up to 0 but rather to bring the\\nnegative average log probability down to 0. The negative average log probability is\\nsimply the average log probability multiplie d by –1, which corresponds to step 6 in\\nfigure 5.7:\\nneg_avg_log_probas = avg_log_probas * -1\\nprint(neg_avg_log_probas)\\nThis prints tensor(10.7940). In deep learning, the term  for turning this negative\\nvalue, –10.7940, into 10.7940, is known as the cross entropy loss. PyTorch comes in\\nhandy here, as it already has a built-in cross_entropy function that takes care of all\\nthese six steps in figure 5.7 for us.\\nBefore we apply the cross_entropy function, let’s briefly recall the shape of the logits\\nand target tensors:\\nprint(\"Logits shape:\", logits.shape)\\nprint(\"Targets shape:\", targets.shape)\\nCross entropy loss\\nAt its core, the cross entropy loss is a popular measure in machine learning and deep\\nlearning that measures th e difference between two pr obability distributions—typi-\\ncally, the true distribution of labels (here, tokens in a dataset) and the predicted dis-\\ntribution from a model (for instance, the token probabilities generated by an LLM). \\nIn the context of machine learning and specifically in frameworks like PyTorch, the\\ncross_entropy function computes this measure for discrete outcomes, which is\\nsimilar to the negative average log probability of the target tokens given the model’s\\ngenerated token probabilities, making the terms “cross entropy” and “negative aver-\\nage log probability” related and often used interchangeably in practice.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 160, 'page_label': '139'}, page_content='1395.1 Evaluating generative text models\\nThe resulting shapes are\\nLogits shape: torch.Size([2, 3, 50257])\\nTargets shape: torch.Size([2, 3])\\nAs we can see, the logits tensor has three dimensions: batch size, number of tokens,\\nand vocabulary size. The targets tensor has two dimensions: batch size and number\\nof tokens.\\n For the cross_entropy loss function in PyTorch, we want to flatten these tensors\\nby combining them over the batch dimension:\\nlogits_flat = logits.flatten(0, 1)\\ntargets_flat = targets.flatten()\\nprint(\"Flattened logits:\", logits_flat.shape)\\nprint(\"Flattened targets:\", targets_flat.shape)\\nThe resulting tensor dimensions are\\nFlattened logits: torch.Size([6, 50257])\\nFlattened targets: torch.Size([6])\\nRemember that the targets are the token IDs we want the LLM to generate, and the\\nlogits contain the unscaled model outputs before they enter the softmax function to\\nobtain the probability scores.\\n Previously, we applied the softmax function, selected the probability scores corre-\\nsponding to the target IDs, and comput ed the negative average log probabilities.\\nPyTorch’s cross_entropy function will take care of all these steps for us:\\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\\nprint(loss)\\nThe resulting loss is the same that we obta ined previously when applying the individ-\\nual steps in figure 5.7 manually:\\ntensor(10.7940)\\nPerplexity\\nPerplexity is a measure often used alongside cross entropy loss to evaluate the per-\\nformance of models in tasks like language modeling. It can provide a more interpre-\\ntable way to understand the uncertainty of a model in predicting the next token in a\\nsequence. \\nPerplexity measures how well the probabi lity distribution predicted by the model\\nmatches the actual distribution of the words in the dataset. Similar to the loss, a lower\\nperplexity indicates that the model predictions are closer to the actual distribution.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 161, 'page_label': '140'}, page_content='140 CHAPTER 5 Pretraining on unlabeled data\\nWe have now calculated the loss for two sma ll text inputs for illustration purposes.\\nNext, we will apply the loss computation to the entire training and validation sets.\\n5.1.3 Calculating the training and validation set losses\\nWe must first prepare the training and validati on datasets that we will use to train the\\nLLM. Then, as highlighted in figure 5.8, we will calculate the cross entropy for the\\ntraining and validation sets, which is an  important component of the model training\\nprocess.\\nTo compute the loss on the training and validation datasets, we use a very small text\\ndataset, the “The Verdict” short story by  Edith Wharton, which we have already\\nworked with in chapter 2. By selecting a text from the public domain, we circumvent\\nany concerns related to usage rights. Addi tionally, using such a small dataset allows\\nfor the execution of code examples on a standard laptop computer in a matter of\\n(continued)\\nPerplexity can be calculated as perplexity = torch.exp(loss), which returns\\ntensor(48725.8203) when applied to the previously calculated loss. \\nPerplexity is often considered more interpretable than the raw loss value because it sig-\\nnifies the effective vocabulary size about which the model is uncertain at each step. In\\nthe given example, this would translate to the model being unsure about which among\\n48,725 tokens in the vocabulary to generate as the next token.\\nImplement\\nthe loss\\ncomputation to\\nevaluate how\\nwell the model\\nperforms.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nApply the loss to the entire dataset,\\nwhich we split into a training and\\nvalidation portion.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nLoad pretrained weights from\\nOpenAI into our LLM model.Figure 5.8 Having completed steps 1 and 2, including com puting the cross entropy loss, we can now apply this \\nloss computation to the entire text dataset that we will use for model training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 162, 'page_label': '141'}, page_content='1415.1 Evaluating generative text models\\nminutes, even without a high-end GPU, wh ich is particularly advantageous for edu-\\ncational purposes. \\nNOTE Interested readers can also use the supplementary code for this book\\nto prepare a larger-scale dataset consisting of more than 60,000 public domain\\nbooks from Project Gutenberg and trai n an LLM on these (see appendix D\\nfor details).\\nThe following code loads the “The Verdict” short story:\\nfile_path = \"the-verdict.txt\"\\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\\n    text_data = file.read()\\nAfter loading the dataset, we can check the number of characters and tokens in the\\ndataset:\\ntotal_characters = len(text_data)\\ntotal_tokens = len(tokenizer.encode(text_data))\\nprint(\"Characters:\", total_characters)\\nprint(\"Tokens:\", total_tokens)\\nThe output is\\nCharacters: 20479\\nTokens: 5145\\nWith just 5,145 tokens, the text might se em too small to train an LLM, but as men-\\ntioned earlier, it’s for educational purposes  so that we can run the code in minutes\\ninstead of weeks. Plus, later we will load pretrained weights from OpenAI into our\\nGPTModel code.\\n Next, we divide the dataset into a trai ning and a validation set and use the data\\nloaders from chapter 2 to prepare the batches for LLM training. This process is visual-\\nized in figure 5.9. Due to spatial constraints, we use a max_length=6. However, for the\\nactual data loaders, we set the max_length equal to the 256-token context length that\\nthe LLM supports so that the LLM sees longer texts during training.\\nThe cost of pretraining LLMs \\nTo put the scale of our project into perspective, consider the training of the 7 billion\\nparameter Llama 2 model, a relatively popular openly available LLM. This model\\nrequired 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens.\\nAt the time of writing, running an 8 × A100 cloud se rver on AWS costs around $30\\nper hour. A rough estimate puts the total training cost of such an LLM at around\\n$690,000 (calculated as 184,320 hours divided by 8, then multiplied by $30).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 163, 'page_label': '142'}, page_content='142 CHAPTER 5 Pretraining on unlabeled data\\nNOTE We are training the model with trai ning data presented in similarly\\nsized chunks for simplicity and efficiency . However, in practice, it can also be\\nbeneficial to train an LLM with variable-length inputs to help the LLM to bet-\\nter generalize across different types of inputs when it is being used.\\nTo implement the data splitting and loading, we first define a train_ratio to use 90%\\nof the data for training and the remaining 10% as validation data for model evalua-\\ntion during training:\\ntrain_ratio = 0.90\\nsplit_idx = int(train_ratio * len(text_data))\\ntrain_data = text_data[:split_idx]\\nval_data = text_data[split_idx:]\\nIn the realm of visual compositions, where content reigns\\nsupreme, there exists a tranquil harbor known as\\nPlaceholder Bay. Here, amidst the gentle sway of design\\nelements, words meander without true purpose, yet with a\\nsemblance of meaning. This text, neither too captivating nor\\ntoo bland, serves as a beacon for eyes that scan, seeking\\nthe eventual substance that will ﬁll the void. castsAs the sun\\nits golden hues over Placeholder Bay, sentences ﬂow like a\\ngentle river, words ripple like waves against the shore.\\n[[  818,   262, 13360,   286,  5874, 33543 ],\\n[    11,   810,  2695, 13580,    82, 17700 ],\\n[    11,   612,  7160,   257, 46944, 25451 ],\\n[  1900,   355,  8474, 13829,  4696,    13 ],\\n[  3423,    11, 31095,   262, 10296, 20009 ],\\n...\\n[  7850,    11,  2456, 42462,   588,  9813 ]]\\n[[[ 1900,   355,  8474, 13829,  4696,    13 ],\\n[    11,   810,  2695, 13580,    82, 17700 ]],\\n[[ 7850,    11,  2456, 42462,   588,  9813 ],\\n[  3423,    11, 31095,   262, 10296, 20009 ]],\\n...\\n[[   11,   612,  7160,   257, 46944, 25451 ],\\n[   818,   262, 13360,   286,  5874, 33543 ]]]\\nWe use a small portion of the data\\nto construct the validation set.\\nWe use a large portion of\\nthe input text dataset for\\nmodel training.\\n1) Input text dataset Tokenizer\\nThe ﬁrst sample\\nin the tokenized\\ndataset of length 6\\n[[  818,   262, 13360,   286,  5874, 33543 ],\\n[    11,   810,  2695, 13580,    82, 17700 ],\\nProcess the dataset\\nwith a max length\\nof 6\\nThe second sample\\nin the tokenized\\ndataset\\nOrganize dataset\\ninto batches; here\\nwith batch size 2\\nand shufﬂing\\nenabled.\\n2) Tokenized\\ntraining dataset\\n3) Training datasets\\nin chunks of length 6\\n4) Training datasets\\nin batchesBatch 1\\nBatch 2\\nLast batch\\n[ 818, 262, 13360, 286, 5874, 33543, 11, 810,\\n2695, 13580, 82, 17700, 11, 612, 7160, 257,\\n46944, 25451, 1900, 355, 8474, 13829, 4696, 13,\\n3423, 11, 31095, 262, 10296, 20009, 286, 1486,\\n4847, 11, 2456, 502, 4066, 1231, 2081, 4007, 11,\\n1865, 351, 257, 45960, 286, 3616, 13, 770, 2420,\\n11, 6159, 1165, 3144, 39438, 4249, 1165, 34377,\\n11, 9179, 355, 257, 34538, 329, 2951, 326, 9367,\\n11, 6095, 262, 19657, 9136, 326, 481, 6070, 262,\\n7951, 13, 1081, 262, 4252, 3350 ]\\nThe second sample in the\\ntokenized dataset when\\nthe stride is set to 6\\nFigure 5.9 When preparing the data loaders, we split the input text into training and validation set portions. Then \\nwe tokenize the text (only shown for the training set portion for simplicity) and divide the tokenized text into \\nchunks of a user-specified length (here, 6). Finally, we shuffle the rows and organize the chunked text into batches \\n(here, batch size 2), which we can use for model training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 164, 'page_label': '143'}, page_content='1435.1 Evaluating generative text models\\nUsing the train_data and val_data subsets, we can now create the respective data\\nloader reusing the create_dataloader_v1 code from chapter 2:\\nfrom chapter02 import create_dataloader_v1\\ntorch.manual_seed(123)\\ntrain_loader = create_dataloader_v1(\\n    train_data,\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=True,\\n    shuffle=True,\\n    num_workers=0\\n)\\nval_loader = create_dataloader_v1(\\n    val_data,\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=False,\\n    shuffle=False,\\n    num_workers=0\\n)\\nWe used a relatively small batch size to  reduce the computational resource demand\\nbecause we were working with a very small dataset. In practice, training LLMs with\\nbatch sizes of 1,024 or larger is not uncommon.\\n As an optional check, we can iterate th rough the data loaders to ensure that they\\nwere created correctly:\\nprint(\"Train loader:\")\\nfor x, y in train_loader:\\n    print(x.shape, y.shape)\\nprint(\"\\\\nValidation loader:\")\\nfor x, y in val_loader:\\n    print(x.shape, y.shape)\\nWe should see the following outputs:\\nTrain loader:\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\ntorch.Size([2, 256]) torch.Size([2, 256])\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 165, 'page_label': '144'}, page_content='144 CHAPTER 5 Pretraining on unlabeled data\\nValidation loader:\\ntorch.Size([2, 256]) torch.Size([2, 256])\\nBased on the preceding code output, we have nine traini ng set batches with two sam-\\nples and 256 tokens each. Since we allocated only 10% of the data for validation, there\\nis only one validation batch consisting of  two input examples. As expected, the input\\ndata (x) and target data (y) have the same shape (the ba tch size times the number of\\ntokens in each batch) since the targets are the inputs shifted by one position, as dis-\\ncussed in chapter 2.\\n Next, we implement a utility function to calculate the cross entropy loss of a given\\nbatch returned via the training and validation loader:\\ndef calc_loss_batch(input_batch, target_batch, model, device):\\n    input_batch = input_batch.to(device)        \\n    target_batch = target_batch.to(device)      \\n    logits = model(input_batch)\\n    loss = torch.nn.functional.cross_entropy(\\n        logits.flatten(0, 1), target_batch.flatten()\\n    )\\n    return loss\\nWe can now use this calc_loss_batch utility function, which computes the loss for a\\nsingle batch, to implement the following calc_loss_loader function that computes\\nthe loss over all the batches sampled by a given data loader.\\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\\n    total_loss = 0.\\n    if len(data_loader) == 0:\\n        return float(\"nan\")\\n    elif num_batches is None:\\n        num_batches = len(data_loader)    \\n    else:\\n        num_batches = min(num_batches, len(data_loader))  \\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            total_loss += loss.item()   \\n        else:\\n            break\\n    return total_loss / num_batches   \\nBy default, the calc_loss_loader function iterates over all batches in a given data\\nloader, accumulates the loss in the total_loss variable, and then computes and\\nListing 5.2 Function to compute the training and validation loss\\nThe transfer to a \\ngiven device allows \\nus to transfer the \\ndata to a GPU.\\nIteratives over all \\nbatches if no fixed \\nnum_batches is specified\\nReduces the number\\nof batches to match\\nthe total number of\\nbatches in the data\\nloader if num_batches\\nexceeds the number\\nof batches in the\\ndata loader\\nSums loss \\nfor each \\nbatch\\nAverages the loss over all batches\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 166, 'page_label': '145'}, page_content='1455.1 Evaluating generative text models\\naverages the loss over the total number of  batches. Alternatively, we can specify a\\nsmaller number of batches via num_batches to speed up the evaluation during model\\ntraining.\\n Let’s now see this calc_loss_loader function in action, applying it to the training\\nand validation set loaders:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)  \\nwith torch.no_grad():                                       \\n    train_loss = calc_loss_loader(train_loader, model, device)   \\n    val_loss = calc_loss_loader(val_loader, model, device)\\nprint(\"Training loss:\", train_loss)\\nprint(\"Validation loss:\", val_loss)\\nThe resulting loss values are\\nTraining loss: 10.98758347829183\\nValidation loss: 10.98110580444336\\nThe loss values are relatively high becaus e the model has not yet been trained. For\\ncomparison, the loss approaches 0 if the mode l learns to generate the next tokens as\\nthey appear in the training and validation sets.\\n Now that we have a way to measure the quality of the generated text, we will train\\nthe LLM to reduce this loss so that it become s better at generating text, as illustrated\\nin figure 5.10.\\nIf you have a machine with a \\nCUDA-supported GPU, the LLM \\nwill train on the GPU without \\nmaking any changes to the code.\\nDisables gradient tracking\\nfor efficiency because we\\nare not training yet\\nVia the “device” setting,\\nwe ensure the data is loaded onto\\nthe same device as the LLM model.\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nLoad pretrained weights from\\nOpenAI into our LLM model.\\nFigure 5.10 We have recapped the text generation process (step 1) and implemented basic model \\nevaluation techniques (step 2) to compute the training and validation set losses (step 3). Next, we \\nwill go to the training functions and pretrain the LLM (step 4).Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 167, 'page_label': '146'}, page_content='146 CHAPTER 5 Pretraining on unlabeled data\\nNext, we will focus on pretraining the LLM . After model training, we will implement\\nalternative text generation strategies and save and load pretrained model weights.\\n5.2 Training an LLM\\nIt is finally time to implement the code for pretraining the LLM, our GPTModel. For this,\\nwe focus on a straightforward training loop to keep the code concise and readable. \\nNOTE Interested readers can learn about more advanced techniques, includ-\\ning learning rate warmup, cosine annealing, and gradient clipping, in appendix D.\\nThe flowchart in figure 5.11 depicts a typi cal PyTorch neural ne twork training work-\\nflow, which we use for training an LLM. It ou tlines eight steps, st arting with iterating\\nover each epoch, processing batches, resetting gradients, calculating the loss and new\\n1) Iterate over training epochs\\n2) Iterate over batches in\\neach training epoch\\n3) Reset loss gradients from\\nprevious batch iteration\\n4) Calculate loss on\\ncurrent batch\\n5) Backward pass to\\ncalculate loss gradients\\n6) Update model weights\\nusing loss gradients\\n7) Print training and\\nvalidation set losses\\n8) Generate sample text\\nfor visual inspection\\nOptional steps for tracking\\nthe training progress.\\nThese are the usual steps\\nused for training deep\\nneural networks in PyTorch.\\nOne epoch is one complete\\npass over a training set.\\nThe number of batches is\\ndetermined by the training\\nset size divided by the size\\nof each batch.Figure 5.11 A typical training loop for training deep neural networks in \\nPyTorch consists of numerous steps, iterating over the batches in the training \\nset for several epochs. In each loop, we calculate the loss for each training \\nset batch to determine loss gradients, which we use to update the model \\nweights so that the training set loss is minimized.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 168, 'page_label': '147'}, page_content='1475.2 Training an LLM\\ngradients, and updating weights and conclu ding with monitoring  steps like printing\\nlosses and generating text samples. \\nNOTE If you are relatively new to training deep neural networks with PyTorch\\nand any of these steps are unfamiliar, co nsider reading sections A.5 to A.8 in\\nappendix A.\\nWe can implement this training flow via the train_model_simple function in code.\\ndef train_model_simple(model, train_loader, val_loader,\\n                       optimizer, device, num_epochs,\\n                       eval_freq, eval_iter, start_context, tokenizer):\\n    train_losses, val_losses, track_tokens_seen = [], [], []   \\n    tokens_seen, global_step = 0, -1\\n    for epoch in range(num_epochs):   \\n        model.train()\\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()  \\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            loss.backward()                    \\n            optimizer.step()                   \\n            tokens_seen += input_batch.numel()\\n            global_step += 1\\n            if global_step % eval_freq == 0:   \\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader, device, eval_iter)\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                track_tokens_seen.append(tokens_seen)\\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n        generate_and_print_sample(                     \\n            model, tokenizer, device, start_context\\n        )\\n    return train_losses, val_losses, track_tokens_seen\\nNote that the train_model_simple function we just created uses two functions we\\nhave not defined yet: evaluate_model and generate_and_print_sample. \\n The evaluate_model function corresponds to step 7 in figure 5.11. It prints the\\ntraining and validation set losses after each model update so we can evaluate whether\\nthe training improves the mo del. More specifically, the evaluate_model function cal-\\nculates the loss over the training and validation set while ensuring the model is in eval-\\nListing 5.3 The main function for pretraining LLMs\\nInitializes lists to\\ntrack losses and\\ntokens seen\\nStarts the main \\ntraining loop\\nResets loss gradients \\nfrom the previous \\nbatch iteration\\nCalculates loss gradients\\nUpdates model weights \\nusing loss gradients\\nOptional evaluation step\\nPrints a sample text \\nafter each epoch\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 169, 'page_label': '148'}, page_content='148 CHAPTER 5 Pretraining on unlabeled data\\nuation mode with gradient tracking and dropout disabled when calculating the loss\\nover the training and validation sets:\\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\\n    model.eval() \\n    with torch.no_grad():                             \\n        train_loss = calc_loss_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_loss = calc_loss_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n    model.train()\\n    return train_loss, val_loss\\nSimilar to evaluate_model, the generate_and_print_sample function is a convenience\\nfunction that we use to track whether the model improves during the training. In partic-\\nular, the generate_and_print_sample function takes a text snippet (start_context) as\\ninput, converts it into token IDs, and feeds it to the LLM to generate a text sample\\nusing the generate_text_simple function we used earlier:\\ndef generate_and_print_sample(model, tokenizer, device, start_context):\\n    model.eval()\\n    context_size = model.pos_emb.weight.shape[0]\\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\\n    with torch.no_grad():\\n        token_ids = generate_text_simple(\\n            model=model, idx=encoded,\\n            max_new_tokens=50, context_size=context_size\\n        )\\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\\n    print(decoded_text.replace(\"\\\\n\", \" \"))     \\n    model.train()\\nWhile the evaluate_model function gives us a numeric estimate of the model’s train-\\ning progress, this generate_and_print_sample text function provides a concrete text\\nexample generated by the model to judge its capabilities during training.\\nAdamW \\nAdam optimizers are a popular choice for training deep neural networks. However, in\\nour training loop, we opt for the AdamW optimizer. AdamW is a variant of Adam that\\nimproves the weight decay approach, which aims to minimize model complexity and\\nprevent overfitting by penalizing larger we ights. This adjustment allows AdamW to\\nachieve more effective regularization and better generalization; thus, AdamW is fre-\\nquently used in the training of LLMs.\\nDropout is disabled during \\nevaluation for stable, \\nreproducible results.\\nDisables gradient tracking, which is not\\nrequired during evaluation, to reduce\\nthe computational overhead\\nCompact \\nprint format\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 170, 'page_label': '149'}, page_content='1495.2 Training an LLM\\nLet’s see this all in action by training a GPTModel instance for 10 epochs using an\\nAdamW optimizer and the train_model_simple function we defined earlier: \\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\noptimizer = torch.optim.AdamW(\\n     model.parameters(),          \\n    lr=0.0004, weight_decay=0.1\\n)\\nnum_epochs = 10\\ntrain_losses, val_losses, tokens_seen = train_model_simple(\\n    model, train_loader, val_loader, optimizer, device,\\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\\n    start_context=\"Every effort moves you\", tokenizer=tokenizer\\n)\\nExecuting the train_model_simple function starts the trai ning process, which takes\\nabout 5 minutes to complete on a MacBook Air or a similar laptop. The output\\nprinted during this execution is as follows:\\nEp 1 (Step 000000): Train loss 9.781, Val loss 9.933\\nEp 1 (Step 000005): Train loss 8.111, Val loss 8.339\\nEvery effort moves you,,,,,,,,,,,,.                                     \\nEp 2 (Step 000010): Train loss 6.661, Val loss 7.048\\nEp 2 (Step 000015): Train loss 5.961, Val loss 6.616\\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and,\\n and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\\n[...]                                                  \\nEp 9 (Step 000080): Train loss 0.541, Val loss 6.393\\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted\\nhim vindicated--and by me!\"  He laughed again, and threw back the \\nwindow-curtains, I had the donkey. \"There were days when I\\nEp 10 (Step 000085): Train loss 0.391, Val loss 6.452\\nEvery effort moves you know,\" was one of the axioms he laid down across the\\nSevres and silver of an exquisitely appointed luncheon-table, when, on a\\nlater day, I had again run over from Monte Carlo; and Mrs. Gis\\nAs we can see, the training loss improves drastically, starting with a value of 9.781\\nand converging to 0.391. Th e language skills of the model have improved quite a\\nlot. In the beginning, the model is only able to append commas to the start context\\n(Every effort moves you,,,,,,,,,,,,) or repeat the word and. At the end of the\\ntraining, it can generate grammatically correct text. \\n Similar to the training set loss, we ca n see that the validation loss starts high\\n(9.933) and decreases during the training. Ho wever, it never becomes as small as the\\ntraining set loss and remains at 6.452 after the 10th epoch.\\n Before discussing the validation loss in more detail, let’s create a simple plot that\\nshows the training and validation set losses side by side:\\n \\nThe .parameters() method \\nreturns all trainable weight \\nparameters of the model.\\nIntermediate\\nresults removed\\nto save space\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 171, 'page_label': '150'}, page_content='150 CHAPTER 5 Pretraining on unlabeled data\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.ticker import MaxNLocator\\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\\n    fig, ax1 = plt.subplots(figsize=(5, 3))\\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\\n    ax1.plot(\\n        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\\n    )\\n    ax1.set_xlabel(\"Epochs\")\\n    ax1.set_ylabel(\"Loss\")\\n    ax1.legend(loc=\"upper right\")\\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\\n    ax2 = ax1.twiny()                  \\n    ax2.plot(tokens_seen, train_losses, alpha=0)    \\n    ax2.set_xlabel(\"Tokens seen\")\\n    fig.tight_layout()\\n    plt.show()\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\\nThe resulting training and validation loss plot  is shown in figure 5.12. As we can see,\\nboth the training and validation losses star t to improve for the first epoch. However,\\nthe losses start to diverge past the second epoch. This divergence and the fact that the\\nvalidation loss is much larger than the trai ning loss indicate that the model is overfit-\\nting to the training data. We can confirm that the model memorizes the training data\\nverbatim by searching for the generated text snippets, such as quite insensible to\\nthe irony in the “The Verdict” text file. \\nCreates a second \\nx-axis that shares \\nthe same y-axis\\nInvisible plot for \\naligning ticks\\nFigure 5.12 At the beginning of the tr aining, both the training and validation \\nset losses sharply decrease, which is a sign that the model is learning. However, \\nthe training set loss continues to decrease past the second epoch, whereas the \\nvalidation loss stagnates. This is a sign that the model is still learning, but it’s \\noverfitting to the training set past epoch 2.Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 172, 'page_label': '151'}, page_content='1515.3 Decoding strategies to control randomness\\nThis memorization is expected since we ar e working with a very , very small training\\ndataset and training the model for multiple  epochs. Usually, it’s common to train a\\nmodel on a much larger dataset for only one epoch. \\nNOTE As mentioned earlier, interested readers can try to train the model on\\n60,000 public domain books from Project Gutenberg, where this overfitting\\ndoes not occur; see appendix B for details. \\nAs illustrated in figure 5.13, we have comp leted four of our objectives for this chaper.\\nNext, we will cover text generation strategies for LLMs to reduce training data memo-\\nrization and increase the originality of the LLM-generated text before we cover weight\\nloading and saving and loading pretrained weights from OpenAI’s GPT model.\\n5.3 Decoding strategies to control randomness\\nLet’s look at text generation strategies (also called decoding strategies) to generate\\nmore original text. First, we will briefly revisit the generate_text_simple function that\\nwe used inside generate_and_print_sample earlier. Then we will cover two techniques,\\ntemperature scaling and top-k sampling, to improve this function.\\n We begin by transferring the model ba ck from the GPU to the CPU since infer-\\nence with a relatively small model does not require a GPU. Also, after training, we put\\nthe model into evaluation mode to turn off random components such as dropout:\\nmodel.to(\"cpu\")\\nmodel.eval()\\nNext, we plug the GPTModel instance ( model) into the generate_text_simple func-\\ntion, which uses the LLM to generate one token at a time:\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text.\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization.\\nAt the end of this chapter,\\nload pretrained weights from\\nOpenAI into our LLM model.\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later.\\nFigure 5.13 Our model can generate coherent text  after implementing the training function. \\nHowever, it often memorizes passages from the training set verbatim. Next, we will discuss \\nstrategies to generate more diverse output texts.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 173, 'page_label': '152'}, page_content='152 CHAPTER 5 Pretraining on unlabeled data\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\\n    max_new_tokens=25,\\n    context_size=GPT_CONFIG_124M[\"context_length\"]\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe generated text is\\nOutput text:\\nEvery effort moves you know,\" was one of the axioms he laid down across the\\nSevres and silver of an exquisitely appointed lun\\nAs explained earlier, the generated token is  selected at each generation step corre-\\nsponding to the largest probability score among all tokens in the vocabulary. This\\nmeans that the LLM will always generate th e same outputs even if we run the preced-\\ning \\ngenerate_text_simple function multiple times on  the same start context ( Every\\neffort moves you).\\n5.3.1 Temperature scaling\\nLet’s now look at temperature scaling, a technique that adds a pr obabilistic selection\\nprocess to the next-token generation task. Previously, inside the generate_text_simple\\nfunction, we always sampled the token with the highest probability as the next token\\nusing torch.argmax, also known as greedy decoding. To generate text with more variety,\\nwe can replace argmax with a function that samples from a probability distribution\\n(here, the probability scores the LLM gene rates for each vocabulary entry at each\\ntoken generation step).\\n To illustrate the probabilistic sampling wi th a concrete example, let’s briefly dis-\\ncuss the next-token generation process usin g a very small vocabulary for illustration\\npurposes:\\nvocab = { \\n    \"closer\": 0,\\n    \"every\": 1, \\n    \"effort\": 2, \\n    \"forward\": 3,\\n    \"inches\": 4,\\n    \"moves\": 5, \\n    \"pizza\": 6,\\n    \"toward\": 7,\\n    \"you\": 8,\\n} \\ninverse_vocab = {v: k for k, v in vocab.items()}\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 174, 'page_label': '153'}, page_content='1535.3 Decoding strategies to control randomness\\nNext, assume the LLM is given the start context \"every effort moves you\" and gener-\\nates the following next-token logits:\\nnext_token_logits = torch.tensor(\\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\\n)\\nAs discussed in chapter 4, inside generate_text_simple, we convert the logits into\\nprobabilities via the softmax function and obtain the token ID corresponding to the\\ngenerated token via the argmax function, which we can then map back into text via\\nthe inverse vocabulary:\\nprobas = torch.softmax(next_token_logits, dim=0)\\nnext_token_id = torch.argmax(probas).item()\\nprint(inverse_vocab[next_token_id])\\nSince the largest logit value and, corresp ondingly, the largest softmax probability\\nscore are in the fourth position (index posi tion 3 since Python uses 0 indexing), the\\ngenerated word is \"forward\". \\n To implement a probabilistic samp ling process, we can now replace argmax with\\nthe multinomial function in PyTorch:\\ntorch.manual_seed(123) \\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\\nprint(inverse_vocab[next_token_id])\\nThe printed output is \"forward\" just like before. What happened? The multinomial\\nfunction samples the next token proportional  to its probability score. In other words,\\n\"forward\" is still the most likely toke n and will be selected by multinomial most of\\nthe time but not all the time. To illustrate this, let’s implement a function that repeats\\nthis sampling 1,000 times:\\ndef print_sampled_tokens(probas):\\n    torch.manual_seed(123)\\n    sample = [torch.multinomial(probas, num_samples=1).item()\\n             for i in range(1_000)]\\n    sampled_ids = torch.bincount(torch.tensor(sample))\\n    for i, freq in enumerate(sampled_ids):\\n        print(f\"{freq} x {inverse_vocab[i]}\")\\nprint_sampled_tokens(probas)\\nThe sampling output is\\n73 x closer\\n0 x every\\n0 x effort\\n582 x forward\\n2 x inches\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 175, 'page_label': '154'}, page_content=\"154 CHAPTER 5 Pretraining on unlabeled data\\n0 x moves\\n0 x pizza\\n343 x toward\\nAs we can see, the word forward is sampled most of the time (582 out of 1,000 times),\\nbut other tokens such as closer, inches, and toward will also be sampled some of\\nthe time. This means that if we replaced the argmax function with the multinomial\\nfunction inside the generate_and_print_sample  function, the LLM would some-\\ntimes generate texts such as every effort moves you toward, every effort moves\\nyou inches, and every effort moves you closer instead of every effort moves you\\nforward.\\n We can further control the distribution and selection process via a concept called\\ntemperature scaling. Temperature scaling is just a fancy description for dividing the logits\\nby a number greater than 0:\\ndef softmax_with_temperature(logits, temperature):\\n    scaled_logits = logits / temperature\\n    return torch.softmax(scaled_logits, dim=0)\\nTemperatures greater than 1 result in more uniformly distributed token probabilities,\\nand temperatures smaller than 1 will result in more confident (sharper or more peaky)\\ndistributions. Let’s illustrate this by plotting the original probabilities alongside proba-\\nbilities scaled with different temperature values:\\ntemperatures = [1, 0.1, 5]                                    \\nscaled_probas = [softmax_with_temperature(next_token_logits, T)\\n                for T in temperatures]\\nx = torch.arange(len(vocab))\\nbar_width = 0.15\\nfig, ax = plt.subplots(figsize=(5, 3))\\nfor i, T in enumerate(temperatures):\\n    rects = ax.bar(x + i * bar_width, scaled_probas[i], \\n                   bar_width, label=f'Temperature = {T}')\\nax.set_ylabel('Probability')\\nax.set_xticks(x)\\nax.set_xticklabels(vocab.keys(), rotation=90)\\nax.legend()\\nplt.tight_layout()\\nplt.show()\\nThe resulting plot is shown in figure 5.14.\\n A temperature of 1 divides the logits by 1 before passing them to the softmax func-\\ntion to compute the probability scores. In other words, using a temperature of 1 is the\\nsame as not using any temperature scaling. In  this case, the tokens are selected with a\\nprobability equal to the original softmax probability scores via the multinomial sam-\\npling function in PyTorch. For example, for the temperature setting 1, the token cor-\\nresponding to “forward” would be selected  about 60% of the time, as we can see in\\nfigure 5.14.\\nOriginal, lower, \\nand higher \\nconfidence\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 176, 'page_label': '155'}, page_content='1555.3 Decoding strategies to control randomness\\nAlso, as we can see in figure 5.14, applying  very small temperatures, such as 0.1, will\\nresult in sharper dist ributions such that the behavior of the multinomial function\\nselects the most likely token (here, \"forward\") almost 100% of the time, approaching\\nthe behavior of the argmax function. Likewise, a temper ature of 5 results in a more\\nuniform distribution where other tokens are selected more often. This can add more\\nvariety to the generated texts but also mo re often results in nonsensical text. For\\nexample, using the temperature of  5 results in texts such as every effort moves you\\npizza about 4% of the time.\\n5.3.2 Top-k sampling\\nWe’ve now implemented a probabilistic sampling approach coupled with temperature\\nscaling to increase the diversity of the outputs. We saw that higher temperature values\\nresult in more uniformly distributed next-t oken probabilities, which result in more\\ndiverse outputs as it reduces the likelihood of the model repeatedly selecting the most\\nprobable token. This method allows for the exploring of less likely but potentially\\nmore interesting and creative paths in th e generation process. However, one down-\\nside of this approach is that it someti mes leads to grammatically incorrect or com-\\npletely nonsensical outputs such as every effort moves you pizza.\\nExercise 5.1\\nUse the print_sampled_tokens function to print the sampling frequencies of the\\nsoftmax probabilities scaled with the temperatures shown in figure 5.14. How often\\nis the word pizza sampled in each case? Can you think of a faster and more accurate\\nway to determine how often the word pizza is sampled?\\nFigure 5.14 A temperature of 1 r epresents the unscaled probability \\nscores for each token in the vocabulary. Decreasing the temperature to \\n0.1 sharpens the distribution, so the most likely token (here, “forward”) \\nwill have an even higher probability score. Likewise, increasing the \\ntemperature to 5 makes the distribution more uniform.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 177, 'page_label': '156'}, page_content='156 CHAPTER 5 Pretraining on unlabeled data\\n Top-k sampling, when combined with probabilist ic sampling and te mperature scal-\\ning, can improve the text generation result s. In top-k sampling, we can restrict the\\nsampled tokens to the top-k mo st likely tokens and exclude all other tokens from the\\nselection process by masking their probability scores, as illustrated in figure 5.15.\\nThe top-k approach replaces all nonselected logits with negative infinity value (-inf),\\nsuch that when computing the softmax values , the probability scores of the non-top-k\\ntokens are 0, and the remain ing probabilities sum up to 1. (Careful readers may\\nremember this masking trick from the ca usal attention module we implemented in\\nchapter 3, section 3.5.1.)\\n In code, we can implement the top-k proc edure in figure 5.15 as follows, starting\\nwith the selection of the tokens with the largest logit values:\\ntop_k = 3\\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\\nprint(\"Top logits:\", top_logits)\\nprint(\"Top positions:\", top_pos)\\nLogits\\nTop-k (k = 3)\\n-infmask\\nSoftmax\\n= [ 4.51,  0.89, -1.90,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n= [ 4.51,  0.89, -1.90,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]4.51,\\n0       1     2      3      4      5      6      7      8Index position:\\n\"closer\"\"every\"\"effort\"\"forward\"\"inches\"\"moves\"\"pizza\"\"toward\"\"you\"\\nVocabulary:\\n= [ 4.51,  -inf, -inf,   6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]\\n= [ 0.06,  0.00,  0.00,  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ]\\n,  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ],  6.75,  1.63, -1.62, -1.89,  6.28,  1.79 ]\\n4.51, 6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]6.75,  -inf,  -inf,  -inf,  6.28,  -inf ]\\n0.06, ,  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ],  0.57,  0.00,  0.00,  0.00,  0.36,  0.00 ]\\nBy assigning zero probabilities to the\\nnon-top-k positions, we ensure that\\nthe next token is always sampled\\nfrom a top-k position.\\nFigure 5.15 Using top-k sampling with k = 3, we focus on the three tokens associated with the highest logits \\nand mask out all other tokens with negative infinity (–inf) before applying the softmax function. This results \\nin a probability distribution with a probability value 0 assigned to all non-top-k tokens. (The numbers in this figure \\nare truncated to two digits after the decimal point to reduce visual clutter. The values in the “Softmax” row \\nshould add up to 1.0.)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 178, 'page_label': '157'}, page_content=\"1575.3 Decoding strategies to control randomness\\nThe logits values and token IDs of the top three tokens, in descending order, are\\nTop logits: tensor([6.7500, 6.2800, 4.5100])\\nTop positions: tensor([3, 7, 0])\\nSubsequently, we apply PyTorch’s where function to set the logit values of tokens that are\\nbelow the lowest logit value within our top-three selection to negative infinity (-inf): \\nnew_logits = torch.where(\\n    condition=next_token_logits < top_logits[-1],   \\n    input=torch.tensor(float('-inf')),    \\n    other=next_token_logits    \\n)\\nprint(new_logits)\\nThe resulting logits for the next token in the nine-token vocabulary are\\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,\\n     -inf])\\nLastly, let’s apply the softmax function to turn these into next-token probabilities:\\ntopk_probas = torch.softmax(new_logits, dim=0)\\nprint(topk_probas)\\nAs we can see, the result of this top-th ree approach are three non-zero probability\\nscores:\\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610,\\n        0.0000])\\nWe can now apply the temperature scaling and multinomial function for probabilistic\\nsampling to select the next token among these three non-zero probability scores to\\ngenerate the next token. We do this next by modifying the text generation function.\\n5.3.3 Modifying the text generation function\\nNow, let’s combine temperature sampling and top-k sampling to modify the generate_\\ntext_simple function we used to generate text via the LLM earlier, creating a new\\ngenerate function.\\ndef generate(model, idx, max_new_tokens, context_size,\\n             temperature=0.0, top_k=None, eos_id=None):\\n    for _ in range(max_new_tokens):           \\n        idx_cond = idx[:, -context_size:]\\n        with torch.no_grad():\\n            logits = model(idx_cond)\\n        logits = logits[:, -1, :]\\nListing 5.4 A modified text generation function with more diversity\\nIdentifies logits less than \\nthe minimum in the top 3\\nAssigns –inf to these lower logits\\nRetains the original logits \\nfor all other tokens\\nThe for loop is the same \\nas before: gets logits \\nand only focuses on the \\nlast time step.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 179, 'page_label': '158'}, page_content='158 CHAPTER 5 Pretraining on unlabeled data\\n        if top_k is not None:               \\n            top_logits, _ = torch.topk(logits, top_k)\\n            min_val = top_logits[:, -1]\\n            logits = torch.where(\\n                logits < min_val,\\n                torch.tensor(float(\\'-inf\\')).to(logits.device),\\n                logits\\n            )\\n        if temperature > 0.0:                 \\n            logits = logits / temperature\\n            probs = torch.softmax(logits, dim=-1)\\n            idx_next = torch.multinomial(probs, num_samples=1)\\n        else:   \\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\\n        if idx_next == eos_id:             \\n            break\\n        idx = torch.cat((idx, idx_next), dim=1)\\n    return idx\\nLet’s now see this new generate function in action:\\ntorch.manual_seed(123)\\ntoken_ids = generate(\\n    model=model,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\\n    max_new_tokens=15,\\n    context_size=GPT_CONFIG_124M[\"context_length\"],\\n    top_k=25,\\n    temperature=1.4\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe generated text is\\nOutput text:\\n Every effort moves you stand to work on surprise, a one of us had gone\\n with random-\\nAs we can see, the generated text is very different from the one we previously gener-\\nated via the generate_simple function in section 5.3 (\"Every effort moves you know,\"\\nwas one of the axioms he laid...! ), which was a memorized passage from the train-\\ning set.\\nExercise 5.2\\nPlay around with different temperatures and top-k settings. Based on your observa-\\ntions, can you think of applic ations where lower temperature and top-k settings are\\ndesired? Likewise, can you think of applications where higher temperature and top-k\\nsettings are preferred? (It’s recommended to also revisit this exercise at the end of\\nthe chapter after loading the pretrained weights from OpenAI.)\\nFilters logits with \\ntop_k sampling\\nApplies \\ntemperature \\nscaling\\nCarries out \\ngreedy next-\\ntoken selection \\nas before when \\ntemperature \\nscaling is \\ndisabled\\nStops generating early \\nif end-of-sequence \\ntoken is encountered\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 180, 'page_label': '159'}, page_content='1595.4 Loading and saving model weights in PyTorch\\n5.4 Loading and saving model weights in PyTorch\\nThus far, we have discussed how to numerically evaluate the training progress and pre-\\ntrain an LLM from scratch. Even though  both the LLM and dataset were relatively\\nsmall, this exercise showed that pretraining LLMs is computationally expensive. Thus,\\nit is important to be able to save the LLM so that we don’t have to rerun the training\\nevery time we want to use it in a new session. \\n So, let’s discuss how to save and load a pretrained model, as highlighted in fig-\\nure 5.16. Later, we will load a more capable pretrained GPT model from OpenAI into\\nour GPTModel instance.\\nFortunately, saving a PyTorch model is re latively straightforward. The recommended\\nway is to save a model’s state_dict, a dictionary mapping each layer to its parameters,\\nusing the torch.save function:\\ntorch.save(model.state_dict(), \"model.pth\")\\n\"model.pth\" is the filename where the state_dict is saved. The .pth extension is a\\nconvention for PyTorch files, though we could technically use any file extension. \\n Then, after saving th e model weights via the state_dict, we can load the model\\nweights into a new GPTModel model instance:\\nExercise 5.3\\nWhat are the different combinations of settings for the generate function to force\\ndeterministic behavior, that is, disabling the random sampling such that it always pro-\\nduces the same outputs similar to the generate_simple function?\\n1) Text\\ngeneration\\n2) Text\\nevaluation\\n3) Training\\n& validation\\nlosses\\n4) LLM training\\nfunction\\n5) Text generation\\nstrategies\\n6) Weight saving &\\nloading\\n7) Pretrained weights\\nfrom OpenAI\\nTrain the model\\nto generate\\nhuman-like text\\nImplement additional LLM text\\ngeneration strategies to reduce\\ntraining data memorization\\nAt the end of this chapter,\\nload pretrained weights from\\nOpenAI into our LLM model\\nImplement functions to save and\\nload the LLM weights to use or\\ncontinue training the LLM later\\nFigure 5.16 After training and inspecting the model, it is often helpful to save the model so that \\nwe can use or continue training it later (step 6).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 181, 'page_label': '160'}, page_content='160 CHAPTER 5 Pretraining on unlabeled data\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\\nmodel.eval()\\nAs discussed in chapter 4, dropout helps prevent the model from overfitting to the\\ntraining data by randomly “dropping out” of a layer’s neurons during training. How-\\never, during inference, we don’t want to randomly drop out any of the information\\nthe network has learned. Using model.eval() switches the model to evaluation mode\\nfor inference, disabling the dropout layers of the model. If we plan to continue pre-\\ntraining a model later—for example, using the train_model_simple function we\\ndefined earlier in this chapter—saving the optimizer state is also recommended.\\n Adaptive optimizers such as AdamW stor e additional parameters for each model\\nweight. AdamW uses historical data to adjust learning rates for each model parameter\\ndynamically. Without it, the optimizer re sets, and the model may learn suboptimally\\nor even fail to converge properly, which means it will lose the ability to generate coher-\\nent text. Using torch.save, we can save both the model and optimizer state_dict\\ncontents:\\ntorch.save({\\n    \"model_state_dict\": model.state_dict(),\\n    \"optimizer_state_dict\": optimizer.state_dict(),\\n    }, \\n    \"model_and_optimizer.pth\"\\n)\\nThen we can restore the model and optimizer states by first loading the saved data via\\ntorch.load and then using the load_state_dict method:\\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nmodel.train();\\n5.5 Loading pretrained weights from OpenAI\\nPreviously, we trained a small GPT-2 model using a limited dataset comprising a short-\\nstory book. This approach allowed us to focus on the fundamentals without the need\\nfor extensive time and computational resources. \\nExercise 5.4\\nAfter saving the weights, load the mode l and optimizer in a new Python session or\\nJupyter notebook file and continue pretraining it for one more epoch using the\\ntrain_model_simple function. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 182, 'page_label': '161'}, page_content='1615.5 Loading pretrained weights from OpenAI\\n Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus elimi-\\nnating the need to invest tens to hundreds  of thousands of dollars in retraining the\\nmodel on a large corpus ourselves. So, let’s load these weights into our GPTModel class\\nand use the model for text generation. Here, weights refer to the weight parameters\\nstored in the .weight attributes of PyTorch’s Linear and Embedding layers, for exam-\\nple. We accessed them earlier via model.parameters() when training the model. In\\nchapter 6, will reuse these pretrained weights to fine-tune the model for a text classifi-\\ncation task and follow instructions similar to ChatGPT.\\n Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we\\nhave to install to load the weights in Py thon. The following code will use a progress\\nbar tool called tqdm to track the download process, which we also have to install.\\n You can install these libraries by executing the following command in your terminal:\\npip install tensorflow>=2.15.0  tqdm>=4.66\\nThe download code is relatively long, most ly boilerplate, and not very interesting.\\nHence, instead of devoting precious space to discussing Python code for fetching files\\nfrom the internet, we download the gpt_download.py Python module directly from\\nthis chapter’s online repository:\\nimport urllib.request\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/\"\\n    \"LLMs-from-scratch/main/ch05/\"\\n    \"01_main-chapter-code/gpt_download.py\"\\n)\\nfilename = url.split(\\'/\\')[-1]\\nurllib.request.urlretrieve(url, filename)\\nNext, after downloading this file to the lo cal directory of your Python session, you\\nshould briefly inspect the contents of this file to ensure that it was saved correctly and\\ncontains valid Python code.\\n We can now import the download_and_load_gpt2 function from the gpt_download\\n.py file as follows, which will load the GPT-2 architecture settings ( settings) and\\nweight parameters (params) into our Python session:\\nfrom gpt_download import download_and_load_gpt2\\nsettings, params = download_and_load_gpt2(\\n    model_size=\"124M\", models_dir=\"gpt2\"\\n)\\nExecuting this code downloads the foll owing seven files associated with the 124M\\nparameter GPT-2 model:\\ncheckpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, \\n                                                         63.9kiB/s]\\nencoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00,\\n                                                           2.20MiB/s]\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 183, 'page_label': '162'}, page_content='162 CHAPTER 5 Pretraining on unlabeled data\\nhprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00<00:00,\\n                                                         78.3kiB/s]\\nmodel.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09<00:00,\\n                                                         7.16MiB/s]\\nmodel.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00<00:00,\\n                                                           3.24MiB/s]\\nmodel.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, \\n                                                         2.46MiB/s]\\nvocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00,\\n                                                         1.70MiB/s]\\nNOTE If the download code does not work for you, it could be due to inter-\\nmittent internet connection, server problems, or changes in how OpenAI\\nshares the weights of the open-source GP T-2 model. In this case, please visit\\nthis chapter’s online code repository at https:/ /github.com/rasbt/LLMs-\\nfrom-scratch for alternative and updated inst ructions, and reach out via the\\nManning Forum for further questions. \\nAssuming the execution of the previous code has completed, let’s inspect the contents\\nof settings and params:\\nprint(\"Settings:\", settings)\\nprint(\"Parameter dictionary keys:\", params.keys())\\nThe contents are\\nSettings: {\\'n_vocab\\': 50257, \\'n_ctx\\': 1024, \\'n_embd\\': 768, \\'n_head\\': 12,\\n           \\'n_layer\\': 12}\\nParameter dictionary keys: dict_keys([\\'blocks\\', \\'b\\', \\'g\\', \\'wpe\\', \\'wte\\'])\\nBoth settings and params are Python dictionaries. The settings dictionary stores the\\nLLM architecture settings similarly to our manually defined GPT_CONFIG_124M settings.\\nThe params dictionary contains the actual weight  tensors. Note that we only printed\\nthe dictionary keys because printing the weight contents would take up too much\\nscreen space; however, we can inspect thes e weight tensors by printing the whole dic-\\ntionary via print(params) or by selecting individual te nsors via the respective dictio-\\nnary keys, for example, the embedding layer weights:\\nprint(params[\"wte\"])\\nprint(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\\nThe weights of the token embedding layer are\\n[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]\\n [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]\\n [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]\\n ...\\n [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]\\n [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 184, 'page_label': '163'}, page_content='1635.5 Loading pretrained weights from OpenAI\\n [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]\\nToken embedding weight tensor dimensions: (50257, 768)\\nWe downloaded and loaded the weights of the smallest GPT-2 model via the download_\\nand_load_gpt2(model_size=\"124M\", ...) setting. OpenAI also shares the weights of\\nlarger models: 355M, 774M, and 1558M. The overall architecture of these differently\\nsized GPT models is the same, as illustra ted in figure 5.17, except that different\\nRepeat this transformer block:\\n• 12 × in “gpt2-small”\\n• 24 × in “gpt2-medium”\\n• 36 × in “gpt2-large”\\n• 48 × in “gpt2-xl”\\nNumber of heads in\\nmulti-head attention:\\n• 12 in “gpt2-small”\\n• 16 in “gpt2-medium”\\n• 20 in “gpt2-large”\\n• 25 in gpt2-xl”“\\nEmbedding dimensions:\\n• 768 in “gpt2-small”\\n• 1,024 in “gpt2-medium”\\n• 1,280 in “gpt2-large”\\n• 1,600 in “gpt2-xl”\\nGPT\\nmodel\\nMasked multi-head\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nEvery effort moves you\\nFinal LayerNorm\\nLinear output layer\\nN\\nDropout\\nPositional embedding layer\\nTotal number of parameters:\\n• 124 M in “gpt2-small”\\n• 355 M in “gpt2-medium”\\n• 774 M in “gpt2-large”\\n• 1558 M in “gpt2-xl”\\nFigure 5.17 GPT-2 LLMs come in several different model sizes, ranging from 124 million to 1,558 million \\nparameters. The core architecture is the same, with the only difference being the embedding sizes and the number \\nof times individual components like the attention heads and transformer blocks are repeated.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 185, 'page_label': '164'}, page_content='164 CHAPTER 5 Pretraining on unlabeled data\\narchitectural elements are repeated diffe rent numbers of time s and the embedding\\nsize differs. The remaining code in this ch apter is also compatible with these larger\\nmodels.\\n After loading the GPT-2 model weights into  Python, we still need to transfer them\\nfrom the settings and params dictionaries into our GPTModel instance. First, we cre-\\nate a dictionary that lists the differences  between the different GPT model sizes in\\nfigure 5.17:\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nSuppose we are interested in loading the smallest model, \"gpt2-small (124M)\". We can\\nuse the corresponding settings from the model_configs table to update our full-length\\nGPT_CONFIG_124M we defined and used earlier:\\nmodel_name = \"gpt2-small (124M)\"\\nNEW_CONFIG = GPT_CONFIG_124M.copy()\\nNEW_CONFIG.update(model_configs[model_name])\\nCareful readers may remember that we used a 256-token length earlier, but the origi-\\nnal GPT-2 models from OpenAI were trained with a 1,024-token length, so we have to\\nupdate the NEW_CONFIG accordingly:\\nNEW_CONFIG.update({\"context_length\": 1024})\\nAlso, OpenAI used bias vectors in the mult i-head attention modu le’s linear layers to\\nimplement the query, key, and value matrix computations. Bias vectors are not com-\\nmonly used in LLMs anymore as they don’ t improve the modeling performance and\\nare thus unnecessary. However, since we are working with pretrained weights, we need\\nto match the settings for consistency and enable these bias vectors:\\nNEW_CONFIG.update({\"qkv_bias\": True})\\nWe can now use the updated NEW_CONFIG dictionary to initialize a new GPTModel\\ninstance:\\ngpt = GPTModel(NEW_CONFIG)\\ngpt.eval()\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 186, 'page_label': '165'}, page_content='1655.5 Loading pretrained weights from OpenAI\\nBy default, the GPTModel instance is initialized with random weights for pretraining.\\nThe last step to using OpenAI’s model weig hts is to override these random weights\\nwith the weights we loaded into the params dictionary. For this, we will first define a\\nsmall assign utility function that checks whether two tensors or arrays ( left and\\nright)  have the same dimensions or shape an d returns the right tensor as trainable\\nPyTorch parameters:\\ndef assign(left, right):\\n    if left.shape != right.shape:\\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\\n                          \"Right: {right.shape}\"\\n        )\\n    return torch.nn.Parameter(torch.tensor(right))\\nNext, we define a load_weights_into_gpt function that loads the weights from the\\nparams dictionary into a GPTModel instance gpt.\\nimport numpy as np\\ndef load_weights_into_gpt(gpt, params):          \\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\\'wpe\\'])\\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\\'wte\\'])\\n    \\n    for b in range(len(params[\"blocks\"])):    \\n        q_w, k_w, v_w = np.split(                           \\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\\n        gpt.trf_blocks[b].att.W_query.weight = assign(\\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\\n        gpt.trf_blocks[b].att.W_key.weight = assign(\\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\\n        gpt.trf_blocks[b].att.W_value.weight = assign(\\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\\n        q_b, k_b, v_b = np.split(\\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\\n        gpt.trf_blocks[b].att.W_query.bias = assign(\\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\\n        gpt.trf_blocks[b].att.W_key.bias = assign(\\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\\n        gpt.trf_blocks[b].att.W_value.bias = assign(\\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\\n            gpt.trf_blocks[b].att.out_proj.weight, \\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\\nListing 5.5 Loading OpenAI weights into our GPT model code\\nSets the model’s positional \\nand token embedding weights \\nto those specified in params.\\nIterates over each transformer block in the model\\nThe np.split function is used to divide the attention and bias weights\\ninto three equal parts for the query, key, and value components.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 187, 'page_label': '166'}, page_content='166 CHAPTER 5 Pretraining on unlabeled data\\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\\n            gpt.trf_blocks[b].att.out_proj.bias, \\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\\n            gpt.trf_blocks[b].ff.layers[0].weight, \\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\\n            gpt.trf_blocks[b].ff.layers[0].bias, \\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\\n            gpt.trf_blocks[b].ff.layers[2].weight, \\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\\n            gpt.trf_blocks[b].ff.layers[2].bias, \\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\\n        gpt.trf_blocks[b].norm1.scale = assign(\\n            gpt.trf_blocks[b].norm1.scale, \\n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\\n        gpt.trf_blocks[b].norm1.shift = assign(\\n            gpt.trf_blocks[b].norm1.shift, \\n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\\n        gpt.trf_blocks[b].norm2.scale = assign(\\n            gpt.trf_blocks[b].norm2.scale, \\n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\\n        gpt.trf_blocks[b].norm2.shift = assign(\\n            gpt.trf_blocks[b].norm2.shift, \\n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])   \\nIn the load_weights_into_gpt function, we carefully match the weights from\\nOpenAI’s implementation with our GPTModel implementation. To pick a specific\\nexample, OpenAI stored the weight tensor  for the output projection layer for the\\nfirst transformer block as params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"w\"] . In our\\nimplementation, this weight tensor corresponds to gpt.trf_blocks[b].att.out_proj\\n.weight, where gpt is a GPTModel instance.\\n Developing the load_weights_into_gpt function took a lot of guesswork since\\nOpenAI used a slightly different naming  convention from ours. However, the assign\\nfunction would alert us if we try to match two tensors with different dimensions. Also,\\nif we made a mistake in this function, we  would notice this, as the resulting GPT\\nmodel would be unable to produce coherent text.\\n Let’s now try the load_weights_into_gpt out in practice and load the OpenAI\\nmodel weights into our GPTModel instance gpt:\\nload_weights_into_gpt(gpt, params)\\ngpt.to(device)\\nThe original GPT-2 model\\nby OpenAI reused the token\\nembedding weights in the\\noutput layer to reduce the\\ntotal number of parameters,\\nwhich is a concept known as\\nweight tying.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 188, 'page_label': '167'}, page_content='167Summary\\nIf the model is loaded correctly, we can now use it to generate new text using our pre-\\nvious generate function:\\ntorch.manual_seed(123)\\ntoken_ids = generate(\\n    model=gpt,\\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\\n    max_new_tokens=25,\\n    context_size=NEW_CONFIG[\"context_length\"],\\n    top_k=50,\\n    temperature=1.5\\n)\\nprint(\"Output text:\\\\n\", token_ids_to_text(token_ids, tokenizer))\\nThe resulting text is as follows:\\nOutput text:\\n Every effort moves you toward finding an ideal new way to practice \\nsomething!\\nWhat makes us want to be on top of that?\\nWe can be confident that we loaded the model weights correctly because the model can\\nproduce coherent text. A tiny mistake in this process would cause the model to fail. In\\nthe following chapters, we will work further with this pretrained model and fine-tune it\\nto classify text and follow instructions.\\nSummary\\n\\uf0a1 When LLMs generate text, they output one token at a time.\\n\\uf0a1 By default, the next token is generated by converting the model outputs into\\nprobability scores and selecting the token from the vocabulary that corresponds\\nto the highest probability score, which is known as “greedy decoding.”\\n\\uf0a1 Using probabilistic sampling and temper ature scaling, we can influence the\\ndiversity and coherence of the generated text.\\n\\uf0a1 Training and validation set losses can be used to gauge the quality of text gener-\\nated by LLM during training.\\nExercise 5.5\\nCalculate the training and validation set losses of the GPTModel with the pretrained\\nweights from OpenAI on the “The Verdict” dataset.\\nExercise 5.6\\nExperiment with GPT-2 models of different sizes—for example, the largest 1,558 mil-\\nlion parameter model— and compare the generated text to the 124 million model. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 189, 'page_label': '168'}, page_content='168 CHAPTER 5 Pretraining on unlabeled data\\n\\uf0a1 Pretraining an LLM involves changing its weights to minimize the training loss.\\n\\uf0a1 T h e  t r a i n i n g  l o o p  f o r  L L M s  i t s e l f  i s  a standard procedure in deep learning,\\nusing a conventional cross entropy loss and AdamW optimizer.\\n\\uf0a1 Pretraining an LLM on a large text corpus is time- and resource-intensive, so we\\ncan load openly available weights as an  alternative to pretraining the model on\\na large dataset ourselves.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 190, 'page_label': '169'}, page_content='169\\nFine-tuning\\nfor classification\\nSo far, we have coded the LLM architecture, pretrained it, and learned how to\\nimport pretrained weights from an exte rnal source, such as OpenAI, into our\\nmodel. Now we will reap the fruits of our labor by fine-tuning the LLM on a specific\\ntarget task, such as classifying text. The concrete example we examine is classifying\\ntext messages as “spam” or “not spam.” Figure 6.1 highlights the two main ways of\\nfine-tuning an LLM: fine-tuning for classification (step 8) and fine-tuning to follow\\ninstructions (step 9).\\nThis chapter covers\\n\\uf0a1 Introducing different LLM fine-tuning approaches\\n\\uf0a1 Preparing a dataset for text classification\\n\\uf0a1 Modifying a pretrained LLM for fine-tuning \\n\\uf0a1 Fine-tuning an LLM to identify spam messages\\n\\uf0a1 Evaluating the accuracy of a fine-tuned LLM \\nclassifier\\n\\uf0a1 Using a fine-tuned LLM to classify new data\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 191, 'page_label': '170'}, page_content='170 CHAPTER 6 Fine-tuning for classification\\n6.1 Different categories of fine-tuning\\nThe most common ways to fine-tune language models are instruction fine-tuning  and\\nclassification fine-tuning. Instruction fine-tuning involves  training a language model on\\na set of tasks using specific instructions to  improve its ability to understand and exe-\\ncute tasks described in natural language prompts, as illustrated in figure 6.2. \\nIn classification fine-tuning, a concept yo u might already be acquainted with if you\\nhave a background in machine learning, th e model is trained to recognize a specific\\nIn this chapter, we will\\nﬁne-tune the pretrained\\nLLM to classify texts\\nIn chapter 5, we\\npretrained an LLM\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 6.1 The three main stages of coding an LLM. This chapter focus on stage 3 (step 8): fine-tuning a \\npretrained LLM as a classifier.\\nYes.\\nAdd instructions\\nfor the model\\nLLM\\nLLM\\nModel input Model output\\nAnswer with \\'yes\\' or \\'no\\'.\\nIs the following text ‘spam’?\\n“You are a winner you have been\\nspecially selected to receive $1000\\ncash or a $2000 award.”\\nTranslate into German:\\n“The quick brown fox\\njumps over the lazy dog.”\\n\"Der schnelle braune\\nFuchs springt über den\\nfaulen Hund.\"\\nFigure 6.2 Two different instruction fine-tuning scenario s. At the top, the model is tasked with determining \\nwhether a given text is spam. At the bottom, the model is given an instruction on how to translate an English \\nsentence into German.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 192, 'page_label': '171'}, page_content=\"1716.1 Different categories of fine-tuning\\nset of class labels, such as “spam” and “not spam.” Examples of classification tasks extend\\nbeyond LLMs and email filtering: they incl ude identifying different species of plants\\nfrom images; categorizing news articles into topics like sports, politics, and technology;\\nand distinguishing between benign and malignant tumors in medical imaging.\\n The key point is that a classification fi ne-tuned model is restricted to predicting\\nclasses it has encountered during its traini ng. For instance, it can determine whether\\nsomething is “spam” or “not spam,” as illustrated in figure 6.3, but it can’t say anything\\nelse about the input text. \\nIn contrast to the classification fine-tuned model depicted in figure 6.3, an instruction\\nfine-tuned model typically can undertake a broader range of tasks. We can view a clas-\\nsification fine-tuned model as highly specialized, and generally, it is easier to develop a\\nspecialized model than a generalist model that works well across various tasks.\\nChoosing the right approach \\nInstruction fine-tuning improves a model’s ability to understand and generate responses\\nbased on specific user instructions. Instruction fine-tuning is best suited for models\\nthat need to handle a variety of tasks based on complex user instructions, improving\\nflexibility and interaction quality. Classification fine-tuning is ideal for projects requir-\\ning precise categorization of data into predefined classes, such as sentiment analy-\\nsis or spam detection. \\nWhile instruction fine-tuning is more versatile, it demands larger datasets and greater\\ncomputational resources to develop models proficient in various tasks. In contrast,\\nclassification fine-tuning requires less data and compute power, but its use is con-\\nfined to the specific classes on which the model has been trained.\\nSpam\\nLLM\\nLLM\\nModel input without\\ninstructions\\nModel can only output\\ntwo types of responses:\\n“Spam” and “Not spam.”\\nNot\\nspam\\n“You are a winner you have been\\nspecially selected to receive $1000\\ncash or a $2000 award.”\\n“Hey, just wanted to check if\\nwe're still on for dinner tonight?\\nLet me know!”\\nFigure 6.3 A text classification scenario using an LLM. A model fine-tuned for spam \\nclassification does not require further instruction alongside the input. In contrast to \\nan instruction fine-tuned model, it can only respond with “spam” or “not spam.”\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 193, 'page_label': '172'}, page_content='172 CHAPTER 6 Fine-tuning for classification\\n6.2 Preparing the dataset\\nWe will modify and classification fine-tune the GPT model we previously implemented\\nand pretrained. We begin by downloading an d preparing the dataset, as highlighted\\nin figure 6.4. To provide an intuitive and useful example of classification fine-tuning,\\nwe will work with a text message dataset that consists of spam and non-spam messages. \\nNOTE Text messages typically sent via ph one, not email. However, the same\\nsteps also apply to email classification, and interested readers can find links to\\nemail spam classification datasets in appendix B.\\nThe first step is to download the dataset.\\nimport urllib.request\\nimport zipfile\\nimport os\\nfrom pathlib import Path\\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\\nzip_path = \"sms_spam_collection.zip\"\\nextracted_path = \"sms_spam_collection\"\\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\\ndef download_and_unzip_spam_data(\\n        url, zip_path, extracted_path, data_file_path):\\n    if data_file_path.exists():\\nListing 6.1 Downloading and unzipping the dataset\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nWe start with downloading,\\ninspecting, and preparing the\\ndataset that we will use to\\nﬁne-tune the model.\\nFigure 6.4 The three-stage process for classification fine-tuning an LLM. Stage 1 involves dataset \\npreparation. Stage 2 focuses on model setup. Stage 3 covers fine-tuning and evaluating the model.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 194, 'page_label': '173'}, page_content='1736.2 Preparing the dataset\\n        print(f\"{data_file_path} already exists. Skipping download \"\\n              \"and extraction.\"\\n        )\\n        return\\n    with urllib.request.urlopen(url) as response:   \\n        with open(zip_path, \"wb\") as out_file:\\n            out_file.write(response.read())\\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \\n        zip_ref.extractall(extracted_path)\\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\\n    os.rename(original_file_path, data_file_path)              \\n    print(f\"File downloaded and saved as {data_file_path}\")\\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\nAfter executing the preceding code, the datase t is saved as a tab-separated text file,\\nSMSSpamCollection.tsv, in the sms_spam_collection folder. We can load it into a\\npandas DataFrame as follows:\\nimport pandas as pd\\ndf = pd.read_csv(\\n    data_file_path, sep=\"\\\\t\", header=None, names=[\"Label\", \"Text\"]\\n)\\ndf     \\nFigure 6.5 shows the resulting data frame of the spam dataset.\\nLet’s examine the class label distribution:\\nprint(df[\"Label\"].value_counts())\\nDownloads \\nthe file\\nUnzips the file\\nAdds a .tsv \\nfile extension\\nRenders the data frame in a Jupyter \\nnotebook. Alternatively, use print(df).\\nFigure 6.5 Preview of the \\nSMSSpamCollection dataset \\nin a pandas DataFrame, showing \\nclass labels (“ham” or “spam”) and \\ncorresponding text messages. The \\ndataset consists of 5,572 rows \\n(text messages and labels).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 195, 'page_label': '174'}, page_content='174 CHAPTER 6 Fine-tuning for classification\\nExecuting the previous code, we find that the data contains “ham” (i.e., not spam) far\\nmore frequently than “spam”:\\nLabel\\nham     4825\\nspam     747\\nName: count, dtype: int64\\nFor simplicity, and because we prefer a small dataset (which will facilitate faster fine-\\ntuning of the LLM), we choose to undersam ple the dataset to include 747 instances\\nfrom each class. \\nNOTE There are several other methods to handle class imbalances, but these\\nare beyond the scope of this book. Readers interested in exploring methods for\\ndealing with imbalanced data can find additional information in appendix B.\\nWe can use the code in the following list ing to undersample and create a balanced\\ndataset.\\ndef create_balanced_dataset(df):\\n    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \\n    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\\n        num_spam, random_state=123\\n    )                                        \\n    balanced_df = pd.concat([\\n        ham_subset, df[df[\"Label\"] == \"spam\"]\\n    ])                              \\n    return balanced_df\\nbalanced_df = create_balanced_dataset(df)\\nprint(balanced_df[\"Label\"].value_counts())\\nAfter executing the previous code to balance the dataset, we can see that we now have\\nequal amounts of spam and non-spam messages:\\nLabel\\nham     747\\nspam    747\\nName: count, dtype: int64\\nNext, we convert the “string” class labels \"ham\" and \"spam\" into integer class labels 0\\nand 1, respectively:\\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\\nThis process is similar to converting text into token IDs. However, instead of using the\\nGPT vocabulary, which consists of more than 50,000 words, we are dealing with just\\ntwo token IDs: 0 and 1.\\nListing 6.2 Creating a balanced dataset\\nCounts the instances \\nof “spam”\\nRandomly samples “ham” \\ninstances to match the number \\nof “spam” instances\\nCombines ham \\nsubset with “spam”\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 196, 'page_label': '175'}, page_content='1756.3 Creating data loaders\\n Next, we create a random_split function to split the dataset into three parts: 70%\\nfor training, 10% for validation, and 20% fo r testing. (These ratios are common in\\nmachine learning to train, adjust, and evaluate models.)\\ndef random_split(df, train_frac, validation_frac):\\n    \\n    df = df.sample(\\n        frac=1, random_state=123\\n    ).reset_index(drop=True)              \\n    train_end = int(len(df) * train_frac)         \\n    validation_end = train_end + int(len(df) * validation_frac)\\n                                 \\n    train_df = df[:train_end]\\n    validation_df = df[train_end:validation_end]\\n    test_df = df[validation_end:]\\n    return train_df, validation_df, test_df\\ntrain_df, validation_df, test_df = random_split(\\n    balanced_df, 0.7, 0.1)                    \\nLet’s save the dataset as CSV (comma-separated value) files so we can reuse it later:\\ntrain_df.to_csv(\"train.csv\", index=None)\\nvalidation_df.to_csv(\"validation.csv\", index=None)\\ntest_df.to_csv(\"test.csv\", index=None)\\nThus far, we have downloaded the dataset, balanced it, and split it into training and\\nevaluation subsets. Now we will set up the PyTorch data loaders that will be used to\\ntrain the model.\\n6.3 Creating data loaders\\nWe will develop PyTorch data loaders conc eptually similar to those we implemented\\nwhile working with text data. Previously, we utilized a sliding window technique to\\ngenerate uniformly sized text chunks, whic h we then grouped into batches for more\\nefficient model training. Each chunk function ed as an individual training instance.\\nHowever, we are now working with a spam da taset that contains text messages of vary-\\ning lengths. To batch these messages as we did with the text chunks, we have two pri-\\nmary options:\\n\\uf0a1 Truncate all messages to the length of the shortest message in the dataset or batch.\\n\\uf0a1 Pad all messages to the length of the longest message in the dataset or batch.\\nThe first option is computationally cheaper,  but it may result in significant informa-\\ntion loss if shorter messages are much small er than the average or longest messages,\\nListing 6.3 Splitting the dataset\\nShuffles the entire \\nDataFrame\\nCalculates \\nsplit indices\\nSplits the DataFrame\\nTest size is implied \\nto be 0.2 as the \\nremainder.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 197, 'page_label': '176'}, page_content='176 CHAPTER 6 Fine-tuning for classification\\npotentially reducing model performance. So, we opt for the second option, which\\npreserves the entire content of all messages.\\n To implement batching, where all message s are padded to the length of the lon-\\ngest message in the dataset, we add padding  tokens to all shorter messages. For this\\npurpose, we use \"<|endoftext|>\" as a padding token. \\n However, instead of appending the string \"<|endoftext|>\" t o  e a c h  o f  t h e  t e x t\\nmessages directly, we can add the token ID corresponding to \"<|endoftext|>\" to the\\nencoded text messages, as illustrated in figure 6.6. 50256 is the token ID of the padding\\ntoken \"<|endoftext|>\". We can double-check whethe r the token ID is correct by\\nencoding the \"<|endoftext|>\" using the GPT-2 tokenizer from the tiktoken package\\nthat we used previously:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\\nIndeed, executing the preceding code returns [50256].\\n We first need to implement a PyTorch Dataset, which specifies how the data is\\nloaded and processed before we can instan tiate the data loaders. For this purpose,\\nwe define the SpamDataset class, which implements the concepts in figure 6.6. This\\nSpamDataset class handles several key tasks: it id entifies the longest sequence in the\\ntraining dataset, encodes the text messages,  and ensures that all other sequences are\\npadded with a padding token  to match the length of the longest sequence.\\n \\nThis is the first text\\nmessage\\nThis is another text\\nmessage\\nThis is the third text\\nmessage, which is\\nvery long\\n1212, 318, 262, 717,\\n2420, 3275\\n1212, 318, 1194,\\n2420, 3275\\n1212, 318, 262, 2368,\\n2420, 3275, 11, 543, 318,\\n845, 890\\n1. Tokenize\\ntexts\\nInput text\\nmessage\\nToken IDs\\n2. Pad to longest\\nsequence\\n1212, 318, 262, 2368,\\n2420, 3275, 11, 543,\\n318, 845, 890\\n1212, 318, 262, 717,\\n2420, 3275, 50256, 50256,\\n50256, 50256, 50256\\nPadded\\ntoken IDs\\n1212, 318, 1194, 2420,\\n3275, 50256, 50256, 50256,\\n50256, 50256, 50256\\nWe use 50256 as\\na padding token.\\nNo padding in this last\\nexample because it is\\nthe longest message\\nFigure 6.6 The input text preparation process. First, each input text message is converted into a sequence of \\ntoken IDs. Then, to ensure uniform sequence lengths, shorter sequences are padded with a padding token (in this \\ncase, token ID 50256) to match the length of the longest sequence.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 198, 'page_label': '177'}, page_content='1776.3 Creating data loaders\\nimport torch\\nfrom torch.utils.data import Dataset\\nclass SpamDataset(Dataset):\\n    def __init__(self, csv_file, tokenizer, max_length=None,\\n                 pad_token_id=50256):\\n        self.data = pd.read_csv(csv_file)\\n                                            \\n        self.encoded_texts = [\\n            tokenizer.encode(text) for text in self.data[\"Text\"]\\n        ]\\n        if max_length is None:\\n            self.max_length = self._longest_encoded_length()\\n        else:\\n            self.max_length = max_length\\n                                           \\n            self.encoded_texts = [\\n                encoded_text[:self.max_length]\\n                for encoded_text in self.encoded_texts\\n            ]\\n                                         \\n        self.encoded_texts = [\\n            encoded_text + [pad_token_id] * \\n            (self.max_length - len(encoded_text))\\n            for encoded_text in self.encoded_texts\\n        ]\\n    def __getitem__(self, index):\\n        encoded = self.encoded_texts[index]\\n        label = self.data.iloc[index][\"Label\"]\\n        return (\\n            torch.tensor(encoded, dtype=torch.long),\\n            torch.tensor(label, dtype=torch.long)\\n        )\\n    def __len__(self):\\n        return len(self.data)\\n    def _longest_encoded_length(self):\\n        max_length = 0\\n        for encoded_text in self.encoded_texts:\\n            encoded_length = len(encoded_text)\\n            if encoded_length > max_length:\\n                max_length = encoded_length\\n        return max_length\\nListing 6.4 Setting up a Pytorch Dataset class\\nPretokenizes texts\\nTruncates sequences if they \\nare longer than max_length\\nPads sequences to \\nthe longest sequence\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 199, 'page_label': '178'}, page_content='178 CHAPTER 6 Fine-tuning for classification\\nThe SpamDataset class loads data from the CSV files we created earlier, tokenizes\\nthe text using the GPT-2 tokenizer from tiktoken, and allows us to pad or truncate\\nthe sequences to a uniform length determin ed by either the lo ngest sequence or a\\npredefined maximum length. This ensures ea ch input tensor is of the same size,\\nwhich is necessary to create the batches in  the training data loader we implement\\nnext:\\ntrain_dataset = SpamDataset(\\n    csv_file=\"train.csv\",\\n    max_length=None,\\n    tokenizer=tokenizer\\n)\\nThe longest sequence length is stored in the dataset’s max_length attribute. If you are\\ncurious to see the number of tokens in th e longest sequence, you can use the follow-\\ning code:\\nprint(train_dataset.max_length)\\nThe code outputs 120, showing that the longest sequ ence contains no more than\\n120 tokens, a common length for text messages. The model can handle sequences\\nof up to 1,024 tokens, given its context le ngth limit. If your dataset includes longer\\ntexts, you can pass max_length=1024 when creating the trai ning dataset in the pre-\\nceding code to ensure that  the data does not exceed the model’s supported input\\n(context) length.\\n Next, we pad the validation and test sets  to match the length of the longest train-\\ning sequence. Importantly, any validation and test set samples exceeding the length of\\nthe longest training example are truncated using encoded_text[:self.max_length]\\nin the SpamDataset code we defined earlier. This truncation is optional; you can set\\nmax_length=None for both validation and test sets , provided there are no sequences\\nexceeding 1,024 tokens in these sets:\\nval_dataset = SpamDataset(\\n    csv_file=\"validation.csv\",\\n    max_length=train_dataset.max_length,\\n    tokenizer=tokenizer\\n)\\ntest_dataset = SpamDataset(\\n    csv_file=\"test.csv\",\\n    max_length=train_dataset.max_length,\\n    tokenizer=tokenizer\\n)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 200, 'page_label': '179'}, page_content='1796.3 Creating data loaders\\nUsing the datasets as inputs, we can instan tiate the data loaders similarly to when we\\nwere working with text data. However, in this case, the targets represent class labels\\nrather than the next tokens in the text. For instance, if we choose a batch size of 8,\\neach batch will consist of eight training examples of length 120 and the correspond-\\ning class label of each example, as illustrated in figure 6.7.\\nExercise 6.1 Increasing the context length \\nPad the inputs to the maximum number of  tokens the model supports and observe\\nhow it affects the predictive performance.\\n20266, 40621, 15762, ..., 50256\\n2953,   644,   640, ..., 50256\\n55, 31180, 15895, ..., 50256\\nAt what time are\\nyou ...\\nXMAS Prize\\ndraws!  ...\\nDear voucher\\nholder ...\\nK. I will sent it\\nagain ... 42,    13,   314, ..., 50256\\n1\\n2\\n3\\n8\\n0\\n1\\n1\\n0\\nOriginal message text\\nEach batch consists of\\neight training examples.\\nEach entry (row) represents\\nthe token IDs corresponding\\nto the original message text\\nThe class label of the\\neighth training example\\nClass label array, where 1\\nstands for “spam” and 0\\nfor “ham” (not spam)\\nEach text message is\\npadded to 120 tokens.\\n1 2 3 120\\nFigure 6.7 A single training batch consisting of eight text messages represented as token IDs. Each \\ntext message consists of 120 token IDs. A class label array stores the eight class labels corresponding \\nto the text messages, which can be either 0 (“not spam”) or 1 (“spam”).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 201, 'page_label': '180'}, page_content='180 CHAPTER 6 Fine-tuning for classification\\nThe code in the following listing creates the training, validation, and test set data load-\\ners that load the text messages and labels in batches of size 8.\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0     \\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=num_workers,\\n    drop_last=True,\\n)\\nval_loader = DataLoader(\\n    dataset=val_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\ntest_loader = DataLoader(\\n    dataset=test_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\nTo ensure that the data loaders are working and are, indeed, returning batches of the\\nexpected size, we iterate over the training  loader and then print the tensor dimen-\\nsions of the last batch:\\nfor input_batch, target_batch in train_loader:\\n    pass\\nprint(\"Input batch dimensions:\", input_batch.shape)\\nprint(\"Label batch dimensions\", target_batch.shape)\\nThe output is\\nInput batch dimensions: torch.Size([8, 120])\\nLabel batch dimensions torch.Size([8])\\nAs we can see, the input batches consist of  eight training examples with 120 tokens\\neach, as expected. The label tensor stores the class labels corresponding to the eight\\ntraining examples.\\n Lastly, to get an idea of the dataset size , let’s print the total number of batches in\\neach dataset:\\nListing 6.5 Creating PyTorch data loaders\\nThis setting ensures compatibility \\nwith most computers.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 202, 'page_label': '181'}, page_content='1816.4 Initializing a model with pretrained weights\\nprint(f\"{len(train_loader)} training batches\")\\nprint(f\"{len(val_loader)} validation batches\")\\nprint(f\"{len(test_loader)} test batches\")\\nThe number of batches in each dataset are\\n130 training batches\\n19 validation batches\\n38 test batches\\nNow that we’ve prepared the data, we need to prepare the model for fine-tuning.\\n6.4 Initializing a model with pretrained weights\\nWe must prepare the model for classificati on fine-tuning to id entify spam messages.\\nWe start by initializing our pretrained model, as highlighted in figure 6.8.\\nTo begin the model preparation process, we employ the same configurations we used\\nto pretrain unlabeled data:\\nCHOOSE_MODEL = \"gpt2-small (124M)\"\\nINPUT_PROMPT = \"Every effort moves\"\\nIn this section, we initialize the\\npretrained model from the previous\\nchapter that we will ﬁne-tune.\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nIn the previous section,\\nwe prepared the SPAM\\nprediction dataset for\\nclassiﬁcation ﬁne-tuning.\\nFigure 6.8 The three-stage process for classificatio n fine-tuning the LLM. Having completed stage 1, \\npreparing the dataset, we now must initialize the LLM, which we will then fine-tune to classify spam \\nmessages.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 203, 'page_label': '182'}, page_content='182 CHAPTER 6 Fine-tuning for classification\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,         \\n    \"context_length\": 1024,      \\n    \"drop_rate\": 0.0,            \\n    \"qkv_bias\": True             \\n}\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nNext, we import the download_and_load_gpt2 function from the gpt_download.py\\nfile and reuse the GPTModel class and load_weights_into_gpt function from pretrain-\\ning (see chapter 5) to load the downloaded weights into the GPT model.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter05 import GPTModel, load_weights_into_gpt\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval()\\nAfter loading the model weights into the GPTModel, we reuse the text generation util-\\nity function from chapters 4 and 5 to ensure that the model generates coherent text:\\nfrom chapter04 import generate_text_simple\\nfrom chapter05 import text_to_token_ids, token_ids_to_text\\ntext_1 = \"Every effort moves you\"\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_1, tokenizer),\\n    max_new_tokens=15,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe following output shows the model generates coherent text, which is indicates that\\nthe model weights have been loaded correctly:\\nEvery effort moves you forward.\\nThe first step is to understand the importance of your work\\nListing 6.6 Loading a pretrained GPT model\\nVocabulary size\\nContext length\\nDropout rate\\nQuery-key-value bias\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 204, 'page_label': '183'}, page_content='1836.5 Adding a classification head\\nBefore we start fine-tuning the model as a spam classifier, let’s see whether the model\\nalready classifies spam messages by prompting it with instructions:\\ntext_2 = (\\n    \"Is the following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\':\"\\n    \" \\'You are a winner you have been specially\"\\n    \" selected to receive $1000 cash or a $2000 award.\\'\"\\n)\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_2, tokenizer),\\n    max_new_tokens=23,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe model output is\\nIs the following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\': \\'You are a winner\\nyou have been specially selected to receive $1000 cash \\nor a $2000 award.\\'\\nThe following text \\'spam\\'? Answer with \\'yes\\' or \\'no\\': \\'You are a winner\\nBased on the output, it’s apparent that the model is struggling to follow instructions.\\nThis result is expected, as it has only undergone pretraining and lacks instruction\\nfine-tuning. So, let’s prepare the model for classification fine-tuning.\\n6.5 Adding a classification head\\nWe must modify the pretrained LLM to prepare it for classification fine-tuning. To do\\nso, we replace the original ou tput layer, which maps the hidden representation to a\\nvocabulary of 50,257, with a smaller outp ut layer that maps to two classes: 0 (“not\\nspam”) and 1 (“spam”), as shown in figure 6.9. We use the same model as before, except\\nwe replace the output layer.\\n \\nOutput layer nodes \\nWe could technically use a single output node since we are dealing with a binary clas-\\nsification task. However, it would require modifying the loss function, as I discuss in\\n“Losses Learned—Optimizing Negative Log- Likelihood and Cross-Entropy in PyTorch”\\n(https:/ /mng.bz/NRZ2). Therefore, we choose a more general approach, where the\\nnumber of output nodes matches the number of classes. For example, for a three-\\nclass problem, such as classifying news articles as “Technology,” “Sports,” or “Pol-\\nitics,” we would use three output nodes, and so forth.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 205, 'page_label': '184'}, page_content='184 CHAPTER 6 Fine-tuning for classification\\nThe GPT model we implemented\\nin chapter 5 and loaded in the\\nprevious section\\nOutputs\\nThe original linear output layer mapped 768\\nhidden units to 50,257 units (the number of\\ntokens in the vocabulary).\\n1 768\\n50,2571\\n1 768\\n1 2\\nWe replace the original linear output layer above\\nwith a layer that maps from 768 hidden units to\\nonly 2 units, where the 2 units represent the two\\nclasses (\"spam\" and \"not spam\").\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nInputs\\nFigure 6.9 Adapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear \\noutput layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer \\nwith a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not \\nspam.”\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 206, 'page_label': '185'}, page_content='1856.5 Adding a classification head\\nBefore we attempt the modification shown in figure 6.9, let’s print the model architec-\\nture via print(model):\\nGPTModel(\\n  (tok_emb): Embedding(50257, 768)\\n  (pos_emb): Embedding(1024, 768)\\n  (drop_emb): Dropout(p=0.0, inplace=False)\\n  (trf_blocks): Sequential(\\n...\\n    (11): TransformerBlock(\\n      (att): MultiHeadAttention(\\n        (W_query): Linear(in_features=768, out_features=768, bias=True)\\n        (W_key): Linear(in_features=768, out_features=768, bias=True)\\n        (W_value): Linear(in_features=768, out_features=768, bias=True)\\n        (out_proj): Linear(in_features=768, out_features=768, bias=True)\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n      (ff): FeedForward(\\n        (layers): Sequential(\\n          (0): Linear(in_features=768, out_features=3072, bias=True)\\n          (1): GELU()\\n          (2): Linear(in_features=3072, out_features=768, bias=True)\\n        )\\n      )\\n      (norm1): LayerNorm()\\n      (norm2): LayerNorm()\\n      (drop_resid): Dropout(p=0.0, inplace=False)\\n    )\\n  )\\n  (final_norm): LayerNorm()\\n  (out_head): Linear(in_features=768, out_features=50257, bias=False)\\n)\\nThis output neatly lays out the architecture we laid out in chapter 4. As previously dis-\\ncussed, the GPTModel consists of embedding layers followed by 12 identical transformer\\nblocks (only the last block is shown for brevity), followed by a final LayerNorm and the\\noutput layer, out_head. \\n Next, we replace the out_head with a new output layer (see figure 6.9) that we will\\nfine-tune.\\nFine-tuning selected layers vs. all layers\\nSince we start with a pretrained model, it’s not necessary to fine-tune all model layers.\\nIn neural network-based language models, the lower layers generally capture basic lan-\\nguage structures and semantics applicable across a wide range of tasks and datasets.\\nSo, fine-tuning only the last layers (i.e., layers near the output), which are more specific\\nto nuanced linguistic patterns and task-specific features, is often sufficient to adapt the\\nmodel to new tasks. A nice side effect is that it is computationally more efficient to fine-\\ntune only a small number of layers. Interested readers can find more information,\\nincluding experiments, on which layers to fine-tune in appendix B.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 207, 'page_label': '186'}, page_content='186 CHAPTER 6 Fine-tuning for classification\\nTo get the model ready for classification fine-tuning, we first freeze the model, meaning\\nthat we make all layers nontrainable:\\nfor param in model.parameters():\\n    param.requires_grad = False\\nThen, we replace the output layer ( model.out_head), which originally maps the layer\\ninputs to 50,257 dimensions, the size of the vocabulary (see figure 6.9).\\ntorch.manual_seed(123)\\nnum_classes = 2\\nmodel.out_head = torch.nn.Linear(\\n    in_features=BASE_CONFIG[\"emb_dim\"], \\n    out_features=num_classes\\n)\\nTo keep the code more general, we use BASE_CONFIG[\"emb_dim\"], which is equal to\\n768 in the \"gpt2-small (124M)\" model. Thus, we can also use the same code to work\\nwith the larger GPT-2 model variants.\\n This new model.out_head output layer has its requires_grad attribute set to\\nTrue by default, which means that it’s th e only layer in the model that will be\\nupdated during training. Technically, training the output layer we just added is suffi-\\ncient. However, as I found in experiments,  fine-tuning additional layers can notice-\\nably improve the predictive performance of  the model. (For more details, refer to\\nappendix B.) We also configure the last transformer block and the final LayerNorm\\nmodule, which connects this block to the ou tput layer, to be trainable, as depicted\\nin figure 6.10.\\n To make the final LayerNorm and last transformer bloc k trainable, we set their\\nrespective requires_grad to True:\\nfor param in model.trf_blocks[-1].parameters():\\n    param.requires_grad = True\\nfor param in model.final_norm.parameters():\\n    param.requires_grad = True\\nEven though we added a new output layer and marked certain layers as trainable or\\nnontrainable, we can still use this model similarly to how we have previously. For\\nListing 6.7 Adding a classification layer\\nExercise 6.2 Fine-tuning the whole model \\nInstead of fine-tuning just the final transformer block, fine-tune the entire model and\\nassess the effect on predictive performance.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 208, 'page_label': '187'}, page_content='1876.5 Adding a classification head\\ninstance, we can feed it an example text identical to our previously used example\\ntext:\\ninputs = tokenizer.encode(\"Do you have time\")\\ninputs = torch.tensor(inputs).unsqueeze(0)\\nprint(\"Inputs:\", inputs)\\nprint(\"Inputs dimensions:\", inputs.shape)   \\nAs we saw in chapter 5,\\nthis transformer block\\nis repeated 12x in the\\n124M-parameter GPT-2\\nmodel\\nWe make the output layer,\\nﬁnal LayerNorm, and the last\\ntransformer block trainable\\nThe GPT model we\\nimplemented in chapter 5\\nand loaded in the previous\\nsection\\nOutputs\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nInputs\\nTransformer\\nblock\\nFigure 6.10 The GPT model includes 12 repeated transformer blocks. Alongside the output layer, we set the final \\nLayerNorm and the last transformer block as trainable. The remaining 11 transformer blocks and the embedding \\nlayers are kept nontrainable.\\nshape: (batch_size, \\nnum_tokens)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 209, 'page_label': '188'}, page_content='188 CHAPTER 6 Fine-tuning for classification\\nThe print output shows that the preceding code encodes the inputs into a tensor con-\\nsisting of four input tokens:\\nInputs: tensor([[5211,  345,  423,  640]])\\nInputs dimensions: torch.Size([1, 4])\\nThen, we can pass the encoded token IDs to the model as usual:\\nwith torch.no_grad():\\n    outputs = model(inputs)\\nprint(\"Outputs:\\\\n\", outputs)\\nprint(\"Outputs dimensions:\", outputs.shape)\\nThe output tensor looks like the following:\\nOutputs:\\n tensor([[[-1.5854,  0.9904],\\n          [-3.7235,  7.4548],\\n          [-2.2661,  6.6049],\\n          [-3.5983,  3.9902]]])\\nOutputs dimensions: torch.Size([1, 4, 2])\\nA similar input would have previously produced an output tensor of [1, 4, 50257],\\nwhere 50257 represents the vocabulary size. The number of output rows corresponds\\nto the number of input tokens (in this case, four). However, each output’s embedding\\ndimension (the number of columns) is now 2 instead of 50,257 since we replaced the\\noutput layer of the model.\\n Remember that we are interested in fine -tuning this model to return a class label\\nindicating whether a model input is “spam” or “not spam.” We don’t need to fine-\\ntune all four output rows; instead, we can focus on a single output token. In particu-\\nlar, we will focus on the last row correspondi ng to the last output token, as shown in\\nfigure 6.11.\\n To extract the last output token from the output tensor, we use the following code:\\nprint(\"Last output token:\", outputs[:, -1, :])\\nThis prints\\nLast output token: tensor([[-3.5983,  3.9902]])\\nWe still need to convert the values into a class-label prediction. But first, let’s under-\\nstand why we are particularly interested in the last output token only.\\n We have already explored the attention mechanism, which establishes a relationship\\nbetween each input token and every othe r input token, and the concept of a causal\\nattention mask, commonly used in GPT-like models (see chapter 3). This mask restricts a\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 210, 'page_label': '189'}, page_content='1896.5 Adding a classification head\\n[[-1.5854,  0.9904],\\n[-3.7235,  7.4548],\\n[-2.2661,  6.6049],\\n[-3.5983,  3.9902]]\\nA 4 × 2–dimensional tensor\\nThe number of rows corresponds\\nto the number of input tokens,\\nas discussed in chapter 4.\\nThe last row corresponds\\nto the last token.\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nDo you have time\\nThis transformer block\\nis repeated 12x in the\\n124M-parameter GPT-2\\nmodel.\\nThe GPT model we\\nimplemented in chapter 5\\nand loaded in the previous\\nsection\\nFigure 6.11 The GPT model with a four-token example input and output. The output tensor consists of \\ntwo columns due to the modified output layer. We are only interested in the last row corresponding to \\nthe last token when fine-tuning the model for spam classification.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 211, 'page_label': '190'}, page_content='190 CHAPTER 6 Fine-tuning for classification\\ntoken’s focus to its current position and the those before it, ensuring that each token\\ncan only be influenced by itself and the preceding tokens, as illustrated in figure 6.12.\\nGiven the causal attention mask setup in figure 6.12, the last token in a sequence accu-\\nmulates the most information since it is the only token with access to data from all the\\nprevious tokens. Therefore, in our spam classification task, we focus on this last token\\nduring the fine-tuning process.\\n We are now ready to transform the last token into class label predictions and calcu-\\nlate the model’s initial prediction accuracy. Subsequently, we will fine-tune the model\\nfor the spam classification task.\\n6.6 Calculating the classification loss and accuracy\\nOnly one small task remains before we fi ne-tune the model: we must implement the\\nmodel evaluation functions used during fine-tuning, as illustrated in figure 6.13.\\n Before implementing the evaluation utilit ies, let’s briefly discuss how we convert\\nthe model outputs into class label predicti ons. We previously computed the token ID\\nof the next token generated by the LLM by converting the 50,257 outputs into proba-\\nbilities via the softmax function and then returning the position of the highest proba-\\nbility via the argmax function. We take the same appr oach here to calculate whether\\nthe model outputs a “spam” or  “not spam” prediction for a given input, as shown in\\nfigure 6.14. The only difference is that we  work with 2-dimensional instead of 50,257-\\ndimensional outputs. \\n \\nExercise 6.3 Fine-tuning the first vs. last token \\nTry fine-tuning the first output token. Noti ce the changes in predictive performance\\ncompared to fine-tuning the last output token.\\nDo\\nhave\\ntime\\nyou 0.55 0.45\\n1.0\\n0.38 0.30 0.32\\n0.27 0.24 0.24 0.25\\nDo have timeyou\\nTokens masked out via\\nthe causal attention mask.\\nThe last token is the only\\ntoken with an attention\\nscore to all other tokens.\\nFigure 6.12 The causal attention \\nmechanism, where the attention scores \\nbetween input tokens are displayed in a \\nmatrix format. The empty cells indicate \\nmasked positions due to the causal attention \\nmask, preventing tokens from attending to \\nfuture tokens. The values in the cells \\nrepresent attention scores; the last token, \\ntime, is the only one that computes \\nattention scores for all preceding tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 212, 'page_label': '191'}, page_content=\"1916.6 Calculating the classification loss and accuracy\\nImplement the utility\\nfunction to calculate the\\nclassiﬁcation loss and\\naccuracy of the model.\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n7) Implement\\nevaluation utilities\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nFigure 6.13 The three-stage process for classification fine-tuning the LLM. \\nWe've completed the first six steps. We are now ready to undertake the last step \\nof stage 2: implementing the functions to evaluate the model’s performance to \\nclassify spam messages before, during, and after the fine-tuning.\\nYou won the lottery\\nDo you have time\\nInput text\\nmessage\\n[ -3.9846,  5.2940 ]\\nLLM\\nLLM\\nOutputs corresponding\\nto the last row (token)\\n0        1Index position:\\n2. Locate the index position with\\nthe highest probability value in\\neach row vector, which is done\\nvia the function.\\nargmax\\n[ 3.5983,  -3.9902 ]\\n[ 0.01,   0.99 ]\\n[ 0.99,   0.01 ]\\n(spam)(not spam)\\n10\\n11 (spam)\\n(not spam)\\n1. Convert outputs to\\nsoftmax probabilities.\\nThe predicted\\nlabels\\nFigure 6.14 The model outputs corresponding to the last  token are converted into probability scores for each \\ninput text. The class labels are obtained by looking up the index position of the highest probability score. The \\nmodel predicts the spam labels incorrectly because it has not yet been trained.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 213, 'page_label': '192'}, page_content='192 CHAPTER 6 Fine-tuning for classification\\nLet’s consider the last token output using a concrete example:\\nprint(\"Last output token:\", outputs[:, -1, :])\\nThe values of the tensor corresponding to the last token are\\nLast output token: tensor([[-3.5983,  3.9902]])\\nWe can obtain the class label:\\nprobas = torch.softmax(outputs[:, -1, :], dim=-1)\\nlabel = torch.argmax(probas)\\nprint(\"Class label:\", label.item())\\nIn this case, the code returns 1, meaning the model predicts that the input text is\\n“spam.” Using the softmax function here is optional because the largest outputs\\ndirectly correspond to the highest probability scores. Hence, we can simplify the code\\nwithout using softmax:\\nlogits = outputs[:, -1, :]\\nlabel = torch.argmax(logits)\\nprint(\"Class label:\", label.item())\\nThis concept can be used to compute the classification accuracy, which measures the\\npercentage of correct predictions across a dataset.\\n To determine the classifica tion accuracy, we apply the argmax-based prediction\\ncode to all examples in the dataset and calculate the proportion of correct predictions\\nby defining a calc_accuracy_loader function.\\ndef calc_accuracy_loader(data_loader, model, device, num_batches=None):\\n    model.eval()\\n    correct_predictions, num_examples = 0, 0\\n    if num_batches is None:\\n        num_batches = len(data_loader)\\n    else:\\n        num_batches = min(num_batches, len(data_loader))\\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            input_batch = input_batch.to(device)\\n            target_batch = target_batch.to(device)\\n            with torch.no_grad():\\n                logits = model(input_batch)[:, -1, :]    \\n            predicted_labels = torch.argmax(logits, dim=-1)\\n            num_examples += predicted_labels.shape[0]\\n            correct_predictions += (\\nListing 6.8 Calculating the classification accuracy\\nLogits of last \\noutput token\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 214, 'page_label': '193'}, page_content='1936.6 Calculating the classification loss and accuracy\\n                (predicted_labels == target_batch).sum().item()\\n            )\\n        else:\\n            break\\n    return correct_predictions / num_examples\\nLet’s use the function to determine the classification accuracies across various datasets\\nestimated from 10 batches for efficiency:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nVia the device setting, the model automatically ru ns on a GPU if a GPU with Nvidia\\nCUDA support is available and otherwise runs on a CPU. The output is\\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nAs we can see, the prediction accuracies are near a random prediction, which would be\\n50% in this case. To improve the prediction accuracies, we need to fine-tune the model.\\n However, before we begin fine-tuning the model, we must define the loss function\\nwe will optimize during training. Our objecti ve is to maximize the spam classification\\naccuracy of the model, which means that the preceding code should output the cor-\\nrect class labels: 0 for non-spam and 1 for spam. \\n Because classification accuracy is not a differentiable function, we use cross-\\nentropy loss as a proxy to maximize accuracy. Accordingly, the calc_loss_batch func-\\ntion remains the same, with one adjustment : we focus on optimizing only the last\\ntoken, model(input_batch)[:, -1, :], rather than all tokens, model(input_batch):\\ndef calc_loss_batch(input_batch, target_batch, model, device):\\n    input_batch = input_batch.to(device)\\n    target_batch = target_batch.to(device)\\n    logits = model(input_batch)[:, -1, :]    \\nLogits of last \\noutput token\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 215, 'page_label': '194'}, page_content='194 CHAPTER 6 Fine-tuning for classification\\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\\n    return loss\\nWe use the calc_loss_batch function to compute the loss for a single batch obtained\\nfrom the previously defined data loaders. To calculate the loss for all batches in a data\\nloader, we define the calc_loss_loader function as before.\\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\\n    total_loss = 0.\\n    if len(data_loader) == 0:\\n        return float(\"nan\")\\n    elif num_batches is None:\\n        num_batches = len(data_loader)\\n    else:                                       \\n        num_batches = min(num_batches, len(data_loader))\\n    for i, (input_batch, target_batch) in enumerate(data_loader):\\n        if i < num_batches:\\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            total_loss += loss.item()\\n        else:\\n            break\\n    return total_loss / num_batches\\nSimilar to calculating the training accuracy, we now compute the initial loss for each\\ndata set:\\nwith torch.no_grad():                \\n    train_loss = calc_loss_loader(\\n        train_loader, model, device, num_batches=5\\n    )\\n    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\\n    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\\nprint(f\"Training loss: {train_loss:.3f}\")\\nprint(f\"Validation loss: {val_loss:.3f}\")\\nprint(f\"Test loss: {test_loss:.3f}\")\\nThe initial loss values are\\nTraining loss: 2.453\\nValidation loss: 2.583\\nTest loss: 2.322\\nNext, we will implement a training functi on to fine-tune the model, which means\\nadjusting the model to minimize the training set loss. Minimizing the training set loss\\nwill help increase the classification accuracy, which is our overall goal.\\nListing 6.9 Calculating the classification loss\\nEnsures number of \\nbatches doesn’t exceed \\nbatches in data loader\\nDisables gradient tracking \\nfor efficiency because we \\nare not training yet\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 216, 'page_label': '195'}, page_content='1956.7 Fine-tuning the model on supervised data\\n6.7 Fine-tuning the mode l on supervised data\\nWe must define and use the training func tion to fine-tune the pretrained LLM and\\nimprove its spam classification accuracy. Th e training loop, illust rated in figure 6.15,\\nis the same overall training loop we used for pretraining; the only difference is that\\nwe calculate the classification  accuracy instead of generating a sample text to evalu-\\nate the model.\\nThe training function implementing the concepts shown in figure 6.15 also closely mir-\\nrors the train_model_simple function used for pretraining the model. The only two dis-\\ntinctions are that we now track the number of training examples seen (examples_seen)\\ninstead of the number of tokens, and we calculate the accuracy after each epoch instead\\nof printing a sample text.\\n \\n \\n1) For each training epoch\\n2) For each batch in training set\\n3) Reset loss gradients from\\nthe previous batch iteration\\n4) Calculate loss on\\ncurrent batch\\n5) Backward pass to\\ncalculate loss gradients\\n6) Update model weights\\nusing loss gradients\\n7) Print training and\\nvalidation set losses\\n8) Generate sample text\\nfor visual inspection\\nOptional steps for tracking\\nthe training progress\\nThese are the usual steps\\nused for training deep\\nneural networks in PyTorch.\\nOne epoch is one complete\\npass over a training set.\\nThe number of batches is\\ndetermined by the training\\nset size divided by the size\\nof each batch.\\nFigure 6.15 A typical training loop for training deep neural networks in \\nPyTorch consists of several steps, iterating over the batches in the training \\nset for several epochs. In each loop, we calculate the loss for each training \\nset batch to determine loss gradients, which we use to update the model \\nweights to minimize the training set loss.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 217, 'page_label': '196'}, page_content='196 CHAPTER 6 Fine-tuning for classification\\ndef train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs, eval_freq, eval_iter):\\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \\n    examples_seen, global_step = 0, -1\\n    for epoch in range(num_epochs):   \\n        model.train()            \\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()                     \\n            loss = calc_loss_batch(\\n                input_batch, target_batch, model, device\\n            )\\n            loss.backward()                         \\n            optimizer.step()                         \\n            examples_seen += input_batch.shape[0]   \\n            global_step += 1\\n              \\n            if global_step % eval_freq == 0:\\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader, device, eval_iter)\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n                                                  \\n        train_accuracy = calc_accuracy_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_accuracy = calc_accuracy_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\\n        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\n        train_accs.append(train_accuracy)\\n        val_accs.append(val_accuracy)\\n    return train_losses, val_losses, train_accs, val_accs, examples_seen\\nThe evaluate_model function is identical to the one we used for pretraining:\\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\\n    model.eval()\\n    with torch.no_grad():\\nListing 6.10 Fine-tuning the model to classify spam\\nInitialize lists to\\ntrack losses and\\nexamples seen\\nMain training loop\\nSets model to training mode\\nResets loss gradients \\nfrom the previous \\nbatch iteration\\nCalculates loss \\ngradients\\nUpdates model \\nweights using \\nloss gradients\\nNew: tracks examples \\ninstead of tokens\\nOptional\\nevaluation\\nstep\\nCalculates accuracy \\nafter each epoch\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 218, 'page_label': '197'}, page_content='1976.7 Fine-tuning the model on supervised data\\n        train_loss = calc_loss_loader(\\n            train_loader, model, device, num_batches=eval_iter\\n        )\\n        val_loss = calc_loss_loader(\\n            val_loader, model, device, num_batches=eval_iter\\n        )\\n    model.train()\\n    return train_loss, val_loss\\nNext, we initialize the optimizer, set the nu mber of training epoc hs, and initiate the\\ntraining using the train_classifier_simple function. The training takes about 6\\nminutes on an M3 MacBook Air laptop comp uter and less than half a minute on a\\nV100 or A100 GPU:\\nimport time\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\\nnum_epochs = 5\\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\\\\n    train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs=num_epochs, eval_freq=50,\\n        eval_iter=5\\n    )\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe output we see during the training is as follows:\\nEp 1 (Step 000000): Train loss 2.153, Val loss 2.392\\nEp 1 (Step 000050): Train loss 0.617, Val loss 0.637\\nEp 1 (Step 000100): Train loss 0.523, Val loss 0.557\\nTraining accuracy: 70.00% | Validation accuracy: 72.50%\\nEp 2 (Step 000150): Train loss 0.561, Val loss 0.489\\nEp 2 (Step 000200): Train loss 0.419, Val loss 0.397\\nEp 2 (Step 000250): Train loss 0.409, Val loss 0.353\\nTraining accuracy: 82.50% | Validation accuracy: 85.00%\\nEp 3 (Step 000300): Train loss 0.333, Val loss 0.320\\nEp 3 (Step 000350): Train loss 0.340, Val loss 0.306\\nTraining accuracy: 90.00% | Validation accuracy: 90.00%\\nEp 4 (Step 000400): Train loss 0.136, Val loss 0.200\\nEp 4 (Step 000450): Train loss 0.153, Val loss 0.132\\nEp 4 (Step 000500): Train loss 0.222, Val loss 0.137\\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nEp 5 (Step 000550): Train loss 0.207, Val loss 0.143\\nEp 5 (Step 000600): Train loss 0.083, Val loss 0.074\\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nTraining completed in 5.65 minutes.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 219, 'page_label': '198'}, page_content='198 CHAPTER 6 Fine-tuning for classification\\nWe then use Matplotlib to plot the loss function for the training and validation set.\\nimport matplotlib.pyplot as plt\\ndef plot_values(\\n        epochs_seen, examples_seen, train_values, val_values,\\n        label=\"loss\"):\\n    fig, ax1 = plt.subplots(figsize=(5, 3))\\n                                     \\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\\n    ax1.plot(\\n        epochs_seen, val_values, linestyle=\"-.\",\\n        label=f\"Validation {label}\"\\n    )\\n    ax1.set_xlabel(\"Epochs\")\\n    ax1.set_ylabel(label.capitalize())\\n    ax1.legend()\\n                                  \\n    ax2 = ax1.twiny()\\n    ax2.plot(examples_seen, train_values, alpha=0)   \\n    ax2.set_xlabel(\"Examples seen\")\\n    fig.tight_layout()            \\n    plt.savefig(f\"{label}-plot.pdf\")\\n    plt.show()\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\\nFigure 6.16 plots the resulting loss curves.\\nListing 6.11 Plotting the classification loss\\nPlots training \\nand validation loss \\nagainst epochs\\nCreates a \\nsecond x-axis for \\nexamples seen\\nInvisible plot for \\naligning ticks\\nAdjusts layout \\nto make room\\nFigure 6.16 The model’s training and \\nvalidation loss over the five training \\nepochs. Both the training loss, \\nrepresented by the solid line, and the \\nvalidation loss, represented by the \\ndashed line, sharply decline in the first \\nepoch and gradually stabilize toward the \\nfifth epoch. This pattern indicates good \\nlearning progress and suggests that the \\nmodel learned from the training data \\nwhile generalizing well to the unseen \\nvalidation data.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 220, 'page_label': '199'}, page_content='1996.7 Fine-tuning the model on supervised data\\nAs we can see based on the sharp downward slope in figure 6.16, the model is learning\\nwell from the training data, an d there is little to no indication of overfitting; that is,\\nthere is no noticeable gap between the training and validation set losses.\\nUsing the same plot_values function, let’s now plot the classification accuracies:\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\\nplot_values(\\n    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\\n    label=\"accuracy\"\\n)\\nFigure 6.17 graphs the resulting accuracy. The model achieves a relatively high training\\nand validation accuracy after epochs 4 and 5. Importantly, we previously set eval_iter=5\\nChoosing the number of epochs \\nEarlier, when we initiated the training, we set the number of epochs to five. The num-\\nber of epochs depends on the dataset and the task’s difficulty, and there is no uni-\\nversal solution or recommendation, although an epoch number of five is usually a\\ngood starting point. If the model overfits after the first few epochs as a loss plot (see\\nfigure 6.16), you may need to reduce the number of epochs. Conversely, if the trend-\\nline suggests that the validation loss could improve with further training, you should\\nincrease the number of epochs. In this concrete case, five epochs is a reasonable\\nnumber as there are no signs of early overfitting, and the validation loss is close to 0.\\nFigure 6.17 Both the training accuracy (solid line) and the validation \\naccuracy (dashed line) increase substantially in the early epochs and \\nthen plateau, achieving almost perfect accuracy scores of 1.0. The \\nclose proximity of the two lines throughout the epochs suggests that \\nthe model does not overfit the training data very much.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 221, 'page_label': '200'}, page_content='200 CHAPTER 6 Fine-tuning for classification\\nwhen using the train_classifier_simple function, which means our estimations of\\ntraining and validation performance are ba sed on only five batches for efficiency\\nduring training. \\n Now we must calculate the performance metrics for the traini ng, validation, and\\ntest sets across the entire dataset by runni ng the following code, this time without\\ndefining the eval_iter value:\\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nThe resulting accuracy values are\\nTraining accuracy: 97.21%\\nValidation accuracy: 97.32%\\nTest accuracy: 95.67%\\nThe training and test set performances are almost identical. Th e slight discrepancy\\nbetween the training and test set accuracies  suggests minimal overfitting of the train-\\ning data. Typically, the validation set accura cy is somewhat higher than the test set\\naccuracy because the model development often involves tuning hyperparameters to\\nperform well on the validation set, which might not generalize as effectively to the test\\nset. This situation is common, but the gap could potentially be minimized by adjusting\\nthe model’s settings, such as increasing the dropout rate (\\ndrop_rate) or the weight_\\ndecay parameter in the optimizer configuration.\\n6.8 Using the LLM as a spam classifier\\nHaving fine-tuned and evaluated the model,  we are now ready to classify spam mes-\\nsages (see figure 6.18). Let’s use our fine-tuned GPT-based spam classification model.\\nThe following classify_review function follows data preprocessing steps similar\\nto those we used in the SpamDataset implemented earlier. Then, after processing\\ntext into token IDs, the fu nction uses the model to pr edict an integer class label,\\nsimilar to what we implemented in sectio n 6.6, and then returns the corresponding\\nclass name.\\n \\n \\n \\n \\n \\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 222, 'page_label': '201'}, page_content='2016.8 Using the LLM as a spam classifier\\ndef classify_review(\\n        text, model, tokenizer, device, max_length=None,\\n        pad_token_id=50256):\\n    model.eval()\\n    input_ids = tokenizer.encode(text)         \\n    supported_context_length = model.pos_emb.weight.shape[1]\\n    input_ids = input_ids[:min(             \\n        max_length, supported_context_length\\n    )]\\n    input_ids += [pad_token_id] * (max_length - len(input_ids))   \\n    \\n    input_tensor = torch.tensor(\\n        input_ids, device=device\\n    ).unsqueeze(0)             \\n    \\n    with torch.no_grad():                               \\n        logits = model(input_tensor)[:, -1, :]    \\n    predicted_label = torch.argmax(logits, dim=-1).item()\\n    return \"spam\" if predicted_label == 1 else \"not spam\"    \\nListing 6.12 Using the model to classify new texts\\nStage 1:\\nDataset preparation\\n1) Download\\nthe dataset\\n2) Preprocess\\ndataset\\n3) Create data\\nloaders\\n4) Initialize\\nmodel\\n5) Load pretrained\\nweights\\n6) Modify model\\nfor ﬁne-tuning\\n7) Implement\\nevaluation utilities\\n8) Fine-tune\\nmodel\\n9) Evaluate\\nﬁne-tuned model\\n10) Use model\\non new data\\n10) Use model\\non new data\\nStage 2:\\nModel setup\\nStage 3:\\nModel ﬁne-tuning\\nand usage\\nWe are ready to try the model\\non new text messages.\\nFigure 6.18 The three-stage process for classification fine-tuning our LLM. Step \\n10 is the final step of stage 3—using the fine-tuned model to classify new spam \\nmessages.\\nPrepares inputs \\nto the model\\nTruncates sequences if \\nthey are too long\\nPads sequences\\nto the longest\\nsequence\\nAdds batch \\ndimension\\nModels inference \\nwithout gradient \\ntracking\\nLogits of the last output token Returns the classified result\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 223, 'page_label': '202'}, page_content='202 CHAPTER 6 Fine-tuning for classification\\nLet’s try this classify_review function on an example text: \\ntext_1 = (\\n    \"You are a winner you have been specially\"\\n    \" selected to receive $1000 cash or a $2000 award.\"\\n)\\nprint(classify_review(\\n    text_1, model, tokenizer, device, max_length=train_dataset.max_length\\n))\\nThe resulting model correctly predicts \"spam\". Let’s try another example:\\ntext_2 = (\\n    \"Hey, just wanted to check if we\\'re still on\"\\n    \" for dinner tonight? Let me know!\"\\n)\\nprint(classify_review(\\n    text_2, model, tokenizer, device, max_length=train_dataset.max_length\\n))\\nThe model again makes a correct prediction and returns a “not spam” label.\\n Finally, let’s save the model in case we want to reuse the model later without having\\nto train it again. We can use the torch.save method:\\ntorch.save(model.state_dict(), \"review_classifier.pth\")\\nOnce saved, the model can be loaded:\\nmodel_state_dict = torch.load(\"review_classifier.pth, map_location=device\")\\nmodel.load_state_dict(model_state_dict)\\nSummary\\n\\uf0a1 There are different strategies for fine -tuning LLMs, including classification\\nfine-tuning and instruction fine-tuning.\\n\\uf0a1 Classification fine-tuning involves repl acing the output layer of an LLM via a\\nsmall classification layer.\\n\\uf0a1 In the case of classifying text messages as “spam” or “not spam,” the new classifi-\\ncation layer consists of only two output  nodes. Previously, we used the number\\no f  o u t p u t  n o d e s  e q u a l  t o  t h e  n u m b e r  o f  u n i q u e  t o k e n s  i n  t h e  v o c a b u l a r y\\n(i.e., 50,256).\\n\\uf0a1 Instead of predicting the next token in th e text as in pretraining, classification\\nfine-tuning trains the model to outp ut a correct class label—for example,\\n“spam” or “not spam.”\\n\\uf0a1 The model input for fine-tuning is text  converted into token IDs, similar to\\npretraining.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 224, 'page_label': '203'}, page_content='203Summary\\n\\uf0a1 Before fine-tuning an LLM, we load the pretrained model as a base model.\\n\\uf0a1 Evaluating a classification model involves  calculating the classification accuracy\\n(the fraction or percentage of correct predictions).\\n\\uf0a1 Fine-tuning a classification model uses the same cross entr opy loss function as\\nwhen pretraining the LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 225, 'page_label': '204'}, page_content='204\\nFine-tuning to follow\\ninstructions\\nPreviously, we implemented the LLM arch itecture, carried out pretraining, and\\nimported pretrained weights from external sources into our model. Then, we\\nfocused on fine-tuning our LLM for a spec ific classification task: distinguishing\\nbetween spam and non-spam text messages. Now we’ll implement the process for\\nfine-tuning an LLM to follow human inst ructions, as illustrated in figure 7.1.\\nInstruction fine-tuning is one of the main  techniques behind developing LLMs for\\nchatbot applications, personal assistants, and other conversational tasks.\\nThis chapter covers\\n\\uf0a1 The instruction fine-tuning process of LLMs\\n\\uf0a1 Preparing a dataset for supervised instruction \\nfine-tuning\\n\\uf0a1 Organizing instruction data in training batches\\n\\uf0a1 Loading a pretrained LLM and fine-tuning it to \\nfollow human instructions\\n\\uf0a1 Extracting LLM-generated instruction responses \\nfor evaluation\\n\\uf0a1 Evaluating an instruction-fine-tuned LLM\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 226, 'page_label': '205'}, page_content='2057.1 Introduction to instruction fine-tuning\\nFigure 7.1 shows two main wa ys of fine-tuning an LLM: fine-tuning for classification\\n(step 8) and fine-tuning an LLM to follow instructions (step 9). We implemented step\\n8 in chapter 6. Now we will fine-tune an LLM using an instruction dataset.\\n7.1 Introduction to instruction fine-tuning\\nWe now know that pretraining an LLM invo lves a training procedure where it learns\\nto generate one word at a time. The resulting pretrained LLM is capable of text comple-\\ntion, meaning it can finish sentences or wr ite text paragraphs given a fragment as\\ninput. However, pretrained LLMs often struggle with specific instructions, such as “Fix\\nthe grammar in this text” or “Convert this text into passive voice.” Later, we will exam-\\nine a concrete example where we load the pretrained LLM as the basis for instruction\\nfine-tuning, also known as supervised instruction fine-tuning.\\n Here, we focus on improving the LLM’s ability to follow such instructions and gen-\\nerate a desired response, as illustrated in figure 7.2. Preparing the dataset is a key\\naspect of instruction fine-tuning. Then we’ll complete all the steps in the three stages\\nof the instruction fine-tunin g process, beginning with the dataset preparation, as\\nshown in figure 7.3.\\nFigure 7.1 The three main stages of coding an LLM. This chapter focuses on step 9 of stage 3: fine-tuning a \\npretrained LLM to follow human instructions.\\nIn the previous chapter,\\nwe ﬁne-tuned the pretrained\\nLLM to classify texts.\\nIn chapter 5, we\\npretrained an LLM.\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture.\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture.\\n1) Data\\npreparation\\nand sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n9) Fine-tuning\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nIn this chapter, we\\nﬁne-tune the pretrained\\nLLM to follow instructions.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 227, 'page_label': '206'}, page_content='206 CHAPTER 7 Fine-tuning to follow instructions\\nFigure 7.2 Examples of instructions that are processed by an LLM to \\ngenerate desired responses\\nConvert 45 kilometers to meters. 45 kilometers is 45000 meters.\\nEdit the following sentence to\\nremove all passive voice: “The\\nsong was composed by the artist.”\\nThe artist composed the song.\\nInstruction Desired response\\nThe instructions serve\\nas inputs for the LLM.\\nThe goal for the\\nLLM is to generate\\na desired response.Provide a synonym for “bright.” A synonym for “bright” is “radiant.”\\nFigure 7.3 The three-stage process for instruction fine-tuning an LLM. Stage 1 involves \\ndataset preparation, stage 2 focuses on model setup and fine-tuning, and stage 3 covers \\nthe evaluation of the model. We will begin with step 1 of stage 1: downloading and \\nformatting the dataset.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nWe start with downloading,\\ninspecting, and preparing\\nthe dataset that we will use\\nto ﬁne-tune the model.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 228, 'page_label': '207'}, page_content='2077.2 Preparing a dataset for supervised instruction fine-tuning\\n7.2 Preparing a dataset fo r supervised instruction \\nfine-tuning\\nLet’s download and format th e instruction dataset for in struction fine-tuning a pre-\\ntrained LLM. The dataset consists of 1,100 instruction–response pairs similar to those in\\nfigure 7.2. This dataset was created specifically for this book, but interested readers\\ncan find alternative, publicly available instruction datasets in appendix B.\\n The following code implements and execut es a function to download this dataset,\\nwhich is a relatively small file (only 204 KB) in JSON format. JSON, or JavaScript Object\\nNotation, mirrors the structur e of Python dictionaries, providing a simple structure\\nfor data interchange that is both human readable and machine friendly.\\nimport json\\nimport os\\nimport urllib\\ndef download_and_load_file(file_path, url):\\n    if not os.path.exists(file_path):\\n        with urllib.request.urlopen(url) as response:\\n            text_data = response.read().decode(\"utf-8\")\\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n            file.write(text_data)\\n    else:                                               \\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n            text_data = file.read()\\n    with open(file_path, \"r\") as file:\\n        data = json.load(file)\\n    return data\\nfile_path = \"instruction-data.json\"\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\\n    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\\n)\\ndata = download_and_load_file(file_path, url)\\nprint(\"Number of entries:\", len(data))\\nThe output of executing the preceding code is\\nNumber of entries: 1100\\nThe data list that we loaded from the JSON fi le contains the 1,100 entries of the\\ninstruction dataset. Let’s print one of the entries to see how each entry is structured:\\nprint(\"Example entry:\\\\n\", data[50])\\nListing 7.1 Downloading the dataset\\nSkips download if \\nfile was already \\ndownloaded\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 229, 'page_label': '208'}, page_content='208 CHAPTER 7 Fine-tuning to follow instructions\\nThe content of the example entry is \\nExample entry:\\n {\\'instruction\\': \\'Identify the correct spelling of the following word.\\',\\n  \\'input\\': \\'Ocassion\\', \\'output\\': \"The correct spelling is \\'Occasion.\\'\"}\\nAs we can see, the example entries are Python dictionary objects containing an\\n\\'instruction\\', \\'input\\', and \\'output\\'. Let’s take a look at another example:\\nprint(\"Another example entry:\\\\n\", data[999])\\nBased on the contents of this entry, the \\'input\\' field may occasionally be empty:\\nAnother example entry:\\n {\\'instruction\\': \"What is an antonym of \\'complicated\\'?\", \\n  \\'input\\': \\'\\',\\n  \\'output\\': \"An antonym of \\'complicated\\' is \\'simple\\'.\"}\\nInstruction fine-tuning involves training a model on a dataset where the input-output\\npairs, like those we extracted from the JS ON file, are explicitly provided. There are\\nvarious methods to format these entries for LLMs. Figure 7.4 illu strates two different\\nFigure 7.4 Comparison of prompt styles for instruct ion fine-tuning in LLMs. The Alpaca style (left) uses a \\nstructured format with defined sections for instruction, input, and response, while the Phi-3 style (right) employs \\na simpler format with designated \\n<|user|> and <|assistant|> tokens.\\n{\\n\"instruction\": \"Identify the correct spelling of the following word.\",\\n\"input\": \"Ocassion\",\\n\"output\": \"The correct spelling is \\'Occasion.\\'\"\\n},\\nApply Alpaca prompt style template. Apply Phi-3 prompt style template.\\nAn entry in the\\ninstruction dataset\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nIdentify the correct spelling of the\\nfollowing word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion\\'.\\n<|user|>\\nIdentify the correct spelling of the\\nfollowing word: \\'Ocassion\\'\\n<|assistant|>\\nThe correct spelling is \\'Occasion\\'.\\nOne way to format\\nthe data entry to\\ntrain the LLM\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 230, 'page_label': '209'}, page_content='2097.2 Preparing a dataset for supervised instruction fine-tuning\\nexample formats, often referred to as prompt styles , used in the training of notable\\nLLMs such as Alpaca and Phi-3. \\n Alpaca was one of the early LLMs to public ly detail its instruction fine-tuning pro-\\ncess. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt\\nstyles. The rest of this chapter uses the Alpa ca prompt style since it is one of the most\\npopular ones, largely because it helped define the original approach to fine-tuning.\\nLet’s define a format_input function that we can use to convert the entries in the\\ndata list into the Alpaca-style input format. \\ndef format_input(entry):\\n    instruction_text = (\\n        f\"Below is an instruction that describes a task. \"\\n        f\"Write a response that appropriately completes the request.\"\\n        f\"\\\\n\\\\n### Instruction:\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    \\n    input_text = (\\n        f\"\\\\n\\\\n### Input:\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    )\\n    return instruction_text + input_text\\nThis format_input function takes a dictionary entry as input and constructs a format-\\nted string. Let’s test it to dataset entry data[50], which we looked at earlier:\\nmodel_input = format_input(data[50])\\ndesired_response = f\"\\\\n\\\\n### Response:\\\\n{data[50][\\'output\\']}\"\\nprint(model_input + desired_response)\\nThe formatted input looks like as follows:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nIdentify the correct spelling of the following word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion.\\'\\nExercise 7.1 Changing prompt styles \\nAfter fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style\\nshown in figure 7.4 and observe whether it affects the response quality of the model.\\nListing 7.2 Implementing the prompt formatting function\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 231, 'page_label': '210'}, page_content='210 CHAPTER 7 Fine-tuning to follow instructions\\nNote that the format_input skips the optional ### Input: section if the \\'input\\' field\\nis empty, which we can test out by applying the format_input function to entry\\ndata[999] that we inspected earlier:\\nmodel_input = format_input(data[999])\\ndesired_response = f\"\\\\n\\\\n### Response:\\\\n{data[999][\\'output\\']}\"\\nprint(model_input + desired_response)\\nThe output shows that entries with an empty \\'input\\' field don’t contain an ###\\nInput: section in the formatted input:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nWhat is an antonym of \\'complicated\\'?\\n### Response:\\nAn antonym of \\'complicated\\' is \\'simple\\'.\\nBefore we move on to setting up the PyTorch data loaders in the next section, let’s\\ndivide the dataset into traini ng, validation, and test sets analogous to what we have\\ndone with the spam classification dataset in the previous chapter. The following listing\\nshows how we calculate the portions.\\ntrain_portion = int(len(data) * 0.85)   \\ntest_portion = int(len(data) * 0.1)           \\nval_portion = len(data) - train_portion - test_portion   \\ntrain_data = data[:train_portion]\\ntest_data = data[train_portion:train_portion + test_portion]\\nval_data = data[train_portion + test_portion:]\\nprint(\"Training set length:\", len(train_data))\\nprint(\"Validation set length:\", len(val_data))\\nprint(\"Test set length:\", len(test_data))\\nThis partitioning results in the following dataset sizes:\\nTraining set length: 935\\nValidation set length: 55\\nTest set length: 110\\nListing 7.3 Partitioning the dataset\\nUse 85% of the data for training\\nUse 10% for \\ntesting\\nUse remaining \\n5% for validation\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 232, 'page_label': '211'}, page_content='2117.3 Organizing data into training batches\\nHaving successfully downloaded and partitioned the dataset and gained a clear under-\\nstanding of the dataset prompt formatting, we are now ready for the core implementa-\\ntion of the instruction fine-tuning process.  Next, we focus on developing the method\\nfor constructing the training batches for fine-tuning the LLM.\\n7.3 Organizing data into training batches\\nAs we progress into the implementation ph ase of our instruction fine-tuning process,\\nthe next step, illustrated in figure 7.5, fo cuses on constructing the training batches\\neffectively. This involves defining a method  that will ensure our model receives the\\nformatted training data during the fine-tuning process.\\nIn the previous chapter, the training batches were created automatically by the PyTorch\\nDataLoader class, which employs a default collate function to combine lists of samples\\ninto batches. A collate function is responsi ble for taking a list of individual data sam-\\nples and merging them into a single batch that can be processed efficiently by the\\nmodel during training. \\n However, the batching process for instruction fine-tuning is a bit more involved\\nand requires us to create our own custom collate function that we will later plug into\\nFigure 7.5 The three-stage process for instruction fine-tuning an LLM. Next, we look at step 2 of stage \\n1: assembling the training batches.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nIn this section, we learn how to efﬁciently\\npad the data samples to equal lengths\\nso we can assemble multiple instruction\\nexamples in a batch.\\nThen, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 233, 'page_label': '212'}, page_content='212 CHAPTER 7 Fine-tuning to follow instructions\\nthe DataLoader. We implement this custom collate function to handle the specific\\nrequirements and formatting of our instruction fine-tuning dataset. \\n Let’s tackle the batching process  in several steps, including coding the custom col-\\nlate function, as illustrated in figure 7.6. First, to im plement steps 2.1 and 2.2, we\\ncode an InstructionDataset class that applies format_input and pretokenizes  all\\ninputs in the dataset, similar to the SpamDataset in chapter 6. This two-step process,\\ndetailed in figure 7.7, is implemented in the __init__ constructor method of the\\nInstructionDataset.\\n \\n \\nFigure 7.6 The five substeps involved in implementing the batching process: (2.1) applying the \\nprompt template, (2.2) using tokenization from previous chapters, (2.3) adding padding tokens, \\n(2.4) creating target token IDs, and (2.5) replacing -100 placeholder tokens to mask padding \\ntokens in the loss function.\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3) hAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 234, 'page_label': '213'}, page_content='2137.3 Organizing data into training batches\\nimport torch\\nfrom torch.utils.data import Dataset\\nclass InstructionDataset(Dataset):\\n    def __init__(self, data, tokenizer):\\n        self.data = data\\n        self.encoded_texts = []\\n        for entry in data:        \\n            instruction_plus_input = format_input(entry)\\n            response_text = f\"\\\\n\\\\n### Response:\\\\n{entry[\\'output\\']}\"\\n            full_text = instruction_plus_input + response_text\\n            self.encoded_texts.append(\\n                tokenizer.encode(full_text)\\n            )\\n    def __getitem__(self, index):\\n        return self.encoded_texts[index]\\n    def __len__(self):\\n        return len(self.data)\\nListing 7.4 Implementing an instruction dataset class\\nFigure 7.7 The first two steps involved in implementing the batching process. Entries are first formatted using \\na specific prompt template (2.1) and then tokenized (2.2), resulting in a sequence of token IDs that the model \\ncan process.\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nConvert 45 kilometers to meters.\\n### Response:\\n45 kilometers is 45000 meters.\\nBelow is an instruction that\\ndescribes a task. Write a response\\nthat appropriately completes the\\nrequest.\\n### Instruction:\\nIdentify the correct spelling of the\\nfollowing word.\\n### Input:\\nOcassion\\n### Response:\\nThe correct spelling is \\'Occasion\\'.\\n{\\n\"instruction\": \"Identify the correct\\nspelling of the following word.\",\\n\"input\": \"Ocassion\",\\n\"output\": \"The correct\\nspelling is \\'Occasion.\\'\"\\n}\\n{\\n\"instruction\": \"Convert 45 kilometers\\nto meters.\",\\n\"input\":           ,\"Ocassion\"\\n\"output\": \"45 kilometers is 45000\\nmeters.\\'\"\\n}\\n[21106, 318, 281, 12064,\\n326, 8477, 257, 4876, 13,\\n19430, ..., 29223, 4247,\\n4458]\\n[21106, 318, 281, 12064,\\n326, 8477, 257, 4876, 13,\\n19430, ... , 830, 10700,\\n13]\\n2.1) Format\\ndataset entry.\\n2.2) Tokenize\\nformatted entry.\\nThe input entry is formatted\\nusing the prompt template.\\nThe token IDs that the\\nLLM will receive as input.\\nPretokenizes \\ntexts\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 235, 'page_label': '214'}, page_content='214 CHAPTER 7 Fine-tuning to follow instructions\\nSimilar to the approach used for classification fine-tuning, we want to accelerate train-\\ning by collecting multiple training examples in a batch, which necessitates padding all\\ninputs to a similar length. As with classification fine-tuning, we use the <|endoftext|>\\ntoken as a padding token. \\n Instead of appending the <|endoftext|> tokens to the text inputs, we can append\\nthe token ID corresponding to <|endoftext|> to the pretokenized inputs directly. We\\ncan use the tokenizer’s .encode method on an <|endoftext|> token to remind us\\nwhich token ID we should use:\\nimport tiktoken\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\\nThe resulting token ID is 50256. \\n Moving on to step 2.3 of the process (s ee figure 7.6), we adopt a more sophisti-\\ncated approach by developing a custom coll ate function that we can pass to the data\\nloader. This custom collate function pads th e training examples in each batch to the\\nsame length while allowing different batches to have di fferent lengths, as demon-\\nstrated in figure 7.8. This approach minimizes unnecessary padding by only extending\\nsequences to match the longest one in each batch, not the whole dataset.\\nFigure 7.8 The padding of training examples in batches using token ID 50256 to ensure uniform length within \\neach batch. Each batch may have different lengths, as shown by the first and second.\\n[ 0,     1,     2,     3,     4]\\n[ 5,     6]\\n[7,      8,     9]\\n[    0,     1,     2,     3,     4]\\n[    5,     6, 50256, 50256, 50256]\\n[    7,     8,     9, 50256, 50256]\\nInput 1\\nInput 2\\nInput 3\\n[ 8,     1]\\n[10,     3,     11,     6]\\n[ 5,     22,    13,    13]\\nInput 4\\nInput 5\\nInput 6\\n[    8,     1, 50256, 50256]\\n[   10,     3,    11,     6]\\n[    5,    22,    13,    13]\\nThe ﬁrst\\nbatch\\nThe\\nsecond\\nbatch\\nToken ID 50256 is used\\nas the padding token.\\nToken IDs corresponding to\\nthe ﬁrst training example\\nPad all training examples in a batch\\nso that they have the same length.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 236, 'page_label': '215'}, page_content='2157.3 Organizing data into training batches\\nWe can implement the padding process with a custom collate function:\\ndef custom_collate_draft_1(\\n    batch,\\n    pad_token_id=50256,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)  \\n    inputs_lst = []\\n    for item in batch:    \\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        padded = (\\n            new_item + [pad_token_id] * \\n            (batch_max_length - len(new_item))\\n        )\\n        inputs = torch.tensor(padded[:-1])   \\n        inputs_lst.append(inputs)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)    \\n    return inputs_tensor\\nThe custom_collate_draft_1 we implemented is designed to be integrated into a\\nPyTorch DataLoader, but it can also function as a standalone tool. Here, we use it\\nindependently to test and verify that it oper ates as intended. Let’s try it on three dif-\\nferent inputs that we want to assemble in to a batch, where each example gets padded\\nto the same length:\\ninputs_1 = [0, 1, 2, 3, 4]\\ninputs_2 = [5, 6]\\ninputs_3 = [7, 8, 9]\\nbatch = (\\n    inputs_1,\\n    inputs_2,\\n    inputs_3\\n)\\nprint(custom_collate_draft_1(batch))\\nThe resulting batch looks like the following:\\ntensor([[    0,     1,     2,     3,     4],  \\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\nThis output shows all inputs have been padd ed to the length of the longest input list,\\ninputs_1, containing five token IDs.\\nFinds the longest \\nsequence in the \\nbatch\\nPads and \\nprepares inputs\\nRemoves extra \\npadded token \\nadded earlier\\nConverts the list of \\ninputs to a tensor \\nand transfers it to \\nthe target device\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 237, 'page_label': '216'}, page_content='216 CHAPTER 7 Fine-tuning to follow instructions\\n We have just implemented our first custom collate function to create batches from\\nlists of inputs. However, as we previously learned, we also need to create batches with\\nthe target token IDs corresponding to the batch of input IDs. These target IDs, as\\nshown in figure 7.9, are crucial because they represent what we want the model to\\ngenerate and what we need  during training to calculate the loss for the weight\\nupdates. That is, we modify our custom collate function to return the target token IDs\\nin addition to the input token IDs.\\nSimilar to the process we used to pretrain  an LLM, the target token IDs match the\\ninput token IDs but are shifted one position to  the right. This setup, as shown in fig-\\nure 7.10, allows the LLM to learn how to predict the next token in a sequence.\\n \\n \\nFigure 7.9 The five substeps involved in implementing the batching process. We are now focusing on \\nstep 2.4, the creation of target token IDs. This step is essential as it enables the model to learn and \\npredict the tokens it needs to generate.\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3) hAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 238, 'page_label': '217'}, page_content='2177.3 Organizing data into training batches\\nThe following updated collate function generates the target token IDs from the input\\ntoken IDs:\\ndef custom_collate_draft_2(\\n    batch,\\n    pad_token_id=50256,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)\\n    inputs_lst, targets_lst = [], []\\n    for item in batch:\\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\nFigure 7.10 The input and target token alignment used in the instruction \\nfine-tuning process of an LLM. For each input sequence, the corresponding \\ntarget sequence is created by shifting the token IDs one position to the right, \\nomitting the first token of the input, and appending an end-of-text token. \\n[    0,     1,     2,     3,     4       ]Input 1\\n[    1,     2,     3,     4,    50256    ]Target 1\\nThe token IDs in the target\\nare similar to the input IDs\\nbut shifted by 1 position.\\nThe target vector does not\\ncontain the ﬁrst input ID.\\nWe add an end-of-text\\n(padding) token.\\n[    5,     6, 50256, 50256, 50256    ]Input 2\\n[    6, 50256, 50256, 50256, 50256    ]Target 2\\nWe always add an end-of-text\\n(padding) token to the target.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 239, 'page_label': '218'}, page_content='218 CHAPTER 7 Fine-tuning to follow instructions\\n        padded = (\\n            new_item + [pad_token_id] * \\n            (batch_max_length - len(new_item))\\n        )\\n        inputs = torch.tensor(padded[:-1])    \\n        targets = torch.tensor(padded[1:])   \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\ninputs, targets = custom_collate_draft_2(batch)\\nprint(inputs)\\nprint(targets)\\nApplied to the example batch consisting of three input lists we defined earlier, the\\nnew custom_collate_draft_2 function now returns the input and the target batch:\\ntensor([[    0,     1,     2,     3,     4],   \\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\ntensor([[    1,     2,     3,     4, 50256],  \\n        [    6, 50256, 50256, 50256, 50256],\\n        [    8,     9, 50256, 50256, 50256]])\\nIn the next step, we assign a -100 placeholder value to all padding tokens, as high-\\nlighted in figure 7.11. This special value allows us to exclude these padding tokens\\nfrom contributing to the training loss calculation, ensuring that only meaningful data\\ninfluences model learning. We will discuss th is process in more detail after we imple-\\nment this modification. (When fine-tuning for classification, we did not have to worry\\nabout this since we only trained the model based on the last output token.)\\n However, note that we retain one end-of-text token, ID 50256, in the target list, as\\ndepicted in figure 7.12. Retaining it allows the LLM to learn when to generate an end-\\nof-text token in response to instructions, which we use as an indicator that the gener-\\nated response is complete.\\n In the following listing, we modify our custom collate function to replace tokens\\nwith ID 50256 with -100 in the target lists. Addi tionally, we introduce an allowed_\\nmax_length parameter to optionally limit the length of the samples. This adjustment\\nwill be useful if you plan to work with your own datasets that exceed the 1,024-token\\ncontext size supported by the GPT-2 model. \\n \\n \\n \\n \\n \\nTruncates the \\nlast token for \\ninputs\\nShifts +1 to the \\nright for targets\\nThe first tensor \\nrepresents inputs.\\nThe second tensor \\nrepresents the targets.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 240, 'page_label': '219'}, page_content='2197.3 Organizing data into training batches\\nFigure 7.11 The five substeps involved in implementing the batching process. After creating the \\ntarget sequence by shifting token IDs one position to the right and appending an end-of-text token, in \\nstep 2.5, we replace the end-of-text padding tokens with a placeholder value (-100).\\n2.1) Format data\\nusing prompt\\ntemplate.\\n2.2) Tokenize\\nformatted data.\\n2.5) Replace\\npadding tokens\\nwith placeholders.\\n2.3) hAdjust to t e\\nsame length with\\npadding tokens.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,]\\n2.4) Create target\\ntoken IDs for\\ntraining.\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, 50256, 50256, 50256]\\n[21106, 318, 281, 12064, 326,\\n8477, 257, 4876, 13, ...,\\n50256, 50256, 50256]\\n[318, 281, 12064, 326, 8477,\\n257, 4876, 13, ...,\\n50256, -100, -100, -100]\\nFormat input into an instruction-\\nresponse template.\\nConvert instruction-response\\nentry into token IDs.\\nAdd end-of-text tokens (50256)\\nto pad data samples to the same\\nlength.\\nCreate a list of target token IDs\\nfor the model to learn (these are\\nthe inputs shifted by 1, plus an\\nadditional padding token).\\nReplace certain padding tokens\\nby -100 to exclude them from\\nthe training loss.\\n### Instruction: …\\n### Response: …\\nFigure 7.12 Step 2.4 in the token replacement process in t he target batch for the training data preparation. We \\nreplace all but the first instance of the end-of-text token, which we use as padding, with the placeholder value \\n-100, while keeping the initial end-of-text token in each target sequence.\\n[    1,     2,     3,     4, 50256    ]Target 1\\n[    6, 50256, 50256, 50256, 50256    ]Target 2\\n[    8,     9, 50256, 50256, 50256    ]Target 3\\n[    1,     2,     3,     4, 50256     ]\\n[    6, 50256,  -100,  -100,   -100    ]\\n[    8,     9, 50256,  -100,   -100    ]\\nWe don’t modify the ﬁrst\\ninstance of the end-of-text\\n(padding) token.\\nWe replace all but the ﬁrst\\ninstance of the end-of-text\\n(padding) token with -100.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 241, 'page_label': '220'}, page_content='220 CHAPTER 7 Fine-tuning to follow instructions\\ndef custom_collate_fn(\\n    batch,\\n    pad_token_id=50256,\\n    ignore_index=-100,\\n    allowed_max_length=None,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for item in batch)\\n    inputs_lst, targets_lst = [], []\\n    for item in batch:\\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        \\n        padded = (                              \\n            new_item + [pad_token_id] *         \\n            (batch_max_length - len(new_item))  \\n        )\\n        inputs = torch.tensor(padded[:-1])     \\n        targets = torch.tensor(padded[1:])    \\n        mask = targets == pad_token_id             \\n        indices = torch.nonzero(mask).squeeze()    \\n        if indices.numel() > 1:                    \\n            targets[indices[1:]] = ignore_index    \\n        if allowed_max_length is not None:\\n            inputs = inputs[:allowed_max_length]      \\n            targets = targets[:allowed_max_length]    \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\nAgain, let’s try the collate function on th e sample batch that we created earlier to\\ncheck that it works as intended:\\ninputs, targets = custom_collate_fn(batch)\\nprint(inputs)\\nprint(targets)\\nThe results are as follows, where the first te nsor represents the inputs and the second\\ntensor represents the targets:\\ntensor([[    0,     1,     2,     3,     4],\\n        [    5,     6, 50256, 50256, 50256],\\n        [    7,     8,     9, 50256, 50256]])\\nListing 7.5 Implementing a custom batch collate function\\nPads sequences \\nto max_length\\nTruncates the last token for inputs\\nShifts +1 to the right for targets\\nReplaces all but the first \\npadding tokens in targets \\nby ignore_index\\nOptionally truncates to the \\nmaximum sequence length\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 242, 'page_label': '221'}, page_content='2217.3 Organizing data into training batches\\ntensor([[    1,     2,     3,     4, 50256],\\n        [    6, 50256,  -100,  -100,  -100],\\n        [    8,     9, 50256,  -100,  -100]])\\nThe modified collate function works as expect ed, altering the target  list by inserting\\nthe token ID -100. What is the logic behind this adjustment? Let’s explore the under-\\nlying purpose of this modification.\\n For demonstration purposes, consider the following simple and self-contained\\nexample where each output logit correspond s to a potential token from the model’s\\nvocabulary. Here’s how we might calculate the cross entropy loss (introduced in chap-\\nter 5) during training when the model predic ts a sequence of tokens, which is similar\\nto what we did when we pretrained the model and fine-tuned it for classification:\\nlogits_1 = torch.tensor(\\n    [[-1.0, 1.0],    \\n     [-0.5, 1.5]]     \\n)\\ntargets_1 = torch.tensor([0, 1]) # Correct token indices to generate\\nloss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\\nprint(loss_1)\\nThe loss value calculated by the previous code is 1.1269:\\ntensor(1.1269)\\nAs we would expect, adding an additional token ID affects the loss calculation:\\nlogits_2 = torch.tensor(\\n    [[-1.0, 1.0],\\n     [-0.5, 1.5],\\n     [-0.5, 1.5]]     \\n)\\ntargets_2 = torch.tensor([0, 1, 1])\\nloss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\\nprint(loss_2)\\nAfter adding the third token, the loss value is 0.7936. \\n So far, we have carried out some more or less obvious example calculations using\\nthe cross entropy loss function  in PyTorch, the same loss function we used in the\\ntraining functions for pretraining and fine -tuning for classification. Now let’s get to\\nthe interesting part and see what happens if we replace the third target token ID\\nwith -100:\\ntargets_3 = torch.tensor([0, 1, -100])\\nloss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\\nprint(loss_3)\\nprint(\"loss_1 == loss_3:\", loss_1 == loss_3)\\npredictions for 1st token \\npredictions for 2nd token\\nNew third token \\nID prediction\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 243, 'page_label': '222'}, page_content='222 CHAPTER 7 Fine-tuning to follow instructions\\nThe resulting output is \\ntensor(1.1269)\\nloss_1 == loss_3: tensor(True)\\nThe resulting loss on these three training ex amples is identical to the loss we calcu-\\nlated from the two training examples earlie r. In other words, the cross entropy loss\\nfunction ignored the third entry in the targets_3 vector, the token ID corresponding\\nto -100. (Interested readers can try to replace the -100 value with another token ID\\nthat is not 0 or 1; it will result in an error.)\\n So  wha t ’ s  s o  s pe cia l  a bo ut  -100 that it’s ignored by the cross entropy loss? The\\ndefault setting of the cross en tropy function in PyTorch is cross_entropy(...,\\nignore_index=-100). This means that it ignores targets labeled with -100. We take\\nadvantage of this ignore_index to ignore the additional end-of-text (padding) tokens\\nthat we used to pad the training examples  t o  h a v e  t h e  s a m e  l e n g t h  i n  e a c h  b a t c h .\\nHowever, we want to keep one 50256 (end-of-text) token ID in the targets because it\\nhelps the LLM to learn to generate end-of-t ext tokens, which we can use as an indica-\\ntor that a response is complete.\\n In addition to masking out padding tokens , it is also common to mask out the tar-\\nget token IDs that correspond to the instruction, as illustrated in figure 7.13. By mask-\\ning out the LLM’s target token IDs corres ponding to the instruction, the cross\\nentropy loss is only computed for the generated response target IDs. Thus, the model\\nis trained to focus on generating accurate  responses rather than memorizing instruc-\\ntions, which can help reduce overfitting.\\nFigure 7.13 Left: The formatted input text we tokenize  and then feed to the LLM during training. Right: The \\ntarget text we prepare for the LLM where we can optionally mask out the instruction section, which means \\nreplacing the corresponding token IDs with the -100 ignore_index value.\\nBelow is an instruction that describes a task. Write a\\nresponse that appropriately completes the request.\\n### Instruction:\\nRewrite the following sentence using passive voice.\\n### Input:\\nThe team achieved great results.\\n### Response:\\nGreat results were achieved by the team.\\nis an instruction that describes a task. Write a response\\nthat appropriately completes the request.\\n### Instruction:\\nRewrite the following sentence using passive voice.\\n### Input:\\nThe team achieved great results.\\n### Response:\\nGreat results were achieved by the team.<|endoftext|>\\nInput text: Target text:\\n[21106, 318, 281, 12064, 326, ..., 13][21106, 318, 281, 12064, 326, ..., 13] [-100, -100, -100, -100, -100, ..., 13, 50256][-100, -100, -100, -100, -100, ..., 13, 50256]\\nMask out the instruction\\nwhen calculating the loss.\\nThe instruction tokens\\nare replaced by -100.\\nThe token IDs corresponding\\nto the input text\\nTokenize Tokenize\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 244, 'page_label': '223'}, page_content='2237.4 Creating data loaders for an instruction dataset\\nAs of this writing, researchers are divided on whether masking the instructions is uni-\\nversally beneficial during instruction fine-t uning. For instance, the 2024 paper by Shi\\net al., “Instruction Tuning With Loss Over Instructions” ( https:/ /arxiv.org/abs/\\n2405.14394), demonstrated that not masking the instructions benefits the LLM per-\\nformance (see appendix B for more detail s). Here, we will not apply masking and\\nleave it as an optional exercise for interested readers.\\n7.4 Creating data loaders fo r an instruction dataset\\nWe have completed several stages to implement an InstructionDataset class and a\\ncustom_collate_fn function for the instruction dataset. As shown in figure 7.14, we\\nare ready to reap the fruits of our labor by simply plugging both InstructionDataset\\nobjects and the custom_collate_fn function into PyTorch data loaders. These loaders\\nExercise 7.2 Instruction and input masking\\nAfter completing the chapter and fine-tuning the model with InstructionDataset,\\nreplace the instruction and input tokens with the -100 mask to use the instruction\\nmasking method illustrated in figure 7.13. Then evaluate whether this has a positive\\neffect on model performance.\\nFigure 7.14 The three-stage process for instruction fine-tuning an LLM. Thus far, we have prepared the \\ndataset and implemented a custom collate function to batch the instruction dataset. Now, we can \\ncreate and apply the data loaders to the training, validation, and test sets needed for the LLM \\ninstruction fine-tuning and evaluation.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nIn the previous section, we\\nassembled multiple instruction\\nexamples in a batch.\\nNow, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 245, 'page_label': '224'}, page_content='224 CHAPTER 7 Fine-tuning to follow instructions\\nwill automatically shuffle and organize the batches for the LLM instruction fine-tun-\\ning process.\\n Before we implement the data loader crea tion step, we have to  briefly talk about\\nthe device setting of the custom_collate_fn. The custom_collate_fn includes code\\nto move the input and target tensors (for example, torch.stack(inputs_lst).to\\n(device)) to a specified device, which can be either \"cpu\" or \"cuda\" (for NVIDIA\\nGPUs) or, optionally, \"mps\" for Macs with Apple Silicon chips. \\nNOTE Using an \"mps\" device may result in nu merical differences compared\\nto the contents of this chapter, as Ap ple Silicon support in PyTorch is still\\nexperimental.\\nPreviously, we moved the data onto the target device (for example, the GPU memory\\nwhen device=\"cuda\") in the main training loop. Having this as part of the collate\\nfunction offers the advantage of performing  this device transfer process as a back-\\nground process outside the training loop, preventing it from blocking the GPU\\nduring model training.\\n The following code initializes the device variable:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n# if torch.backends.mps.is_available():  \\n#     device = torch.device(\"mps\")\"      \\nprint(\"Device:\", device)\\nThis will either print \"Device: cpu\" or \"Device: cuda\", depending on your machine.\\n Next, to reuse the chosen device setting in custom_collate_fn when we plug it\\ninto the PyTorch DataLoader class, we use the partial function from Python’s\\nfunctools standard library to create a new version of the function with the device\\nargument prefilled. Additionally, we set the allowed_max_length to 1024, which trun-\\ncates the data to the maximum context length supported by the GPT-2 model, which\\nwe will fine-tune later:\\nfrom functools import partial\\ncustomized_collate_fn = partial(\\n    custom_collate_fn,\\n    device=device,\\n    allowed_max_length=1024\\n)\\nNext, we can set up the data loaders as we did previously, but this time, we will use our\\ncustom collate function for the batching process.\\n \\n \\n \\n \\nUncomments these two \\nlines to use the GPU on \\nan Apple Silicon chip\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 246, 'page_label': '225'}, page_content='2257.4 Creating data loaders for an instruction dataset\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0     \\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_dataset = InstructionDataset(train_data, tokenizer)\\ntrain_loader = DataLoader(\\n    train_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=True,\\n    drop_last=True,\\n    num_workers=num_workers\\n)\\nval_dataset = InstructionDataset(val_data, tokenizer)\\nval_loader = DataLoader(\\n    val_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=False,\\n    drop_last=False,\\n    num_workers=num_workers\\n)\\ntest_dataset = InstructionDataset(test_data, tokenizer)\\ntest_loader = DataLoader(\\n    test_dataset,\\n    batch_size=batch_size,\\n    collate_fn=customized_collate_fn,\\n    shuffle=False,\\n    drop_last=False,\\n    num_workers=num_workers\\n)\\nLet’s examine the dimensions of the input and target batches generated by the train-\\ning loader:\\nprint(\"Train loader:\")\\nfor inputs, targets in train_loader:\\n    print(inputs.shape, targets.shape)\\nThe output is as follows (truncated to conserve space):\\nTrain loader:\\ntorch.Size([8, 61]) torch.Size([8, 61])\\ntorch.Size([8, 76]) torch.Size([8, 76])\\ntorch.Size([8, 73]) torch.Size([8, 73])\\n...\\nListing 7.6 Initializing the data loaders\\nYou can try to increase this number if \\nparallel Python processes are supported \\nby your operating system.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 247, 'page_label': '226'}, page_content='226 CHAPTER 7 Fine-tuning to follow instructions\\ntorch.Size([8, 74]) torch.Size([8, 74])\\ntorch.Size([8, 69]) torch.Size([8, 69])\\nThis output shows that the first input and target batch have dimensions 8 × 61, where\\n8 represents the batch size and 61 is the number of tokens in each training example in\\nthis batch. The second input and target batch have a different number of tokens—for\\ninstance, 76. Thanks to our cu stom collate function, the data  loader is able to create\\nbatches of different lengths. In the next section, we load a pretrained LLM that we\\ncan then fine-tune with this data loader.\\n7.5 Loading a pretrained LLM\\nWe have spent a lot of time preparing the dataset for instruction fine-tuning, which is\\na key aspect of the supervised fine-tuning process. Many other aspects are the same as\\nin pretraining, allowing us to reuse much of the code from earlier chapters.\\n Before beginning instruction fine-tuning,  we must first load a pretrained GPT\\nmodel that we want to fine-tune (see figure 7.15), a process we have undertaken previ-\\nously. However, instead of using the smallest 124-million-parameter model as before,\\nwe load the medium-sized model with 355 million parameters. The reason for this\\nchoice is that the 124-million -parameter model is too limit ed in capacity to achieve\\nFigure 7.15 The three-stage process for instruction fine-tuning an LLM. After the dataset \\npreparation, the process of fine-tuning an LLM for instruction-following begins with loading \\na pretrained LLM, which serves as the foundation for subsequent training. \\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nNow, we are loading\\nthe LLM for ﬁne-tuning.\\nNow, we create the PyTorch\\ndata loaders we will use for\\nﬁne-tuning the LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 248, 'page_label': '227'}, page_content='2277.5 Loading a pretrained LLM\\nsatisfactory results via instru ction fine-tuning. Specifical ly, smaller models lack the\\nnecessary capacity to learn and retain th e intricate patterns and nuanced behaviors\\nrequired for high-quality instruction-following tasks. \\n Loading our pretrained models requires the same code as when we pretrained the\\ndata (section 5.5) and fine-tuned it for classification (section 6.4), except that we now\\nspecify \"gpt2-medium (355M)\" instead of \"gpt2-small (124M)\". \\nNOTE Executing this code will initiate the download of the medium-sized\\nGPT model, which has a st orage requirement of approximately 1.42 giga-\\nbytes. This is roughly three times larger than the storage space needed for the\\nsmall model.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter04 import GPTModel\\nfrom chapter05 import load_weights_into_gpt\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,     # Vocabulary size\\n    \"context_length\": 1024,  # Context length\\n    \"drop_rate\": 0.0,        # Dropout rate\\n    \"qkv_bias\": True         # Query-key-value bias\\n}\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nCHOOSE_MODEL = \"gpt2-medium (355M)\"\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, \\n    models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval();\\nAfter executing the code, several files will be downloaded:\\ncheckpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 156kiB/s]\\nencoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 467kiB/s]\\nhparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 198kiB/s]\\nmodel.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G \\nListing 7.7 Loading the pretrained model\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 249, 'page_label': '228'}, page_content='228 CHAPTER 7 Fine-tuning to follow instructions\\n[05:50<00:00, 4.05MiB/s]\\nmodel.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 18.1MiB/s]\\nmodel.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 454kiB/s]\\nvocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]\\nNow, let’s take a moment to assess the pr etrained LLM’s performance on one of the\\nvalidation tasks by comparing its output to the expected response. This will give us a\\nbaseline understanding of how well the mode l performs on an instruction-following\\ntask right out of the box, prior to fine-tuning, and will help us appreciate the effect\\nof fine-tuning later on. We will use the firs t example from the validation set for this\\nassessment:\\ntorch.manual_seed(123)\\ninput_text = format_input(val_data[0])\\nprint(input_text)\\nThe content of the instruction is as follows:\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.\\n### Instruction:\\nConvert the active sentence to passive: \\'The chef cooks the meal every day.\\'\\nNext we generate the model’s response using the same generate function we used to\\npretrain the model in chapter 5:\\nfrom chapter05 import generate, text_to_token_ids, token_ids_to_text\\ntoken_ids = generate(\\n    model=model,\\n    idx=text_to_token_ids(input_text, tokenizer),\\n    max_new_tokens=35,\\n    context_size=BASE_CONFIG[\"context_length\"],\\n    eos_id=50256,\\n)\\ngenerated_text = token_ids_to_text(token_ids, tokenizer)\\nThe generate function returns the combined input and output text. This behavior was\\npreviously convenient since pretrained LLMs are primarily designed as text-completion\\nmodels, where the input and output are conc atenated to create coherent and legible\\ntext. However, when evaluating the model’s performance on a specific task, we often\\nwant to focus solely on the model’s generated response.\\n To isolate the model’s response text, we need to subtract the length of the input\\ninstruction from the start of the generated_text:\\nresponse_text = generated_text[len(input_text):].strip()\\nprint(response_text)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 250, 'page_label': '229'}, page_content=\"2297.6 Fine-tuning the LLM on instruction data\\nThis code removes the input text from the beginning of the generated_text, leaving\\nus with only the model’s generated response. The strip() function is then applied to\\nremove any leading or trailing whitespace characters. The output is \\n### Response:\\nThe chef cooks the meal every day.\\n### Instruction:\\nConvert the active sentence to passive: 'The chef cooks the\\nThis output shows that the pretrained mode l is not yet capable of correctly following\\nthe given instruction. While it does create a Response section, it simply repeats the\\noriginal input sentence and part of the instruction, failing to convert the active sen-\\ntence to passive voice as requested. So, let’s now implement the fine-tuning process\\nto improve the model’s ability to compre hend and appropriately respond to such\\nrequests.\\n7.6 Fine-tuning the LL M on instruction data\\nIt’s time to fine-tune the LLM for instructio ns (figure 7.16). We will take the loaded\\npretrained model in the previous section and further train it using the previously pre-\\npared instruction dataset prepared earlier in this chapter. We already did all the hard\\nwork when we implemented the instructio n dataset processing at the beginning of\\nFigure 7.16 The three-stage process for instruction fine-t uning an LLM. In step 5, we train the pretrained model \\nwe previously loaded on the instruction dataset we prepared earlier.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nAfter preparing the\\ndataset and loading a\\npretrained model, we\\nnow ﬁne-tune the model\\non the instruction data.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 251, 'page_label': '230'}, page_content='230 CHAPTER 7 Fine-tuning to follow instructions\\nthis chapter. For the fine-tuning process it self, we can reuse the loss calculation and\\ntraining functions implemented in chapter 5:\\nfrom chapter05 import (\\n    calc_loss_loader,\\n    train_model_simple\\n)\\nBefore we begin training, let’s calculate th e initial loss for the training and valida-\\ntion sets:\\nmodel.to(device)\\ntorch.manual_seed(123)\\nwith torch.no_grad():\\n    train_loss = calc_loss_loader(\\n        train_loader, model, device, num_batches=5\\n    )\\n    val_loss = calc_loss_loader(\\n        val_loader, model, device, num_batches=5\\n)\\nprint(\"Training loss:\", train_loss)\\nprint(\"Validation loss:\", val_loss)\\nThe initial loss values are as follows; as previously, our goal is to minimize the loss:\\nTraining loss: 3.825908660888672\\nValidation loss: 3.7619335651397705\\nDealing with hardware limitations\\nUsing and training a larger model like GPT-2 medium (355 million parameters) is more\\ncomputationally intensive than the smaller GPT-2 model (124 million parameters). If\\nyou encounter problems due to hardware limi tations, you can switch to the smaller\\nmodel by changing CHOOSE_MODEL = \"gpt2-medium (355M)\" to CHOOSE_MODEL =\\n\"gpt2-small (124M)\" (see section 7.5). Alternatively, to speed up the model training,\\nconsider using a GPU. The following supplementary section in this book’s code repos-\\nitory lists several options for using cloud GPUs: https:/ /mng.bz/EOEq.\\nThe following table provides reference run times for training each model on various\\ndevices, including CPUs and GPUs, for GPT-2. Running this code on a compatible GPU\\nrequires no code changes and can significantly speed up training. For the results\\nshown in this chapter, I used the GPT-2 medium model and trained it on an A100\\nGPU.Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 252, 'page_label': '231'}, page_content='2317.6 Fine-tuning the LLM on instruction data\\nWith the model and data loaders prepared , we can now proceed to train the model.\\nThe code in listing 7.8 sets up the training process, including initializing the opti-\\nmizer, setting the number of epochs, and defining the evaluation frequency and start-\\ning context to evaluate generated LLM resp onses during training based on the first\\nvalidation set instruction (val_data[0]) we looked at in section 7.5.\\nimport time\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(\\n    model.parameters(), lr=0.00005, weight_decay=0.1\\n)\\nnum_epochs = 2\\ntrain_losses, val_losses, tokens_seen = train_model_simple(\\n    model, train_loader, val_loader, optimizer, device,\\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\\n    start_context=format_input(val_data[0]), tokenizer=tokenizer\\n)\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe following output displays  the training progress over  two epochs, where a steady\\ndecrease in losses indicates improving ab ility to follow instructions and generate\\nappropriate responses:\\nEp 1 (Step 000000): Train loss 2.637, Val loss 2.626\\nEp 1 (Step 000005): Train loss 1.174, Val loss 1.103\\nEp 1 (Step 000010): Train loss 0.872, Val loss 0.944\\nEp 1 (Step 000015): Train loss 0.857, Val loss 0.906\\n...\\nListing 7.8 Instruction fine-tuning the pretrained LLM\\nModel name Device Run time for two epochs\\ngpt2-medium (355M) CPU (M3 MacBook Air) 15.78 minutes\\ngpt2-medium (355M) GPU (NVIDIA L4) 1.83 minutes\\ngpt2-medium (355M) GPU (NVIDIA A100) 0.86 minutes\\ngpt2-small (124M) CPU (M3 MacBook Air) 5.74 minutes\\ngpt2-small (124M) GPU (NVIDIA L4) 0.69 minutes\\ngpt2-small (124M) GPU (NVIDIA A100) 0.39 minutes\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 253, 'page_label': '232'}, page_content='232 CHAPTER 7 Fine-tuning to follow instructions\\nEp 1 (Step 000115): Train loss 0.520, Val loss 0.665\\nBelow is an instruction that describes a task. Write a response that \\nappropriately completes the request.  ### Instruction: Convert the \\nactive sentence to passive: \\'The chef cooks the meal every day.\\' \\n### Response: The meal is prepared every day by the chef.<|endoftext|>\\nThe following is an instruction that describes a task. \\nWrite a response that appropriately completes the request.  \\n### Instruction: Convert the active sentence to passive:\\nEp 2 (Step 000120): Train loss 0.438, Val loss 0.670\\nEp 2 (Step 000125): Train loss 0.453, Val loss 0.685\\nEp 2 (Step 000130): Train loss 0.448, Val loss 0.681\\nEp 2 (Step 000135): Train loss 0.408, Val loss 0.677\\n...\\nEp 2 (Step 000230): Train loss 0.300, Val loss 0.657\\nBelow is an instruction that describes a task. Write a response \\nthat appropriately completes the request.  ### Instruction: \\nConvert the active sentence to passive: \\'The chef cooks the meal \\nevery day.\\'  ### Response: The meal is cooked every day by the \\nchef.<|endoftext|>The following is an instruction that describes \\na task. Write a response that appropriately completes the request.  \\n### Instruction: What is the capital of the United Kingdom\\nTraining completed in 0.87 minutes.\\nThe training output shows that the model is learning effectively, as we can tell based\\non the consistently decreasing training and validation loss values over the two epochs.\\nThis result suggests that the model is gradually improving its ability to understand and\\nfollow the provided instructions. (Since the model demonstrated effective learning\\nwithin these two epochs, extending the training to a third epoch or more is not essen-\\ntial and may even be counterproductive as it could lead to increased overfitting.)\\n Moreover, the generated responses at the end of each epoch let us inspect the\\nmodel’s progress in correctly executing the given task in the validation set example. In\\nthis case, the model successfully  converts the active sentence \"The chef cooks the\\nmeal every day.\" into its passive voice counterpart: \"The meal is cooked every day by\\nthe chef.\"\\n We will revisit and evaluate  the response quality of the model in more detail later.\\nFor now, let’s examine the training and validation loss curves to gain additional\\ninsights into the model’s learning process. For this, we use the same plot_losses\\nfunction we used for pretraining:\\nfrom chapter05 import plot_losses\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\\nFrom the loss plot shown in figure 7.17, we can see that the model’s performance on\\nboth the training and validation sets improves  substantially over the course of train-\\ning. The rapid decrease in losses during the initial phase indicates that the model\\nquickly learns meaningful patterns and representations from the data. Then, as train-\\ning progresses to the second epoch, the lo sses continue to decrease but at a slower\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 254, 'page_label': '233'}, page_content='2337.7 Extracting and saving responses\\nrate, suggesting that the model is fine-tuning its learned representations and converg-\\ning to a stable solution. \\nWhile the loss plot in figure 7.17 indicates that the model is training effectively, the\\nmost crucial aspect is its performance in terms of response qu ality and correctness.\\nSo, next, let’s extract the responses and store them in a format that allows us to evalu-\\nate and quantify the response quality.\\n7.7 Extracting and saving responses\\nHaving fine-tuned the LLM on the training portion of the instruction dataset, we are\\nnow ready to evaluate its performance on the held-out test set. First, we extract the\\nmodel-generated responses for each input in  the test dataset and collect them for\\nmanual analysis, and then we evaluate the LLM to quantify the quality of the\\nresponses, as highlighted in figure 7.18.\\nExercise 7.3 Fine-tuning on the original Alpaca dataset \\nThe Alpaca dataset, by researchers at Stanford, is one of the earliest and most pop-\\nular openly shared instruction datasets, consisting of 52,002 entries. As an alterna-\\ntive to the instruction-data.json file we use here, consider fine-tuning an LLM on\\nthis dataset. The dataset is available at https:/ /mng.bz/NBnE.\\nThis dataset contains 52,002 entries, wh ich is approximately 50 times more than\\nthose we used here, and most entries are longer. Thus, I highly recommend using a\\nGPU to conduct the training, which will acce lerate the fine-tunin g process. If you\\nencounter out-of-memory errors, consider reducing the batch_size from 8 to 4, 2,\\nor even 1. Lowering the allowed_max_length from 1,024 to 512 or 256 can also\\nhelp manage memory problems.\\nFigure 7.17 The training and validation loss trends over two \\nepochs. The solid line represents the training loss, showing a \\nsharp decrease before stabilizing, while the dotted line \\nrepresents the validation loss, which follows a similar pattern. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 255, 'page_label': '234'}, page_content='234 CHAPTER 7 Fine-tuning to follow instructions\\nTo complete the response in struction step, we use the generate function. We then\\nprint the model responses alon gside the expected test set answers for the first three\\ntest set entries, presenting them side by side for comparison:\\ntorch.manual_seed(123)\\nfor entry in test_data[:3]:     \\n    input_text = format_input(entry)\\n    token_ids = generate(              \\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    \\n    response_text = (\\n        generated_text[len(input_text):]\\n        .replace(\"### Response:\", \"\")\\n        .strip()\\n    )\\nFigure 7.18 The three-stage process for instruction fine-tuning the LLM. In the first \\ntwo steps of stage 3, we extract and collect the model responses on the held-out test \\ndataset for further analysis and then evaluate the model to quantify the performance of \\nthe instruction-fine-tuned LLM.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nThen, we compare the model’s\\nresponses with the correct\\nresponses in the test set.\\nIn this section, we extract\\nthe responses from our\\nﬁne-tuned LLM.\\nIterates over the \\nfirst three test set \\nsamples\\nUses the generate function \\nimported in section 7.5\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 256, 'page_label': '235'}, page_content='2357.7 Extracting and saving responses\\n    print(input_text)\\n    print(f\"\\\\nCorrect response:\\\\n>> {entry[\\'output\\']}\")\\n    print(f\"\\\\nModel response:\\\\n>> {response_text.strip()}\")\\n    print(\"-------------------------------------\")\\nAs mentioned earlier, the generate function returns the combined input and output\\ntext, so we use slicing and the .replace() method on the generated_text contents to\\nextract the model’s response. The instructions, followed by the given test set response\\nand model response, are shown next.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\n### Instruction:\\nRewrite the sentence using a simile.\\n### Input:\\nThe car is very fast.\\nCorrect response:\\n>> The car is as fast as lightning.\\nModel response:\\n>> The car is as fast as a bullet.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\n### Instruction:\\nWhat type of cloud is typically associated with thunderstorms?\\nCorrect response:\\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\\nModel response:\\n>> The type of cloud associated with thunderstorms is a cumulus cloud.\\nBelow is an instruction that describes a task. Write a response that appropriately\\ncompletes the request.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 257, 'page_label': '236'}, page_content='236 CHAPTER 7 Fine-tuning to follow instructions\\n### Instruction:\\nName the author of ‘Pride and Prejudice.’\\nCorrect response:\\n>> Jane Austen.\\nModel response:\\n>> The author of ‘Pride and Prejudice’ is Jane Austen.\\nAs we can see based on the test set inst ructions, given respon ses, and the model’s\\nresponses, the model performs relatively well. The answers to the first and last instruc-\\ntions are clearly correct, while the second answer is close but not entirely accurate.\\nThe model answers with “cumulus cloud” instead of “cumulonimbus,” although it’s\\nworth noting that cumulus clouds can deve lop into cumulonimbus clouds, which are\\ncapable of producing thunderstorms.\\n Most importantly, model evaluation is not as straightforward as it is for completion\\nfine-tuning, where we simply calculate the percentage of correct spam/non-spam class\\nlabels to obtain the classification’s accuracy. In practice, instruction-fine-tuned LLMs\\nsuch as chatbots are evaluated via multiple approaches:\\n\\uf0a1 Short-answer and multiple-choice benchmarks, such as Measuring Massive Mul-\\ntitask Language Understanding (MMLU; https:/ /arxiv.org/abs/2009.03300),\\nwhich test the general knowledge of a model.\\n\\uf0a1 Human preference comparison to othe r LLMs, such as LMSYS chatbot arena\\n(https:/ /arena.lmsys.org).\\n\\uf0a1 Automated conversational benchmarks, where another LLM like GPT-4 is\\nused to evaluate the responses, such as AlpacaEval ( https:/ /tatsu-lab.github.io/\\nalpaca_eval/).\\nIn practice, it can be useful to consider all three types of evaluation methods: multiple-\\nchoice question answering, human evalua tion, and automated me trics that measure\\nconversational performance. However, since we are primarily interested in assessing con-\\nversational performance rather than just th e ability to answer multiple-choice ques-\\ntions, human evaluation and automated metrics may be more relevant.\\nConversational performance\\nConversational performance of  LLMs refers to their abilit y to engage in human-like\\ncommunication by understanding context, nuance, and intent. It encompasses skills\\nsuch as providing relevant and coherent responses, maintaining consistency, and\\nadapting to different topics and styles of interaction.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 258, 'page_label': '237'}, page_content='2377.7 Extracting and saving responses\\nHuman evaluation, while providing valuable insights, can be relatively laborious and\\ntime-consuming, especially when dealing with a large number of responses. For\\ninstance, reading and assigning ratings to all 1,100 responses would require a signifi-\\ncant amount of effort.\\n So, considering the scale of the task at hand, we will implement an approach simi-\\nlar to automated conversational benchmarks , which involves evaluating the responses\\nautomatically using another LLM. This method will allow us to efficiently assess the\\nquality of the generated responses withou t the need for extensive human involve-\\nment, thereby saving time and resources while still obt aining meaningful perfor-\\nmance indicators.\\n Let’s employ an approach inspired by AlpacaEval, using another LLM to evaluate\\nour fine-tuned model’s responses. However, instead of relying on a publicly available\\nbenchmark dataset, we use our own custom te st set. This customization allows for a\\nmore targeted and relevant assessment of the model’s performance within the context\\nof our intended use cases, represented in our instruction dataset.\\n To prepare the responses for this evalua tion process, we append the generated\\nmodel responses to the \\ntest_set dictionary and save the updated data as an\\n\"instruction-data-with-response.json\" file for record keeping. Additionally, by\\nsaving this file, we can easily load and an alyze the responses in separate Python ses-\\nsions later on if needed.\\n The following code listing uses the generate method in the same manner as\\nbefore; however, we now iterate over the entire test_set. Also, instead of printing the\\nmodel responses, we add them to the test_set dictionary.\\nfrom tqdm import tqdm\\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\\n    input_text = format_input(entry)\\n    token_ids = generate(\\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    \\n    response_text = (\\n        generated_text[len(input_text):]\\n        .replace(\"### Response:\", \"\")\\n        .strip()\\n    )\\n    test_data[i][\"model_response\"] = response_text\\nwith open(\"instruction-data-with-response.json\", \"w\") as file:\\n    json.dump(test_data, file, indent=4)        \\nListing 7.9 Generating test set responses\\nindent for \\npretty-printing\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 259, 'page_label': '238'}, page_content='238 CHAPTER 7 Fine-tuning to follow instructions\\nProcessing the dataset takes about 1 minute on an A100 GPU and 6 minutes on an M3\\nMacBook Air: \\n100%|██████████| 110/110 [01:05<00:00,  1.68it/s]\\nLet’s verify that the responses ha ve been correctly added to the test_set dictionary\\nby examining one of the entries:\\nprint(test_data[0])\\nThe output shows that the model_response has been added correctly:\\n{\\'instruction\\': \\'Rewrite the sentence using a simile.\\', \\n \\'input\\': \\'The car is very fast.\\', \\n \\'output\\': \\'The car is as fast as lightning.\\', \\n \\'model_response\\': \\'The car is as fast as a bullet.\\'}\\nFinally, we save the model as gpt2-medium355M-sft.pth file to be able to reuse it in\\nfuture projects:\\nimport re\\nfile_name = f\"{re.sub(r\\'[ ()]\\', \\'\\', CHOOSE_MODEL) }-sft.pth\"     \\ntorch.save(model.state_dict(), file_name)\\nprint(f\"Model saved as {file_name}\")\\nThe saved model can then be loaded via model.load_state_dict(torch.load(\"gpt2\\n-medium355M-sft.pth\")).\\n7.8 Evaluating the fine-tuned LLM\\nPreviously, we judged the performance of an  instruction-fine-tuned model by looking\\nat its responses on three examples of the te st set. While this gi ves us a rough idea of\\nhow well the model performs, this method does not scale well to larger amounts of\\nresponses. So, we implement a method to automate the response evaluation of the\\nfine-tuned LLM using another, larger LLM, as highlighted in figure 7.19.\\n To evaluate test set responses in an automated fashion, we utilize an existing\\ninstruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This\\nmodel can be run locally using the open source Ollama application ( https:/ /ollama\\n.com).\\nNOTE Ollama is an efficient applicatio n for running LLMs on a laptop. It\\nserves as a wrapper around the open source llama.cpp library (https:/ /github\\n.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to\\nmaximize efficiency. However, Ollama is only a tool for generating text using\\nLLMs (inference) and does not support training or fine-tuning LLMs.\\nRemoves white spaces\\nand parentheses\\nfrom file name\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 260, 'page_label': '239'}, page_content='2397.8 Evaluating the fine-tuned LLM\\nTo execute the following code, install Ollama by visiting https:/ /ollama.com and fol-\\nlow the provided instructions for your operating system:\\n\\uf0a1 For macOS and Windows users —Open the downloaded Ollama application. If\\nprompted to install command-line usage, select Yes.\\n\\uf0a1 For Linux users—Use the installation command av ailable on the Ollama website.\\nBefore implementing the model evaluation  code, let’s first download the Llama 3\\nmodel and verify that Ollama is functionin g correctly by using it from the command-\\nline terminal. To use Ollama from the command line, you must either start the Ollama\\napplication or run ollama serve in a separate terminal, as shown in figure 7.20.\\nUsing larger LLMs via web APIs \\nThe 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. How-\\never, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI.\\nFor readers intere sted in exploring how to utilize GPT-4 through the OpenAI API to\\nassess generated model responses, an optional code notebook is available within\\nthe supplementary materials accompanying this book at https:/ /mng.bz/BgEv.\\nFigure 7.19 The three-stage process for instruction fine-tuning the LLM. In this last \\nstep of the instruction-fine-tuning pipeline, we implement a method to quantify the \\nperformance of the fine-tuned model by scoring the responses it generated for the test.\\n2) Batching the\\ndataset\\nStage 1:\\nPreparing the dataset\\n1) Dataset\\ndownload and\\nformatting\\n3) Creating\\ndata loaders\\n4) Loading a\\npretrained LLM\\n5) Instruction\\nﬁne-tuning the\\nLLM\\n9) Scoring the\\nresponses\\nStage 2:\\nFine-tuning the LLM\\nStage 3:\\nEvaluating the LLM\\n7) Extracting\\nresponses\\n6) Inspecting\\nthe modeling\\nloss\\n8) Qualitative\\nevaluation\\nAfter extracting the responses by our\\nﬁne-tuned LLM, we use another LLM to\\nautomatically evaluate these responses.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 261, 'page_label': '240'}, page_content='240 CHAPTER 7 Fine-tuning to follow instructions\\nWith the Ollama application or ollama serve running in a different terminal, execute\\nthe following command on the command line (not in a Python session) to try out the\\n8-billion-parameter Llama 3 model:\\nollama run llama3\\nThe first time you execute this command, this model, which takes up 4.7 GB of stor-\\nage space, will be automatically downloaded. The output looks like the following:\\npulling manifest\\npulling 6a0746a1ec1a... 100% |████████████████| 4.7 GB\\npulling 4fa551d4f938... 100% |████████████████|  12 KB\\npulling 8ab4849b038c... 100% |████████████████|  254 B\\npulling 577073ffcc6c... 100% |████████████████|  110 B\\npulling 3f8eb4da87fa... 100% |████████████████|  485 B\\nverifying sha256 digest\\nwriting manifest\\nremoving any unused layers\\nsuccess\\nFigure 7.20 Two options for running Ollama. The left panel illustrates starting Ollama using ollama serve. \\nThe right panel shows a second option in macOS, running the Ollama application in the background instead of \\nusing the ollama serve command to start the application.\\nFirst option: make sure to start ollama in a separate\\nterminal via  the command.ollama serve\\nThen run to downloadollama run llama3\\nand use the 8-billion-parameter Llama 3 model.\\nSecond option: if you are using macOS, you\\ncan also start the ollama application and\\nmake sure it is running in the background\\ninstead of running .\\nollama serve\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 262, 'page_label': '241'}, page_content='2417.8 Evaluating the fine-tuned LLM\\nOnce the model download is complete, we are presented with a command-line inter-\\nface that allows us to interact with the model. For example, try asking the model,\\n“What do llamas eat?”\\n>>> What do llamas eat?\\nLlamas are ruminant animals, which means they have a four-chambered\\nstomach and eat plants that are high in fiber. In the wild, \\nllamas typically feed on:\\n1. Grasses: They love to graze on various types of grasses, including tall\\ngrasses, wheat, oats, and barley.\\nNote that the response you see might differ since Ollama is not deterministic as of this\\nwriting.\\n You can end this ollama run llama3 session using the input /bye. However, make\\nsure to keep the ollama serve command or the Ollama application running for the\\nremainder of this chapter.\\n The following code verifies that the Olla ma session is running properly before we\\nuse Ollama to evaluate the test set responses:\\nimport psutil\\ndef check_if_running(process_name):\\n    running = False\\n    for proc in psutil.process_iter([\"name\"]):\\n        if process_name in proc.info[\"name\"]:\\n            running = True\\n            break\\n    return running\\nollama_running = check_if_running(\"ollama\")\\nif not ollama_running:\\n    raise RuntimeError(\\n        \"Ollama not running. Launch ollama before proceeding.\"\\n)\\nprint(\"Ollama running:\", check_if_running(\"ollama\"))\\nAlternative Ollama models \\nThe llama3 in the ollama run llama3 command refers to the instruction-fine-tuned\\n8-billion-parameter Llama 3 model. Using Ollama with the llama3 model requires\\napproximately 16 GB of RAM. If your machine does not have sufficient RAM, you can\\ntry using a smaller model, such as the 3.8-billion-parameter phi3 model via ollama\\nrun llama3, which only requires around 8 GB of RAM.\\nFor more powerful computers, you can also use the larger 70-billion-parameter Llama\\n3 model by replacing llama3 with llama3:70b. However, this model requires signifi-\\ncantly more computational resources.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 263, 'page_label': '242'}, page_content='242 CHAPTER 7 Fine-tuning to follow instructions\\nEnsure that the output from exec uting the previous code displays Ollama running:\\nTrue. If it shows False, verify that the ollama serve command or the Ollama applica-\\ntion is actively running.\\nAn alternative to the ollama run command for interacting with the model is through\\nits REST API using Python. The query_model function shown in the following listing\\ndemonstrates how to use the API.\\nimport urllib.request\\ndef query_model(\\n    prompt, \\n    model=\"llama3\", \\n    url=\"http://localhost:11434/api/chat\"\\n):\\n    data = {            \\n        \"model\": model,\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        \"options\": {        \\n            \"seed\": 123,\\nRunning the code in a new Python session \\nIf you already closed your Python session or if you prefer to execute the remaining\\ncode in a different Python session, use the following code, which loads the instruction\\nand response data file we previously created and redefines the \\nformat_input func-\\ntion we used earlier (the tqdm progress bar utility is used later): \\nimport json\\nfrom tqdm import tqdm\\nfile_path = \"instruction-data-with-response.json\"\\nwith open(file_path, \"r\") as file:\\n    test_data = json.load(file)\\ndef format_input(entry):\\n    instruction_text = (\\n        f\"Below is an instruction that describes a task. \"\\n        f\"Write a response that appropriately completes the request.\"\\n        f\"\\\\n\\\\n### Instruction:\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    input_text = (\\n        f\"\\\\n\\\\n### Input:\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    )\\n    return instruction_text + input_text\\nListing 7.10 Querying a local Ollama model\\nCreates the data \\npayload as a dictionary\\nSettings for deterministic \\nresponses\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 264, 'page_label': '243'}, page_content='2437.8 Evaluating the fine-tuned LLM\\n            \"temperature\": 0,\\n            \"num_ctx\": 2048\\n        }\\n    }\\n    payload = json.dumps(data).encode(\"utf-8\")   \\n    request = urllib.request.Request(                      \\n        url,                                               \\n        data=payload,                                      \\n        method=\"POST\"                                      \\n    )\\n    \\n    request.add_header(\"Content-Type\", \"application/json\")  \\n    response_data = \"\"\\n    with urllib.request.urlopen(request) as response:  \\n        while True:\\n            line = response.readline().decode(\"utf-8\")\\n            if not line:\\n                break\\n            response_json = json.loads(line)\\n            response_data += response_json[\"message\"][\"content\"]\\n    return response_data\\nBefore running the subsequent code cells in this notebook, ensure that Ollama is still\\nrunning. The previous code cells should print \"Ollama running: True\" to confirm\\nthat the model is active and ready to receive requests.\\n The following is an example of how to use the query_model function we just\\nimplemented:\\nmodel = \"llama3\"\\nresult = query_model(\"What do Llamas eat?\", model)\\nprint(result)\\nThe resulting response is as follows:\\nLlamas are ruminant animals, which means they have a four-chambered \\nstomach that allows them to digest plant-based foods. Their diet \\ntypically consists of:\\n1. Grasses: Llamas love to graze on grasses, including tall grasses, \\nshort grasses, and even weeds.\\n...\\nUsing the query_model function defined earlier, we can evaluate the responses gen-\\nerated by our fine-tuned model that prom pts the Llama 3 model to rate our fine-\\ntuned model’s responses on a scale from  0 to 100 based on the given test set\\nresponse as reference. \\nConverts the\\ndictionary to a JSON-\\nformatted string and\\nencodes it to bytes\\nCreates a request \\nobject, setting the \\nmethod to POST and \\nadding necessary \\nheaders\\nSends the \\nrequest and \\ncaptures the \\nresponse\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 265, 'page_label': '244'}, page_content='244 CHAPTER 7 Fine-tuning to follow instructions\\n First, we apply this approach to the firs t three examples from the test set that we\\npreviously examined:\\nfor entry in test_data[:3]:\\n    prompt = (\\n        f\"Given the input `{format_input(entry)}` \"\\n        f\"and correct output `{entry[\\'output\\']}`, \"\\n        f\"score the model response `{entry[\\'model_response\\']}`\"\\n        f\" on a scale from 0 to 100, where 100 is the best score. \"\\n    )\\n    print(\"\\\\nDataset response:\")\\n    print(\">>\", entry[\\'output\\'])\\n    print(\"\\\\nModel response:\")\\n    print(\">>\", entry[\"model_response\"])\\n    print(\"\\\\nScore:\")\\n    print(\">>\", query_model(prompt))\\n    print(\"\\\\n-------------------------\")\\nThis code prints outputs similar to the following (as of this writing, Ollama is not fully\\ndeterministic, so the generated texts may vary):\\nDataset response:\\n>> The car is as fast as lightning.\\nModel response:\\n>> The car is as fast as a bullet.\\nScore:\\n>> I’d rate the model response “The car is as fast as a bullet.” an 85 out of 100.\\nHere’s why:\\nThe response uses a simile correctly, comparing the speed of the car to something\\nelse (in this case, a bullet).\\nThe comparison is relevant and makes sense, as bullets are known for their high\\nvelocity.\\nThe phrase “as fast as” is used correctly to introduce the simile.\\nThe only reason I wouldn’t give it a perfect score is that some people might find the\\ncomparison slightly less vivid or evocativ e than others. For example, comparing\\nsomething to lightning (as in the original response) can be more dramatic and atten-\\ntion grabbing. However, “as fast as a bullet” is still a strong and effective simile that\\neffectively conveys the idea of the car’s speed.\\nOverall, I think the model did a great job!\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 266, 'page_label': '245'}, page_content='2457.8 Evaluating the fine-tuned LLM\\nDataset response:\\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\\nModel response:\\n>> The type of cloud associated with thunderstorms is a cumulus cloud.\\nScore:\\n>> I’d score this model response as 40 out of 100.\\nHere’s why:\\nThe model correctly identifies that thunde rstorms are related to clouds (correctly\\nidentifying the type of phenomenon).\\nHowever, it incorrectly specifies the type of cloud associated with thunderstorms.\\nCumulus clouds are not typically associated with thunderstorms; cumulonimbus\\nclouds are.\\nThe response lacks precision and accuracy in its description.\\nOverall, while the model attempts to address the instruction, it provides an incorrect\\nanswer, which is a significant error.\\nDataset response:\\n>> Jane Austen.\\nModel response:\\n>> The author of ‘Pride and Prejudice’ is Jane Austen.\\nScore:\\n>> I’d rate my own response as 95 out of 100. Here’s why:\\nThe response accurately answers the question by naming the author of Pride and Prej-\\nudice as Jane Austen.\\nThe response is concise and clear, making it easy to understand.\\nThere are no grammatical errors or ambiguities that could lead to confusion.\\nThe only reason I wouldn’t give myself a perfect score is that the response is slightly\\nredundant—it’s not necessary to rephrase the question in the answer. A more con-\\ncise response would be simply “Jane Austen.”\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 267, 'page_label': '246'}, page_content='246 CHAPTER 7 Fine-tuning to follow instructions\\nThe generated responses show that the Ll ama 3 model provides reasonable evalua-\\ntions and is capable of assigning partial po ints when a model’s answer is not entirely\\ncorrect. For instance, if we consider the evaluation of the “cumulus cloud” answer, the\\nmodel acknowledges the partial correctness of the response.\\n The previous prompt returns highly deta iled evaluations in addition to the score.\\nWe can modify the prompt to just genera te integer scores ranging from 0 to 100,\\nwhere 100 represents the best possible score. This modification allows us to calculate\\nan average score for our mo del, which serves as a mo re concise and quantitative\\nassessment of its performance. The generate_model_scores function shown in the\\nfollowing listing uses a modified  prompt telling the model to \"Respond with the\\ninteger number only.\"\\ndef generate_model_scores(json_data, json_key, model=\"llama3\"):\\n    scores = []\\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\\n        prompt = (\\n            f\"Given the input `{format_input(entry)}` \"\\n            f\"and correct output `{entry[\\'output\\']}`, \"\\n            f\"score the model response `{entry[json_key]}`\"\\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\\n            f\"Respond with the integer number only.\"  \\n        )\\n        score = query_model(prompt, model)\\n        try:\\n            scores.append(int(score))\\n        except ValueError:\\n            print(f\"Could not convert score: {score}\")\\n            continue\\n    return scores\\nLet’s now apply the generate_model_scores function to the entire test_data set,\\nwhich takes about 1 minute on a M3 Macbook Air:\\nscores = generate_model_scores(test_data, \"model_response\")\\nprint(f\"Number of scores: {len(scores)} of {len(test_data)}\")\\nprint(f\"Average score: {sum(scores)/len(scores):.2f}\\\\n\")\\nThe results are as follows:\\nScoring entries: 100%|████████████████████████| 110/110 \\n[01:10<00:00,  1.56it/s]\\nNumber of scores: 110 of 110\\nAverage score: 50.32\\nThe evaluation output shows that our fine -tuned model achieves an average score\\nabove 50, which provid es a useful benchmark for comparison against other models\\nListing 7.11 Evaluating the instruction fine-tuning LLM\\nModified \\ninstruction line \\nto only return \\nthe score\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 268, 'page_label': '247'}, page_content='2477.9 Conclusions\\nor for experimenting with di fferent training configuratio ns to improve the model’s\\nperformance.\\n It’s worth noting that Ollama is not entirely deterministic across operating systems\\nat the time of this writing, which means th at the scores you obtain might vary slightly\\nfrom the previous scores. To obtain more robust results, you can repeat the evaluation\\nmultiple times and average the resulting scores.\\n To further improve our model’s performa nce, we can explore various strategies,\\nsuch as\\n\\uf0a1 Adjusting the hyperparameters during fine -tuning, such as the learning rate,\\nbatch size, or number of epochs\\n\\uf0a1 Increasing the size of the training datase t or diversifying the examples to cover\\na broader range of topics and styles\\n\\uf0a1 Experimenting with different prompts or instruction formats to guide the\\nmodel’s responses more effectively\\n\\uf0a1 Using a larger pretrained model, which may have greater capacity to capture\\ncomplex patterns and generate more accurate responses\\nNOTE For reference, when using the me thodology described herein, the\\nLlama 3 8B base model, without any fine-tuning, achieves an average score of\\n58.51 on the test set. The Llama 3 8B instruct model, which has been fine-\\ntuned on a general instruction-following dataset, achieves an impressive aver-\\nage score of 82.6.\\n7.9 Conclusions\\nThis chapter marks the conclusion of our journey through the LLM development\\ncycle. We have covered all the essential st eps, including implementing an LLM archi-\\ntecture, pretraining an LLM, and fine-tuning it for specific tasks, as summarized in fig-\\nure 7.21. Let’s discuss some ideas for what to look into next.\\n7.9.1 What’s next?\\nWhile we covered the most esse ntial steps, there is an opti onal step that can be per-\\nformed after instruction fine-tuning: preference fine-tuning. Preference fine-tuning is\\nparticularly useful for customizing a model to better align with specific user prefer-\\nences. If you are interested in exploring this further, see the 04_preference-tuning-\\nwith-dpo f o l d e r  i n  t h i s  b o o k ’ s  s u p p l e m e n t a r y  G i t H u b  r e p o s i t o r y  a t  https:/ /mng\\n.bz/dZwD.\\nExercise 7.4 Parameter-efficient fine-tuning with LoRA \\nTo instruction fine-tune an LLM more effici ently, modify the code in this chapter to\\nuse the low-rank adaptation method (LoRA) from appendix E. Compare the training\\nrun time and model performance before and after the modification.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 269, 'page_label': '248'}, page_content='248 CHAPTER 7 Fine-tuning to follow instructions\\nIn addition to the main content covered in this book, the GitHub repository also con-\\ntains a large selection of bo nus material that you may fi nd valuable. To learn more\\nabout these additional resources, visit the Bonus Material section on the repository’s\\nREADME page: https:/ /mng.bz/r12g.\\n7.9.2 Staying up to date in a fast-moving field\\nThe fields of AI and LLM research are evolving at a rapid (and, depending on who\\nyou ask, exciting) pace. One way to keep up with the latest advancements is to explore\\nrecent research papers on arXiv at https:/ /arxiv.org/list/cs.LG/recent. Additionally,\\nmany researchers and practitioners are very active in sharing and discussing the latest\\ndevelopments on social medi a platforms like X (formerl y Twitter) and Reddit. The\\nsubreddit r/LocalLLaMA, in particular, is a good resource for connecting with the\\ncommunity and staying informed about the la test tools and trends. I also regularly\\nshare insights and write about the latest in LLM research on my blog, available at\\nhttps:/ /magazine.sebastianraschka.com and https:/ /sebastianraschka.com/blog/.\\n7.9.3 Final words\\nI hope you have enjoyed this journey of implementing an LLM from the ground up\\nand coding the pretraining and fine-tuning functions from scratch. In my opinion,\\nbuilding an LLM from scratch is the most effective way to gain a deep understanding\\nof how LLMs work. I hope that this hands-on approach has provided you with valuable\\ninsights and a solid foundation in LLM development.\\nFigure 7.21 The three main stages of coding an LLM. \\nIn the previous chapter,\\nwe ﬁne-tuned the pretrained\\nLLM to classify texts.\\nIn chapter 5, we\\npretrained an LLM.\\nIn chapter 4, we\\nimplemented a GPT-like\\nLLM architecture.\\nIn chapter 5, we also loaded\\npretrained model weights\\ninto the LLM architecture.\\n1) Data\\npreparation\\nand sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\nIn this chapter, we\\nﬁne-tune the pretrained\\nLLM to follow instructions.\\n9) Fine-tuning\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 270, 'page_label': '249'}, page_content='249Summary\\n While the primary purpose of this book is educational, you may be interested in\\nutilizing different and more powerful LLMs for real-world applications. For this, I rec-\\nommend exploring popular tools such as Axolotl ( https:/ /github.com/OpenAccess\\n-AI-Collective/axolotl) or LitGPT ( https:/ /github.com/Lightning-AI/litgpt), which I\\nam actively involved in developing.\\n Thank you for joining me on this learning  journey, and I wish you all the best in\\nyour future endeavors in the exciting field of LLMs and AI!\\nSummary\\n\\uf0a1 The instruction-fine-tuning process adap ts a pretrained LLM to follow human\\ninstructions and generate desired responses.\\n\\uf0a1 Preparing the dataset involves download ing an instruction-response dataset,\\nformatting the entries, and splitting it into train, validation, and test sets.\\n\\uf0a1 Training batches are cons tructed using a custom collate function that pads\\nsequences, creates target token IDs, and masks padding tokens.\\n\\uf0a1 We load a pretrained GPT-2 medium model with 355 million parameters to\\nserve as the starting point for instruction fine-tuning.\\n\\uf0a1 The pretrained model is fine-tuned on the instruction dataset using a training\\nloop similar to pretraining.\\n\\uf0a1 Evaluation involves extracting model re sponses on a test set and scoring them\\n(for example, using another LLM).\\n\\uf0a1 The Ollama application with an 8-bil lion-parameter Llama model can be used\\nto automatically score the fine-tuned mode l’s responses on the test set, provid-\\ning an average score to quantify performance.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 271, 'page_label': '250'}, page_content='Licensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 272, 'page_label': '251'}, page_content='251\\nappendix A\\nIntroduction to PyTorch\\nThis appendix is designed to equip you with the necessary skills and knowledge to\\nput deep learning into practice and implement large language models (LLMs)\\nfrom scratch. PyTorch, a popular Python-based deep learning library, will be our\\nprimary tool for this book. I will guid e you through setting up a deep learning\\nworkspace armed with PyTorch and GPU support. \\n Then you’ll learn about the essential concept of tensors and their usage in\\nPyTorch. We will also delve into PyTorc h’s automatic differentiation engine, a fea-\\nture that enables us to conveniently and efficiently use backpropagation, which is a\\ncrucial aspect of neural network training.\\n This appendix is meant as a primer fo r those new to deep learning in PyTorch.\\nWhile it explains PyTorch from the ground  up, it’s not meant to be an exhaustive\\ncoverage of the PyTorch library. Instead,  we’ll focus on the PyTorch fundamentals\\nwe will use to implement LLMs. If you ar e already familiar with deep learning, you\\nmay skip this appendix and directly move on to chapter 2.\\nA.1 What is PyTorch?\\nPyTorch ( https:/ /pytorch.org/) is an open source Python-based deep learning\\nlibrary. According to Papers With Code (https:/ /paperswithcode.com/trends), a plat-\\nform that tracks and analyzes research papers, PyTorch has been the most widely\\nused deep learning library for research since 2019 by a wide margin. And, accord-\\ning to the Kaggle Data Science and Machine Learning Survey 2022 (https:/ /www.kaggle\\n.com/c/kaggle-survey-2022), the number of respondents using PyTorch is approxi-\\nmately 40%, which grows every year.\\n One of the reasons PyTorch is so popular  is its user-friendly interface and effi-\\nciency. Despite its accessibility, it does n’t compromise on fl exibility, allowing\\nadvanced users to tweak lower-level aspect s of their models for customization and\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 273, 'page_label': '252'}, page_content='252 APPENDIX A Introduction to PyTorch\\noptimization. In short, for many practitioners and researchers, PyTorch offers just the\\nright balance between usability and features. \\nA.1.1 The three core co mponents of PyTorch\\nPyTorch is a relatively comprehensive library, and one way to approach it is to focus on\\nits three broad components, summarized in figure A.1. \\nFirst, PyTorch is a tensor library  that extends the concept of the array-oriented pro-\\ngramming library NumPy with the additional feature that accelerates computation on\\nGPUs, thus providing a seamless switch be tween CPUs and GPUs. Second, PyTorch is\\nan automatic differentiation engine , also known as autograd, that enables the automatic\\ncomputation of gradients for tensor oper ations, simplifying backpropagation and\\nmodel optimization. Finally, PyTorch is a deep learning library. It offers modular, flexi-\\nble, and efficient building blocks, including pretrained models, loss functions, and\\noptimizers, for designing and training a wide range of deep learning models, catering\\nto both researchers and developers.\\nA.1.2 Defining deep learning\\nIn the news, LLMs are often referred to as  AI models. However, LLMs are also a type\\nof deep neural network, and PyTorch is a deep learning libr ary. Sound confusing?\\nLet’s take a brief moment and summarize the relationship between these terms before\\nwe proceed. \\n AI is fundamentally about creating comput er systems capable of performing tasks\\nthat usually require human intelligence. These tasks include understanding natural\\nlanguage, recognizing patterns, and making decisions. (Despite significant progress,\\nAI is still far from achieving this level of general intelligence.)\\nTensor library Automatic\\ndiﬀerentiation engine\\nDeep learning\\nlibrary\\n2\\n3 PyTorch’s deep learning\\nutilities make use of its\\ntensor library and automatic\\ndifferentiation engine.\\nPyTorch implements a\\ntensor (array) library for\\nefﬁcient computing.\\nPyTorch includes utilities to\\ndifferentiate computations\\nautomatically\\nFigure A.1 PyTorch’s three main components include a tensor library as \\na fundamental building block for computing, automatic differentiation for \\nmodel optimization, and deep learning utility functions, making it easier to \\nimplement and train deep neural network models.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 274, 'page_label': '253'}, page_content='253A.1 What is PyTorch?\\n Machine learning represents a subfield of AI, as illustrated in figure A.2, that focuses\\non developing and improving learning al gorithms. The key idea behind machine\\nlearning is to enable computers to learn from data and make predictions or decisions\\nwithout being explicitly programmed to pe rform the task. This involves developing\\nalgorithms that can identify patterns, lear n from historical data, and improve their\\nperformance over time with more data and feedback.\\nMachine learning has been integral in the evolution of AI, powering many of the\\nadvancements we see today, including LLMs. Machine learning is also behind technol-\\nogies like recommendatio n systems used by online reta ilers and streaming services,\\nemail spam filtering, voice recognition in vi rtual assistants, and ev en self-driving cars.\\nThe introduction and advancement of mach ine learning have significantly enhanced\\nAI’s capabilities, enabling it to move beyond strict rule-based systems and adapt to new\\ninputs or changing environments.\\n Deep learning is a subcategory of machine learning that focuses on the training and\\napplication of deep neural networks. These deep neural  networks were originally\\ninspired by how the human brain works, particularly the interconnection between\\nmany neurons. The “deep” in deep learning refers to the multiple hidden layers of\\nartificial neurons or nodes that allow them to model complex, nonlinear relationships\\nin the data. Unlike traditional machine learning techniques that excel at simple pat-\\ntern recognition, deep lear ning is particularly good at handling unstructured data\\nlike images, audio, or text, so it is particularly well suited for LLMs.\\n The typical predictive modeling workflow (also referred to as supervised learning) in\\nmachine learning and deep learning is summarized in figure A.3. \\n Using a learning algorithm,  a model is trained on a training dataset consisting of\\nexamples and corresponding labels. In the ca se of an email spam classifier, for exam-\\nple, the training dataset consists of emails and their “spam” and “not spam” labels that\\na human identified. Then the trained model can be used on new observations (i.e.,\\nnew emails) to predict their unknown labe l (“spam” or “not spam”). Of course, we\\nalso want to add a model ev aluation between the traini ng and inference stages to\\nArtiﬁcial intelligence (AI)\\nMachine learning\\nDeep learning\\nDeep learning is machine\\nlearning with neural networks\\nthat have many layers.\\nFigure A.2 Deep learning is a \\nsubcategory of machine learning \\nfocused on implementing deep neural \\nnetworks. Machine learning is a \\nsubcategory of AI that is concerned \\nwith algorithms that learn from data. AI \\nis the broader concept of machines \\nbeing able to perform tasks that \\ntypically require human intelligence.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 275, 'page_label': '254'}, page_content='254 APPENDIX A Introduction to PyTorch\\nensure that the model satisfies our performance criteria before using it in a real-world\\napplication.\\n If we train LLMs to classify texts, the wo rkflow for training and using LLMs is simi-\\nlar to that depicted in figure A.3. If we are interested in training LLMs to generate\\ntexts, which is our main focus, figure A.3 st ill applies. In this case, the labels during\\npretraining can be derived from the text it self (the next-word pr ediction task intro-\\nduced in chapter 1). The LLM will generate entirely new text (instead of predicting\\nlabels), given an input prompt during inference.\\nA.1.3 Installing PyTorch\\nPyTorch can be installed just like any other Python library or package. However, since\\nPyTorch is a comprehensive library featur ing CPU- and GPU-compatible codes, the\\ninstallation may require additional explanation.\\nPython version \\nMany scientific computing libraries do not immediately support the newest version of\\nPython. Therefore, when installing PyTorch, it’s advisable to use a version of Python\\nthat is one or two releases older. For instance, if the latest version of Python is 3.13,\\nusing Python 3.11 or 3.12 is recommended.\\nTraining dataset\\nModel and\\nlearning algorithm\\nTrained model\\nExamples\\nLabels\\nNew observations Predicted labels\\nINFERENCE\\nTRAINING\\nIn supervised learning,\\nwe train a model on a\\nlabeled dataset.\\nOnce a model is trained,\\nwe can use it to predict\\nthe labels of new data.\\nFigure A.3 The supervised learning workflow for predictive modeling \\nconsists of a training stage where a model is trained on labeled examples \\nin a training dataset. The trained model can then be used to predict the \\nlabels of new observations.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 276, 'page_label': '255'}, page_content='255A.1 What is PyTorch?\\nFor instance, there are two versions of PyTorch: a leaner version that only supports CPU\\ncomputing and a full version that supports both CPU an d GPU computing. If your\\nmachine has a CUDA-compatible GPU that can be used for deep learning (ideally, an\\nNVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version. Regard-\\nless, the default command for installing PyTorch in a code terminal is:\\npip install torch\\nSuppose your computer supports a CUDA-com patible GPU. In that case, it will auto-\\nmatically install the PyTorch version that  supports GPU acceleration via CUDA,\\nassuming the Python environment you’re working on has the necessary dependencies\\n(like pip) installed.\\nNOTE As of this writing, PyTorch has also added experimental support for\\nAMD GPUs via ROCm. See https:/ /pytorch.org for additional instructions. \\nTo explicitly install the CUDA-compatible version of PyTorch, it’s often better to spec-\\nify the CUDA you want PyTorch to be compatible with. PyTorch’s official website\\n(https:/ /pytorch.org) provides the commands to inst all PyTorch with CUDA support\\nfor different operating syste ms. Figure A.4 shows a command  that will also install\\nPyTorch, as well as the torchvision and torchaudio libraries, which are optional for\\nthis book.\\nSelect the latest stable version.\\nSelect a CUDA version that is compatible\\nwith your graphics card.\\nIf you don’t have an Nvidia graphics card that supports\\nCUDA, select the CPU version.\\nFigure A.4 Access the PyTorch installation recommendation on https:/ /pytorch.org to customize and select the \\ninstallation command for your system.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 277, 'page_label': '256'}, page_content=\"256 APPENDIX A Introduction to PyTorch\\nI use PyTorch 2.4.0 for the examples, so I recommend that you use the following com-\\nmand to install the exact version to guarantee compatibility with this book:\\npip install torch==2.4.0\\nHowever, as mentioned earlier, given your  operating system, the installation com-\\nmand might differ slightly from the one shown here. Thus, I recommend that you\\nvisit https:/ /pytorch.org and use the installation menu (see figure A.4) to select the\\ninstallation command for your operating system. Remember to replace torch with\\ntorch==2.4.0 in the command.\\n To check the version of PyTorch, execute the following code in PyTorch:\\nimport torch\\ntorch.__version__\\nThis prints\\n'2.4.0'\\nIf you are looking for additional recomm endations and instructions for setting up\\nyour Python environment or installing the other libraries used in this book, visit\\nthe supplementary GitHub re pository of this book at https:/ /github.com/rasbt/\\nLLMs-from-scratch.\\n After installing PyTorch, you can check wh ether your installation recognizes your\\nbuilt-in NVIDIA GPU by running the following code in Python:\\nimport torch\\ntorch.cuda.is_available()\\nThis returns\\nTrue\\nIf the command returns True, you are all set. If the command returns False, your\\ncomputer may not have a co mpatible GPU, or PyTorch does not recognize it. While\\nGPUs are not required for the initial ch apters in this book, which are focused on\\nimplementing LLMs for educational purpose s, they can significantly speed up deep\\nlearning–related computations.\\nPyTorch and Torch \\nThe Python library is named PyTorch primarily because it’s a continuation of the Torch\\nlibrary but adapted for Python (hence, “PyTorch”). “Torch” acknowledges the library’s\\nroots in Torch, a scientific computing framework with wide support for machine learn-\\ning algorithms, which was initially created using the Lua programming language.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 278, 'page_label': '257'}, page_content='257A.1 What is PyTorch?\\n If you don’t have access to a GPU, th ere are several cloud computing providers\\nwhere users can run GPU com putations against an hourly cost. A popular Jupyter\\nnotebook–like environmen t is Google Colab ( https:/ /colab.research.google.com),\\nwhich provides time-limited access to GPUs  as of this writing. Using the Runtime\\nmenu, it is possible to select a GPU, as shown in the screenshot in figure A.5.\\nPyTorch on Apple Silicon \\nIf you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer\\nmodels), you can use its capabilities to a ccelerate PyTorch code execution. To use\\nyour Apple Silicon chip for PyTorch, you first need to install PyTorch as you normally\\nwould. Then, to check whether your Mac supports PyTorch acceleration with its Apple\\nSilicon chip, you can run a simple code snippet in Python:\\nprint(torch.backends.mps.is_available())\\nIf it returns True, it means that your Mac has an Apple Silicon chip that can be used\\nto accelerate PyTorch code.\\nExercise A.1\\nInstall and set up PyTorch on your computer\\nExercise A.2\\nRun the supplementary code at https:/ /mng.bz/o05v that checks whether your envi-\\nronment is set up correctly. \\nSelect GPU instead of TPU or CPU\\nIf an A100 GPU is not available,\\nit’s ok to choose a different GPU.\\nAccess this menu by clicking Change\\nruntime type in the Runtime tab.\\nFigure A.5 Select a GPU device for Google Colab under the Runtime/Change Runtime Type menu.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 279, 'page_label': '258'}, page_content='258 APPENDIX A Introduction to PyTorch\\nA.2 Understanding tensors\\nTensors represent a mathematical concept that generalizes vectors and matrices to\\npotentially higher dimensions. In other wo rds, tensors are math ematical objects that\\ncan be characterized by their order (or ra nk), which provides the number of dimen-\\nsions. For example, a scalar (just a number) is a tensor of rank 0, a vector is a tensor of\\nrank 1, and a matrix is a tensor of rank 2, as illustrated in figure A.6.\\nFrom a computational perspective, tensors serve as data containers. For instance, they\\nhold multidimensional data, where each dimension represents a different feature.\\nTensor libraries like PyTorch can create, manipulate, and compute with these arrays\\nefficiently. In this context, a tensor library functions as an array library. \\n PyTorch tensors are similar to NumPy arra ys but have several additional features\\nthat are important for deep learning. For example, PyTorch adds an automatic differ-\\nentiation engine, simplifying computing gradients  (see section A.4). PyTorch tensors\\nalso support GPU computations to speed up deep neural network training (see sec-\\ntion A.8).A.2.1 Scalars, vectors, matrices, and tensors\\nAs mentioned earlier, PyTorch tensors are da ta containers for array-like structures. A\\nscalar is a zero-dimensional tensor (for in stance, just a number), a vector is a one-\\ndimensional tensor, and a matr ix is a two-dimensional tensor. There is no specific\\nterm for higher-dimensional tensors, so we  typically refer to a three-dimensional ten-\\nsor as just a 3D tensor, and so forth.  We can create objects of PyTorch’s Tensor class\\nusing the torch.tensor function as shown in the following listing.\\nPyTorch with a NumPy-like API \\nPyTorch adopts most of the NumPy array API and syntax for its tensor operations. If\\nyou are new to NumPy, you can get a brief overview of the most relevant concepts via\\nmy article “Scientific Computing in Python: Introduction to NumPy and Matplotlib” at\\nhttps:/ /sebastianraschka.com/blog/2020/numpy-intro.html. \\n2\\nScalar\\n3512\\n1723\\n3349\\n3\\n1\\n3\\nVector Matrix\\n0D tensor 1D tensor 2D tensor\\nA scalar is just a\\nsingle number.\\nAn example of a 3D\\nvector that consists\\nof 3 entries\\nA matrix with 3 rows\\nand 4 columns\\nFigure A.6 Tensors with different \\nranks. Here 0D corresponds to \\nrank 0, 1D to rank 1, and 2D to \\nrank 2. A three-dimensional \\nvector, which consists of three \\nelements, is still a rank 1 tensor.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 280, 'page_label': '259'}, page_content='259A.2 Understanding tensors\\nimport torch\\ntensor0d = torch.tensor(1)    \\ntensor1d = torch.tensor([1, 2, 3])   \\ntensor2d = torch.tensor([[1, 2], \\n                         [3, 4]])    \\ntensor3d = torch.tensor([[[1, 2], [3, 4]], \\n                         [[5, 6], [7, 8]]])   \\nA.2.2 Tensor data types\\nPyTorch adopts the default 64-bit integer data type from Python. We can access the\\ndata type of a tensor via the .dtype attribute of a tensor:\\ntensor1d = torch.tensor([1, 2, 3])\\nprint(tensor1d.dtype)\\nThis prints\\ntorch.int64\\nIf we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision\\nby default:\\nfloatvec = torch.tensor([1.0, 2.0, 3.0])\\nprint(floatvec.dtype)\\nThe output is\\ntorch.float32\\nThis choice is primarily due to the balanc e between precision and computational effi-\\nciency. A 32-bit floating-point number offers sufficient precision for most deep learning\\ntasks while consuming less memory and computational resources than a 64-bit floating-\\npoint number. Moreover, GPU architectures are optimized for 32-bit computations, and\\nusing this data type can significantly speed up model training and inference.\\n Moreover, it is possible to change the precision using a tensor’s .to method. The\\nfollowing code demonstrates this by chan ging a 64-bit integer tensor into a 32-bit\\nfloat tensor:\\nfloatvec = tensor1d.to(torch.float32)\\nprint(floatvec.dtype)\\nThis returns\\ntorch.float32\\nListing A.1 Creating PyTorch tensors \\nCreates a zero-dimensional tensor \\n(scalar) from a Python integer\\nCreates a one-dimensional tensor \\n(vector) from a Python list\\nCreates a two-dimensional tensor \\nfrom a nested Python list\\nCreates a three-dimensional \\ntensor from a nested Python list\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 281, 'page_label': '260'}, page_content='260 APPENDIX A Introduction to PyTorch\\nFor more information about different tensor  data types available in PyTorch, check\\nthe official documentation at https:/ /pytorch.org/docs/stable/tensors.html.\\nA.2.3 Common PyTorch tensor operations\\nComprehensive coverage of all the differe nt PyTorch tensor operations and com-\\nmands is outside the scope of this book. However, I will briefly describe relevant oper-\\nations as we introduce them throughout the book.\\n We have already introduced the torch.tensor() function to create new tensors: \\ntensor2d = torch.tensor([[1, 2, 3], \\n                         [4, 5, 6]])\\nprint(tensor2d)\\nThis prints\\ntensor([[1, 2, 3],\\n        [4, 5, 6]])\\nIn addition, the .shape attribute allows us to access the shape of a tensor:\\nprint(tensor2d.shape)\\nThe output is\\ntorch.Size([2, 3])\\nAs you can see, .shape returns [2, 3], meaning the tensor has two rows and three col-\\numns. To reshape the tensor into a 3 × 2 tensor, we can use the .reshape method:\\nprint(tensor2d.reshape(3, 2))\\nThis prints\\ntensor([[1, 2],\\n        [3, 4],\\n        [5, 6]])\\nHowever, note that the mo re common command for reshaping tensors in PyTorch is\\n.view():\\nprint(tensor2d.view(3, 2))\\nThe output is\\ntensor([[1, 2],\\n        [3, 4],\\n        [5, 6]])\\nSimilar to .reshape and .view, in several cases, PyTorch offers multiple syntax options\\nfor executing the same computation. PyTo rch initially followed the original Lua\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 282, 'page_label': '261'}, page_content='261A.3 Seeing models as computation graphs\\nTorch syntax convention but then, by popular request, added syntax to make it similar\\nto NumPy. (The subtle difference between .view() and .reshape() in PyTorch lies in\\ntheir handling of memory layout: .view() requires the original data to be contiguous\\nand will fail if it isn’t, whereas .reshape() will work regardless, copying the data if nec-\\nessary to ensure the desired shape.)\\n Next, we can use .T to transpose a tensor, which means flipping it across its diago-\\nnal. Note that this is similar to reshaping a tensor, as you can see based on the follow-\\ning result:\\nprint(tensor2d.T)\\nThe output is\\ntensor([[1, 4],\\n        [2, 5],\\n        [3, 6]])\\nLastly, the common way to multiply two matrices in PyTorch is the .matmul method:\\nprint(tensor2d.matmul(tensor2d.T))\\nThe output is\\ntensor([[14, 32],\\n        [32, 77]])\\nHowever, we can also adopt the @ operator, which accomplishes the same thing more\\ncompactly:\\nprint(tensor2d @ tensor2d.T)\\nThis prints\\ntensor([[14, 32],\\n        [32, 77]])\\nAs mentioned earlier, I introduce addition al operations when needed. For readers\\nwho’d like to browse through all the different tensor operations available in PyTorch\\n(we won’t need most of these), I recommend checking out the official documentation\\nat https:/ /pytorch.org/docs/stable/tensors.html.\\nA.3 Seeing models as computation graphs\\nNow let’s look at PyTorch’s automatic differentiation engine, also known as autograd.\\nPyTorch’s autograd system provides functions to comput e gradients in dynamic com-\\nputational graphs automatically. \\n A computational graph is a directed graph that allows us to express and visualize\\nmathematical expressions. In the context of deep learning, a computation graph lays\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 283, 'page_label': '262'}, page_content='262 APPENDIX A Introduction to PyTorch\\nout the sequence of calculations needed to compute the output of a neural network—\\nwe will need this to compute the requir ed gradients for back propagation, the main\\ntraining algorithm for neural networks.\\n Let’s look at a concrete example to illustrate the concept of a computation graph.\\nThe code in the following listing implements the forward pass (prediction step) of a\\nsimple logistic regression classifier, which can be seen as a single-layer neural network.\\nIt returns a score between 0 and 1, which is compared to the true class label (0 or 1)\\nwhen computing the loss.\\nimport torch.nn.functional as F    \\ny = torch.tensor([1.0])         \\nx1 = torch.tensor([1.1])   \\nw1 = torch.tensor([2.2])   \\nb = torch.tensor([0.0])           \\nz = x1 * w1 + b                \\na = torch.sigmoid(z)              \\nloss = F.binary_cross_entropy(a, y)\\nIf not all components in the preceding code make sense to you, don’t worry. The\\npoint of this example is not to implement a logistic regression classifier but rather to\\nillustrate how we can think of a sequence of computations as a computation graph, as\\nshown in figure A.7. \\nIn fact, PyTorch builds such a computation graph in the background, and we can use\\nthis to calculate gradients of a loss func tion with respect to the model parameters\\n(here w1 and b) to train the model.\\nListing A.2 A logistic regression forward pass\\nThis import statement is a common convention \\nin PyTorch to prevent long lines of code.\\nTrue label\\nInput feature\\nWeight parameter\\nBias unit\\nNet input\\nActivation and output\\nAn intermediate result in\\nthe computation graphThe input data\\nA trainable weight\\nparameter A trainable bias unit The target label\\nFigure A.7 A logistic regression forward pass as a computation graph. The input feature \\nx1 is multiplied by a model weight w1 and passed through an activation function σ after \\nadding the bias. The loss is computed by comparing the model output a with a given label y.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 284, 'page_label': '263'}, page_content='263A.4 Automatic differentiation made easy\\nA.4 Automatic differentiation made easy\\nIf we carry out computations in PyTorch, it will build a computational graph internally\\nby default if one of its terminal nodes has the requires_grad attribute set to True.\\nThis is useful if we want to compute gradients. Gradients are required when training\\nneural networks via the popular backpropagation algorithm, which can be considered\\nan implementation of the chain rule from calculus for neural networks, illustrated in\\nfigure A.8.\\nPARTIAL DERIVATIVES AND GRADIENTS \\nFigure A.8 shows partial derivatives, wh ich measure the rate at which a function\\nchanges with respect to one of its variables. A gradient is a vector containing all of the\\npartial derivatives of a multiv ariate function, a function with more than one variable\\nas input.\\n If you are not familiar with or don’t remember the partial derivatives, gradients, or\\nchain rule from calculus, don’t worry. On a high level, all you need to know for this book\\nis that the chain rule is a way to compute gradients of a loss function given the model’s\\nparameters in a computation graph. This provides the information needed to update\\neach parameter to minimize the loss function, which serves as a proxy for measuring the\\nThe partial derivative of the\\nloss with respect to its input\\nThe partial derivative of\\nthe intermediate result z\\nwith respect to the bias unit\\nWe can obtain the partial derivative of\\nthe loss with respect to the trainable\\nweight by chaining the individual partial\\nderivative in the graph.\\nSimilar to above, we can compute the\\npartial derivative of the trainable\\nderivative by applying the chain rule.\\nFigure A.8 The most common way of computing the loss gradients in a \\ncomputation graph involves applying the chain rule from right to left, also called \\nreverse-model automatic differentiation or backpropagation. We start from the \\noutput layer (or the loss itself) and work backward through the network to the input \\nlayer. We do this to compute the gradient of the loss with respect to each parameter \\n(weights and biases) in the network, which informs how we update these \\nparameters during training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 285, 'page_label': '264'}, page_content='264 APPENDIX A Introduction to PyTorch\\nmodel’s performance using a method such as gradient descent. We will revisit the com-\\nputational implementation of this training loop in PyTorch in section A.7.\\n How is this all related to the automatic differentiation (autograd) engine, the sec-\\nond component of the PyTorch library mentioned earlier? PyTorch’s autograd engine\\nconstructs a computational graph in the background by tracking every operation per-\\nformed on tensors. Then, calling the grad function, we can compute the gradient of the\\nloss concerning the model parameter w1, as shown in the following listing.\\nimport torch.nn.functional as F\\nfrom torch.autograd import grad\\ny = torch.tensor([1.0])\\nx1 = torch.tensor([1.1])\\nw1 = torch.tensor([2.2], requires_grad=True)\\nb = torch.tensor([0.0], requires_grad=True)\\nz = x1 * w1 + b \\na = torch.sigmoid(z)\\nloss = F.binary_cross_entropy(a, y)\\ngrad_L_w1 = grad(loss, w1, retain_graph=True)  \\ngrad_L_b = grad(loss, b, retain_graph=True)\\nThe resulting values of the loss given the model’s parameters are\\nprint(grad_L_w1)\\nprint(grad_L_b)\\nThis prints\\n(tensor([-0.0898]),)\\n(tensor([-0.0817]),)\\nHere, we have been using the grad function manually, which can be useful for experi-\\nmentation, debugging, and demonstrating concepts. But, in pr actice, PyTorch pro-\\nvides even more high-level tools to automa te this process. For instance, we can call\\n.backward on the loss, and PyTorch will compute the gradients of all the leaf nodes in\\nthe graph, which will be stored via the tensors’ .grad attributes:\\nloss.backward()\\nprint(w1.grad)\\nprint(b.grad)\\nThe outputs are\\n(tensor([-0.0898]),)\\n(tensor([-0.0817]),)\\nListing A.3 Computing gradients via autograd\\nBy default, PyTorch destroys \\nthe computation graph after \\ncalculating the gradients to \\nfree memory. However, since \\nwe will reuse this \\ncomputation graph shortly, \\nwe set retain_graph=True \\nso that it stays in memory.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 286, 'page_label': '265'}, page_content='265A.5 Implementing multilayer neural networks\\nI’ve provided you with a lot of informatio n, and you may be overwhelmed by the cal-\\nculus concepts, but don’t worr y. While this calculus jargon is a means to explain\\nPyTorch’s autograd component, all you need to take away is that PyTorch takes care of\\nthe calculus for us via the .backward method—we won’t need to compute any deriva-\\ntives or gradients by hand.\\nA.5 Implementing multilayer neural networks\\nNext, we focus on PyTorch as a library fo r implementing deep neural networks. To\\nprovide a concrete example, let’s look at  a multilayer percep tron, a fully connected\\nneural network, as illustrated in figure A.9.\\nWhen implementing a neural network in PyTorch, we can subclass the torch.nn.Module\\nclass to define our own custom network architecture. This Module base class provides a\\nlot of functionality, making it easier to build and train models. For instance, it allows us to\\nencapsulate layers and operations and keep track of the model’s parameters. \\n Within this subclass, we define the network layers in the __init__ constructor and\\nspecify how the layers interact in the fo rward method. The forward method describes\\nhow the input data passes through the netw ork and comes together as a computation\\ngraph. In contrast, the backward method, which we typically do not need to imple-\\nment ourselves, is used during training to compute gradients of the loss function given\\nthe model parameters (see section A.7). The code in the following listing implements a\\nInput layer\\n1st hidden layer\\n2nd hidden layer\\nOutput layer\\nThis network has\\n10 input units.\\nThe 1st hidden layer has\\nsix nodes and one bias unit.\\nThe 2nd hidden layer has\\nfour nodes and a node\\nrepresenting the bias units.\\nThere are three output units.\\nThe edges represent\\nweight connections.This node represents the\\nbias unit in this layer.\\nFigure A.9 A multilayer perceptron with two hidden layers. Each node represents \\na unit in the respective layer. For illustration purposes, each layer has a very small \\nnumber of nodes.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 287, 'page_label': '266'}, page_content='266 APPENDIX A Introduction to PyTorch\\nclassic multilayer perceptron with two hidden layers to illustrate a typical usage of the\\nModule class.\\nclass NeuralNetwork(torch.nn.Module):\\n    def __init__(self, num_inputs, num_outputs):   \\n        super().__init__()\\n        self.layers = torch.nn.Sequential(\\n                \\n            # 1st hidden layer\\n            torch.nn.Linear(num_inputs, 30),   \\n            torch.nn.ReLU(),              \\n            # 2nd hidden layer\\n            torch.nn.Linear(30, 20),   \\n            torch.nn.ReLU(),\\n            # output layer\\n            torch.nn.Linear(20, num_outputs),\\n        )\\n    def forward(self, x):\\n        logits = self.layers(x)\\n        return logits          \\nWe can then instantiate a new neural network object as follows:\\nmodel = NeuralNetwork(50, 3)\\nBefore using this new model object, we can call print on the model to see a summary\\nof its structure:\\nprint(model)\\nThis prints\\nNeuralNetwork(\\n  (layers): Sequential(\\n    (0): Linear(in_features=50, out_features=30, bias=True)\\n    (1): ReLU()\\n    (2): Linear(in_features=30, out_features=20, bias=True)\\n    (3): ReLU()\\n    (4): Linear(in_features=20, out_features=3, bias=True)\\n  )\\n)\\nNote that we use the Sequential class when we implement the NeuralNetwork class.\\nSequential is not required, but it can make our li fe easier if we have a series of lay-\\ners we want to execute in a specific order, as  is the case here. This way, after instanti-\\nating self.layers = Sequential(...) in the __init__ constructor, we just have to\\nListing A.4 A multilayer perceptron with two hidden layers \\nCoding the number of \\ninputs and outputs as \\nvariables allows us to reuse \\nthe same code for datasets \\nwith different numbers of \\nfeatures and classes\\nThe Linear layer takes the \\nnumber of input and output \\nnodes as arguments.\\nNonlinear activation functions are \\nplaced between the hidden layers.\\nThe number of output nodes of one \\nhidden layer has to match the number \\nof inputs of the next layer.\\nThe outputs of the last \\nlayer are called logits.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 288, 'page_label': '267'}, page_content='267A.5 Implementing multilayer neural networks\\ncall the self.layers instead of calling each layer individually in the NeuralNetwork’s\\nforward method.\\n Next, let’s check the total number of trainable parameters of this model:\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis prints\\nTotal number of trainable model parameters: 2213\\nEach parameter for which requires_grad=True counts as a trainable parameter and\\nwill be updated during training (see section A.7).\\n In the case of our neural  network model with the preceding two hidden layers,\\nthese trainable parameters are contained in the torch.nn.Linear layers. A Linear\\nlayer multiplies the inputs with a weight matr ix and adds a bias ve ctor. This is some-\\ntimes referred to as a feedforward or fully connected layer. \\n Based on the print(model) call we executed here, we can see that the first Linear\\nlayer is at index position 0 in the layers attribute. We can access the corresponding\\nweight parameter matrix as follows:\\nprint(model.layers[0].weight)\\nThis prints\\nParameter containing:\\ntensor([[ 0.1174, -0.1350, -0.1227,  ...,  0.0275, -0.0520, -0.0192],\\n        [-0.0169,  0.1265,  0.0255,  ..., -0.1247,  0.1191, -0.0698],\\n        [-0.0973, -0.0974, -0.0739,  ..., -0.0068, -0.0892,  0.1070],\\n        ...,\\n        [-0.0681,  0.1058, -0.0315,  ..., -0.1081, -0.0290, -0.1374],\\n        [-0.0159,  0.0587, -0.0916,  ..., -0.1153,  0.0700,  0.0770],\\n        [-0.1019,  0.1345, -0.0176,  ...,  0.0114, -0.0559, -0.0088]],\\n       requires_grad=True)\\nSince this large matrix is not shown in its entirety, let’s use the .shape attribute to\\nshow its dimensions:\\nprint(model.layers[0].weight.shape)\\nThe result is\\ntorch.Size([30, 50])\\n(Similarly, you could access the bias vector via model.layers[0].bias.)\\n The weight matrix here is a 30 × 50 matrix, and we can see that requires_grad is\\nset to True, which means its entries are trainabl e—this is the default setting for\\nweights and biases in torch.nn.Linear. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 289, 'page_label': '268'}, page_content='268 APPENDIX A Introduction to PyTorch\\n If you execute the preceding code on your computer, the numbers in the weight\\nmatrix will likely differ from those shown. The model weights are initialized with small\\nrandom numbers, which differ each time we  instantiate the network. In deep learn-\\ning, initializing model weights with small random numbers is desired to break symme-\\ntry during training. Otherwise, the nodes would be performing the same operations\\nand updates during backpropagation, whic h would not allow the network to learn\\ncomplex mappings from inputs to outputs.\\n However, while we want to keep using sm all random numbers as initial values for\\nour layer weights, we can make the random  number initialization reproducible by\\nseeding PyTorch’s random number generator via manual_seed:\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(50, 3)\\nprint(model.layers[0].weight)\\nThe result is\\nParameter containing:\\ntensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\\n        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\\n        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\\n        ...,\\n        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\\n        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\\n        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\\n       requires_grad=True)\\nNow that we have spent some time inspecting the NeuralNetwork instance, let’s briefly\\nsee how it’s used via the forward pass:\\ntorch.manual_seed(123)\\nX = torch.rand((1, 50))\\nout = model(X)\\nprint(out)\\nThe result is\\ntensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\\nIn the preceding code, we generated a single random training example X as a toy\\ninput (note that our network expects 50-dimensional feature vectors) and fed it to the\\nmodel, returning three scores. When we call model(x), it will automatically execute\\nthe forward pass of the model. \\n The forward pass refers to calculating output tensors from input tensors. This\\ninvolves passing the input data through al l the neural network layers, starting from\\nthe input layer, through hidden layers, and finally to the output layer.\\n These three numbers returned here corresp ond to a score assigned to each of the\\nthree output nodes. Notice that the output tensor also includes a grad_fn value.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 290, 'page_label': '269'}, page_content='269A.5 Implementing multilayer neural networks\\n Here, grad_fn=<AddmmBackward0> represents the last-use d function to compute a\\nvariable in the computational graph. In particular, grad_fn=<AddmmBackward0> means\\nthat the tensor we are inspecting was crea ted via a matrix multiplication and addition\\noperation. PyTorch will use this information when it computes gradients during back-\\npropagation. The <AddmmBackward0> part of grad_fn=<AddmmBackward0> specifies the\\noperation performed. In this case, it is an Addmm operation. Addmm stands for matrix\\nmultiplication (mm) followed by an addition (Add).\\n If we just want to use a network with out training or back propagation—for exam-\\nple, if we use it for prediction after tr aining—constructing this computational graph\\nfor backpropagation can be wasteful as it performs unnecessary computations and con-\\nsumes additional memory. So, when we use a model for inference (for instance, making\\npredictions) rather than training, the best practice is to use the torch.no_grad() con-\\ntext manager. This tells PyTorch that it do esn’t need to keep track of the gradients,\\nwhich can result in significant savings in memory and computation:\\nwith torch.no_grad():\\n    out = model(X)\\nprint(out)\\nThe result is\\ntensor([[-0.1262,  0.1080, -0.1792]])\\nIn PyTorch, it’s common practice to code models such that they return the outputs of\\nthe last layer (logits) withou t passing them to a nonlinear activation function. That’s\\nbecause PyTorch’s commonly used loss functions combine the softmax (or sigmoid\\nfor binary classification) operation with the negative log-likelihood loss in a single\\nclass. The reason for this is numerical effi ciency and stability. So, if we want to com-\\npute class-membership probabilities for our predictions, we have to call the softmax\\nfunction explicitly:\\nwith torch.no_grad():\\n    out = torch.softmax(model(X), dim=1)\\nprint(out)\\nThis prints\\ntensor([[0.3113, 0.3934, 0.2952]]))\\nThe values can now be interpreted as class-membership probabilities that sum up to 1.\\nThe values are roughly equal for this random input, which is expected for a randomly\\ninitialized model without training. \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 291, 'page_label': '270'}, page_content='270 APPENDIX A Introduction to PyTorch\\nA.6 Setting up efficient data loaders\\nBefore we can train our model, we have to briefly discuss creating efficient data load-\\ners in PyTorch, which we will iterate over  during training. The overall idea behind\\ndata loading in PyTorch is illustrated in figure A.10.\\nFollowing figure A.10, we will implement a custom Dataset class, which we will use to\\ncreate a training and a test dataset that we’ll then use to create the data loaders. Let’s\\nstart by creating a simple toy dataset of five  training examples with two features each.\\nAccompanying the training examples, we al so create a tensor containing the corre-\\nsponding class labels: three examples belong  to class 0, and two examples belong to\\nclass 1. In addition, we make a test set consisting of two entries. The code to create this\\ndataset is shown in the following listing.\\nX_train = torch.tensor([\\n    [-1.2, 3.1],\\n    [-0.9, 2.9],\\n    [-0.5, 2.6],\\n    [2.3, -1.1],\\n    [2.7, -1.5]\\n])\\ny_train = torch.tensor([0, 0, 0, 1, 1])\\nX_test = torch.tensor([\\n    [-0.8, 2.8],\\n    [2.6, -1.6],\\n])\\ny_test = torch.tensor([0, 1])\\nListing A.5 Creating a small toy dataset \\nCustom\\nDatasetclass\\nTraining dataset\\nTest dataset\\nTraining dataloader\\nTest dataloader\\nDataLoaderclass\\nWe create a custom\\nclass that deﬁnes\\nhow individual data\\nrecords are loaded.\\nUsing the Dataset\\nclass, we create\\ndifferent Dataset\\nobjects. Each Dataset object is\\nfed to a data loader.\\nEach DataLoader\\nobject handles\\ndataset shufﬂing,\\nassembling the\\ndata records into\\nbatches, and more\\nInstantiateInstantiate\\nFigure A.10 PyTorch implements a Dataset and a DataLoader class. The Dataset class is used to \\ninstantiate objects that define how each data record is loaded. The DataLoader handles how the data is shuffled \\nand assembled into batches.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 292, 'page_label': '271'}, page_content='271A.6 Setting up efficient data loaders\\nNOTE PyTorch requires that class labels start with label 0, and the largest\\nclass label value should not exceed the number of outp ut nodes minus 1\\n(since Python index counting starts at zero). So, if we have class labels 0, 1, 2,\\n3, and 4, the neural network output layer should consist of five nodes.\\nNext, we create a custom dataset class, \\nToyDataset, by subclassing from PyTorch’s\\nDataset parent class, as shown in the following listing.\\nfrom torch.utils.data import Dataset\\nclass ToyDataset(Dataset):\\n    def __init__(self, X, y):\\n        self.features = X\\n        self.labels = y\\n    def __getitem__(self, index):       \\n        one_x = self.features[index]    \\n        one_y = self.labels[index]      \\n        return one_x, one_y             \\n    def __len__(self):\\n        return self.labels.shape[0]     \\ntrain_ds = ToyDataset(X_train, y_train)\\ntest_ds = ToyDataset(X_test, y_test)\\nThe purpose of this custom ToyDataset class is to instantiate a PyTorch DataLoader.\\nBut before we get to this step, let’s brie fly go over the general structure of the\\nToyDataset code. \\n I n  P y T o r c h ,  t h e  t h r e e  m a i n  c o m p o n e n t s  o f  a  c u s t o m  Dataset class are the\\n__init__ constructor, the __getitem__ method, and the __len__ method (see list-\\ning A.6). In the __init__ method, we set up attributes th at we can access later in the\\n__getitem__ and __len__ methods. These could be file paths, file objects, database\\nconnectors, and so on. Since we created a tensor dataset that sits in memory, we\\nsimply assign \\nX and y to these attributes, which ar e placeholders for our tensor\\nobjects. \\n In the __getitem__ method, we define instructions for returning exactly one item\\nfrom the dataset via an index. This refers to the features and the class label corre-\\nsponding to a single training example or te st instance. (The data loader will provide\\nthis index, which we will cover shortly.)\\n Finally, the __len__ method contains instructions fo r retrieving the length of the\\ndataset. Here, we use the .shape attribute of a tensor to return the number of rows in\\nthe feature array. In the case of the training  dataset, we have five rows, which we can\\ndouble-check:\\nprint(len(train_ds))\\nListing A.6 Defining a custom Dataset class \\nInstructions for retrieving \\nexactly one data record and \\nthe corresponding label\\nInstructions for \\nreturning the total \\nlength of the dataset\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 293, 'page_label': '272'}, page_content='272 APPENDIX A Introduction to PyTorch\\nThe result is\\n5\\nNow that we’ve defined a PyTorch Dataset class we can use for our toy dataset, we can\\nuse PyTorch’s DataLoader class to sample from it, as shown in the following listing.\\nfrom torch.utils.data import DataLoader\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_ds,    \\n    batch_size=2,\\n    shuffle=True,         \\n    num_workers=0    \\n)\\ntest_loader = DataLoader(\\n    dataset=test_ds,\\n    batch_size=2,\\n    shuffle=False,    \\n    num_workers=0\\n)\\nAfter instantiating the training data loader , we can iterate over it. The iteration over\\nthe test_loader works similarly but is omitted for brevity:\\nfor idx, (x, y) in enumerate(train_loader):\\n    print(f\"Batch {idx+1}:\", x, y)\\nThe result is\\nBatch 1: tensor([[-1.2000,  3.1000],\\n                 [-0.5000,  2.6000]]) tensor([0, 0])\\nBatch 2: tensor([[ 2.3000, -1.1000],\\n                 [-0.9000,  2.9000]]) tensor([1, 0])\\nBatch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\\nAs we can see based on the preceding output, the train_loader iterates over the train-\\ning dataset, visiting each training example exactly once. This is known as a training\\nepoch. Since we seeded the random number generator using torch.manual_seed(123)\\nhere, you should get the exact same shufflin g order of training examples. However, if\\nyou iterate over the dataset a second time, you will see that the shuffling order will\\nchange. This is desired to prevent deep neural networks from getting caught in repet-\\nitive update cycles during training.\\n We specified a batch size of 2 here, but the third batch only contains a single exam-\\nple. That’s because we have five training examples, and 5 is not evenly divisible by 2.\\nListing A.7 Instantiating data loaders \\nThe ToyDataset instance \\ncreated earlier serves as \\ninput to the data loader.\\nWhether or not to \\nshuffle the data\\nThe number of \\nbackground processes\\nIt is not necessary to \\nshuffle a test dataset.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 294, 'page_label': '273'}, page_content='273A.6 Setting up efficient data loaders\\nIn practice, having a substantially smaller ba tch as the last batch in a training epoch\\ncan disturb the convergence during  training. To prevent this, set drop_last=True,\\nwhich will drop the last batch in each epoch, as shown in the following listing.\\ntrain_loader = DataLoader(\\n    dataset=train_ds,\\n    batch_size=2,\\n    shuffle=True,\\n    num_workers=0,\\n    drop_last=True\\n)\\nNow, iterating over the training loader, we can see that the last batch is omitted:\\nfor idx, (x, y) in enumerate(train_loader):\\n    print(f\"Batch {idx+1}:\", x, y)\\nThe result is\\nBatch 1: tensor([[-0.9000,  2.9000],\\n        [ 2.3000, -1.1000]]) tensor([0, 1])\\nBatch 2: tensor([[ 2.7000, -1.5000],\\n        [-0.5000,  2.6000]]) tensor([1, 0])\\nLastly, let’s discuss the setting num_workers=0 in the DataLoader. This parameter in\\nPyTorch’s DataLoader function is crucial for parall elizing data loading and prepro-\\ncessing. When num_workers is set to 0, the data loading will be done in the main pro-\\ncess and not in separate worker processes. This might seem unproblematic, but it can\\nlead to significant slowdowns during model training when we train larger networks on\\na GPU. Instead of focusing solely on the pr ocessing of the deep learning model, the\\nCPU must also take time to load and prep rocess the data. As a result, the GPU can sit\\nidle while waiting for the CPU to fini sh these tasks. In contrast, when num_workers is\\nset to a number greater than 0, multiple worker processes are launched to load data in\\nparallel, freeing the main process to focus on training your model and better utilizing\\nyour system’s resources (figure A.11).\\n However, if we are working wi th very small datasets, setting num_workers to 1 or\\nlarger may not be necessary since the total tr aining time takes only fractions of a sec-\\nond anyway. So, if you are working with tiny datasets or interactive environments such\\nas Jupyter notebooks, increasing num_workers may not provide any noticeable speedup.\\nIt may, in fact, lead to some problems. One potential problem is the overhead of spin-\\nning up multiple worker processes, which could take longer than the actual data load-\\ning when your dataset is small. \\n Furthermore, for Jupyter notebooks, setting num_workers to greater than 0 can\\nsometimes lead to problems related to the sharing of resources between different pro-\\ncesses, resulting in errors or notebook crashes. Therefore, it’s essential to understand\\nListing A.8 A training loader that drops the last batch\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 295, 'page_label': '274'}, page_content='274 APPENDIX A Introduction to PyTorch\\nthe tradeoff and make a calculated decision on setting the num_workers parameter.\\nWhen used correctly, it can be a beneficial tool but should be adapted to your specific\\ndataset size and computational environment for optimal results.\\n In my experience, setting num_workers=4 usually leads to optimal performance on\\nmany real-world datasets, but optimal settings depend on your hardware and the code\\nused for loading a training example defined in the Dataset class.\\nA.7 A typical training loop\\nLet’s now train a neural network on the toy dataset. The following listing shows the\\ntraining code.\\nimport torch.nn.functional as F\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)   \\noptimizer = torch.optim.SGD(\\n    model.parameters(), lr=0.5\\n)           \\nnum_epochs = 3\\nfor epoch in range(num_epochs): \\n    \\n    model.train()\\nListing A.9 Neural network training in PyTorch \\nModel training loop\\niteration\\nx, y\\nFor each epoch:\\nFor each batch:\\nLoad data\\nModel training loop\\niteration\\nx, y\\nFor each epoch:\\nFor each batch:\\nx, y\\nx, y\\nx, y\\nLoad data\\nA bottleneck\\nwhere the\\nmodel waits\\nfor the next\\nbatch to be\\nloaded\\nData loading multiple workerswithout\\nModel predicts the labels,\\nthe loss is computed, and the\\nmodel weights are updated.\\nContinue with\\nthe next batch\\nWith multiple workers\\nenabled, the data loader\\ncan prepare the next data\\nbatches in the background.\\nThe next batch is taken\\nfrom the loaded batches\\nthe data loader already\\nqueued up in the\\nbackground.\\nData loading multiple workerswith\\nFigure A.11 Loading data without multiple workers (setting num_workers=0) will create a data loading \\nbottleneck where the model sits idle until the next batch is loaded (left). If multiple workers are enabled, the data \\nloader can queue up the next batch in the background (right).\\nThe dataset has two \\nfeatures and two \\nclasses.\\nThe optimizer needs to \\nknow which parameters \\nto optimize.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 296, 'page_label': '275'}, page_content='275A.7 A typical training loop\\n    for batch_idx, (features, labels) in enumerate(train_loader):\\n        logits = model(features)\\n       \\n        loss = F.cross_entropy(logits, labels)\\n        \\n        optimizer.zero_grad()           \\n        loss.backward()        \\n        optimizer.step()       \\n    \\n        ### LOGGING\\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\\n              f\" | Train Loss: {loss:.2f}\")\\n    model.eval()\\n    # Insert optional model evaluation code\\nRunning this code yields the following outputs:\\nEpoch: 001/003 | Batch 000/002 | Train Loss: 0.75\\nEpoch: 001/003 | Batch 001/002 | Train Loss: 0.65\\nEpoch: 002/003 | Batch 000/002 | Train Loss: 0.44\\nEpoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13\\nEpoch: 003/003 | Batch 000/002 | Train Loss: 0.03\\nEpoch: 003/003 | Batch 001/002 | Train Loss: 0.00\\nAs we can see, the loss reaches 0 after thre e epochs, a sign that the model converged\\non the training set. Here, we initialize a model with two inputs and two outputs\\nbecause our toy dataset has two input features and two class labels to predict. We used\\na stochastic gradient descent (\\nSGD) optimizer with a learning rate ( lr) of 0.5. The\\nlearning rate is a hyperparameter, meaning it’s a tunable setting that we must experi-\\nment with based on observing the loss. Ideally, we want to choose a learning rate such\\nthat the loss converges after a certain numb er of epochs—the number of epochs is\\nanother hyperparameter to choose. \\nIn practice, we often use a third dataset, a so-called validation dataset, to find the opti-\\nmal hyperparameter settings. A validation da taset is similar to a test set. However,\\nwhile we only want to use a test set precisely once to avoid biasing the evaluation, we\\nusually use the validation set multiple times to tweak the model settings. \\n We also introduced new settings called model.train() and model.eval(). As these\\nnames imply, these settings are used to put the model into a training and an evalua-\\ntion mode. This is necessary for components  that behave differently during training\\nand inference, such as dropout or batch normalization layers. Since we don’t have dropout\\nExercise A.3\\nHow many parameters does the neural network introduced in listing A.9 have?\\nSets the gradients from the previous \\nround to 0 to prevent unintended \\ngradient accumulation\\nComputes \\nthe gradients \\nof the loss \\ngiven the \\nmodel \\nparameters\\nThe optimizer uses the gradients \\nto update the model parameters.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 297, 'page_label': '276'}, page_content='276 APPENDIX A Introduction to PyTorch\\nor other components in our NeuralNetwork class that are affected by these settings,\\nusing model.train() and model.eval() is redundant in our preceding code. How-\\never, it’s best practice to include them anyway to avoid unexpected behaviors when we\\nchange the model architecture or reuse the code to train a different model.\\n As discussed earlier, we pass the logits directly into the cross_entropy loss func-\\ntion, which will apply the softmax function internally for efficiency and numerical\\nstability reasons. Then, calling loss.backward() will calculate the gradients in the com-\\nputation graph that PyTorch constructed in the background. The optimizer.step()\\nmethod will use the gradients to update the model parameters to minimize the loss.\\nIn the case of the SGD optimizer, this means multiplying the gradients with the learn-\\ning rate and adding the scaled negative gradient to the parameters. \\nNOTE To prevent undesired gradient accumulation, it is important to include\\nan optimizer.zero_grad() call in each update round to reset the gradients to\\n0. Otherwise, the gradients will accumulate, which may be undesired.\\nAfter we have trained the model, we can use it to make predictions:\\nmodel.eval()\\nwith torch.no_grad():\\n    outputs = model(X_train)\\nprint(outputs)\\nThe results are \\ntensor([[ 2.8569, -4.1618],\\n        [ 2.5382, -3.7548],\\n        [ 2.0944, -3.1820],\\n        [-1.4814,  1.4816],\\n        [-1.7176,  1.7342]])\\nTo obtain the class membership probab ilities, we can then use PyTorch’s softmax\\nfunction:\\ntorch.set_printoptions(sci_mode=False)\\nprobas = torch.softmax(outputs, dim=1)\\nprint(probas)\\nThis outputs\\ntensor([[    0.9991,     0.0009],\\n        [    0.9982,     0.0018],\\n        [    0.9949,     0.0051],\\n        [    0.0491,     0.9509],\\n        [    0.0307,     0.9693]])\\nLet’s consider the first row in the precedin g code output. Here, the first value (col-\\numn) means that the training example has a 99.91% probability of belonging to class\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 298, 'page_label': '277'}, page_content='277A.7 A typical training loop\\n0 and a 0.09% probability of belonging to class 1. (The set_printoptions call is used\\nhere to make the outputs more legible.)\\n We can convert these values into class label predictions using PyTorch’s argmax\\nfunction, which returns the index position of  the highest value in each row if we set\\ndim=1 (setting dim=0 would return the highest value in each column instead):\\npredictions = torch.argmax(probas, dim=1)\\nprint(predictions)\\nThis prints\\ntensor([0, 0, 0, 1, 1])\\nNote that it is unnecessary to compute softmax probabilities to obtain the class labels.\\nWe could also apply the argmax function to the logits (outputs) directly:\\npredictions = torch.argmax(outputs, dim=1)\\nprint(predictions)\\nThe output is\\ntensor([0, 0, 0, 1, 1])\\nHere, we computed the predicted labels for the training dataset. Since the training\\ndataset is relatively small, we could compare it to the true training labels by eye and\\nsee that the model is 100% correct. We can double-check this using the == comparison\\noperator:\\npredictions == y_train\\nThe results are\\ntensor([True, True, True, True, True])\\nUsing torch.sum, we can count the number of correct predictions:\\ntorch.sum(predictions == y_train)\\nThe output is\\n5\\nSince the dataset consists of five training examples, we have five out of five predictions\\nthat are correct, which has 5/5 × 100% = 100% prediction accuracy.\\n To generalize the computation of the prediction accuracy, let’s implement a\\ncompute_accuracy function, as shown in the following listing.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 299, 'page_label': '278'}, page_content='278 APPENDIX A Introduction to PyTorch\\ndef compute_accuracy(model, dataloader):\\n    model = model.eval()\\n    correct = 0.0\\n    total_examples = 0\\n    \\n    for idx, (features, labels) in enumerate(dataloader):\\n       \\n        with torch.no_grad():\\n            logits = model(features)\\n        \\n        predictions = torch.argmax(logits, dim=1)\\n        compare = labels == predictions      \\n        correct += torch.sum(compare)     \\n        total_examples += len(compare)\\n    return (correct / total_examples).item()   \\nThe code iterates over a data loader to compute the number and fraction of the cor-\\nrect predictions. When we work with large datasets, we typically can only call the model\\non a small part of the dataset due to memory limitations. The compute_accuracy func-\\ntion here is a general method that scales to datasets of arbitrary size since, in each iter-\\nation, the dataset chunk that the model receives is the same size as the batch size seen\\nduring training. The internals of the compute_accuracy function are similar to what\\nwe used before when we converted the logits to the class labels. \\n We can then apply the function to the training:\\nprint(compute_accuracy(model, train_loader))\\nThe result is\\n1.0\\nSimilarly, we can apply the function to the test set:\\nprint(compute_accuracy(model, test_loader))\\nThis prints\\n1.0\\nA.8 Saving and loading models\\nNow that we’ve trained our model, let’s see how to save it so we can reuse it later.\\nHere’s the recommended way how we can save and load models in PyTorch:\\ntorch.save(model.state_dict(), \"model.pth\")\\nListing A.10 A function to compute the prediction accuracy \\nReturns a tensor of True/\\nFalse values depending on \\nwhether the labels match\\nThe sum operation counts \\nthe number of True values.\\nThe fraction of correct prediction, \\na value between 0 and 1. .item() \\nreturns the value of the tensor as \\na Python float.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 300, 'page_label': '279'}, page_content='279A.9 Optimizing training performance with GPUs\\nThe model’s state_dict i s  a  P y t h o n  d i c t i o n a r y  o b j e c t  t h a t  m a p s  e a c h  l a y e r  i n  t h e\\nmodel to its trainable parameters (weights and biases). \"model.pth\" is an arbitrary\\nfilename for the model file saved to disk. We  can give it any name and file ending we\\nlike; however, .pth and .pt are the most common conventions.\\n Once we saved the model, we can restore it from disk:\\nmodel = NeuralNetwork(2, 2) \\nmodel.load_state_dict(torch.load(\"model.pth\"))\\nThe torch.load(\"model.pth\") function reads the file \"model.pth\" and recon-\\nstructs the Python dictionary object co ntaining the model’s parameters while\\nmodel.load_state_dict() applies these parameters to the model, effectively restor-\\ning its learned state from when we saved it.\\n The line model = NeuralNetwork(2, 2) is not strictly necessary if you execute this\\ncode in the same session where you saved a model. However, I included it here to\\ni l l u s t r a t e  t h a t  w e  n e e d  a n  i n s t a n c e  o f  the model in memory to apply the saved\\nparameters. Here, the NeuralNetwork(2, 2) architecture needs to match the origi-\\nnal saved model exactly.\\nA.9 Optimizing training performance with GPUs\\nNext, let’s examine how to utilize GPUs, which accelerate deep neural network train-\\ning compared to regular CPUs. First, we’ll look at the main concepts behind GPU\\ncomputing in PyTorch. Then we will train a model on a single GPU. Finally, we’ll look\\nat distributed training using multiple GPUs.\\nA.9.1 PyTorch computations on GPU devices\\nModifying the training loop to  run optionally on a GPU is  relatively simple and only\\nrequires changing three lines of code (see section A.7). Before we make the modifica-\\ntions, it’s crucial to understand the ma in concept behind GPU computations within\\nPyTorch. In PyTorch, a device is where computations occur and data resides. The CPU\\nand the GPU are examples of devices. A Py Torch tensor resides in a device, and its\\noperations are executed on the same device.\\n Let’s see how this works in action. Assu ming that you installed a GPU-compatible\\nversion of PyTorch (see section A.1.3), we can double-che ck that our runtime indeed\\nsupports GPU computing via the following code:\\nprint(torch.cuda.is_available())\\nThe result is\\nTrue\\nNow, suppose we have two tensors that we can add; this computation will be carried\\nout on the CPU by default:\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 301, 'page_label': '280'}, page_content='280 APPENDIX A Introduction to PyTorch\\ntensor_1 = torch.tensor([1., 2., 3.])\\ntensor_2 = torch.tensor([4., 5., 6.])\\nprint(tensor_1 + tensor_2)\\nThis outputs\\ntensor([5., 7., 9.])\\nWe can now use the .to() method. This method is the same as the one we use to\\nchange a tensor’s datatype (see 2.2.2) to  transfer these tensors onto a GPU and per-\\nform the addition there: \\ntensor_1 = tensor_1.to(\"cuda\")\\ntensor_2 = tensor_2.to(\"cuda\")\\nprint(tensor_1 + tensor_2)\\nThe output is\\ntensor([5., 7., 9.], device=\\'cuda:0\\')\\nThe resulting tensor now includ es the device information, device=\\'cuda:0\\', which\\nmeans that the tensors reside on the first GPU. If your machine hosts multiple GPUs,\\nyou can specify which GPU you’d like to transfer the tensors to. You do so by indicat-\\ning the device ID in the transfer command. For instance, you can use .to(\"cuda:0\"),\\n.to(\"cuda:1\"), and so on.\\n However, all tensors must be on the sa me device. Otherwise, the computation will\\nfail, where one tensor resides on the CPU and the other on the GPU:\\ntensor_1 = tensor_1.to(\"cpu\")\\nprint(tensor_1 + tensor_2)\\nThe results are\\nRuntimeError      Traceback (most recent call last)\\n<ipython-input-7-4ff3c4d20fc3> in <cell line: 2>()\\n      1 tensor_1 = tensor_1.to(\"cpu\")\\n----> 2 print(tensor_1 + tensor_2)\\nRuntimeError: Expected all tensors to be on the same device, but found at\\nleast two devices, cuda:0 and cpu!\\nIn sum, we only need to transfer the te nsors onto the same GPU device, and PyTorch\\nwill handle the rest. \\nA.9.2 Single-GPU training\\nNow that we are familiar wi th transferring tensors to the GPU, we can modify the\\ntraining loop to run on a GPU. This step requires only changing three lines of code,\\nas shown in the following listing.\\n \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 302, 'page_label': '281'}, page_content='281A.9 Optimizing training performance with GPUs\\ntorch.manual_seed(123)\\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\\ndevice = torch.device(\"cuda\")     \\nmodel = model.to(device)         \\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\\nnum_epochs = 3\\nfor epoch in range(num_epochs):\\n    \\n    model.train()\\n    for batch_idx, (features, labels) in enumerate(train_loader):\\n        features, labels = features.to(device), labels.to(device)  \\n        logits = model(features)\\n        loss = F.cross_entropy(logits, labels) # Loss function\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n    \\n        ### LOGGING\\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\\n              f\" | Train/Val Loss: {loss:.2f}\")\\n    model.eval()\\n    # Insert optional model evaluation code\\nRunning the preceding code w ill output the following, similar to the results obtained\\non the CPU (section A.7):\\nEpoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\\nEpoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\\nEpoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\\nEpoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\\nEpoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\\nEpoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\\nWe can use .to(\"cuda\") instead of device = torch.device(\"cuda\"). Transferring a\\ntensor to \"cuda\" instead of torch.device(\"cuda\") works as well and is shorter (see\\nsection A.9.1). We can also modify the statement, which will make the same code exe-\\ncutable on a CPU if a GPU is not available. This is considered best practice when shar-\\ning PyTorch code:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nIn the case of the modified  training loop here, we pr obably won’t see a speedup due\\nto the memory transfer cost from CPU to GPU. However, we can expect a significant\\nspeedup when training deep neural networks, especially LLMs. \\nListing A.11 A training loop on a GPU\\nDefines a device variable \\nthat defaults to a GPU\\nTransfers the model \\nonto the GPU\\nTransfers the data\\nonto the GPU\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 303, 'page_label': '282'}, page_content='282 APPENDIX A Introduction to PyTorch\\nA.9.3 Training with multiple GPUs\\nDistributed training is the concept of dividing the mo del training across multiple\\nGPUs and machines. Why do we need this? Ev en when it is possible to train a model\\non a single GPU or machine, the process could be exceedingly time-consuming. The\\ntraining time can be signific antly reduced by distributing  the training process across\\nmultiple machines, each with potentially multiple GPUs. This is particularly crucial in\\nthe experimental stages of model developm ent, where numerous training iterations\\nmight be necessary to fine-tune the model parameters and architecture. \\nNOTE For this book, access to or use of multiple GPUs is not required. This\\nsection is included for those interested in how multi-GPU computing works in\\nPyTorch.\\nLet’s begin with the most basic case of distributed training: PyTorch’s Distributed-\\nDataParallel (DDP) strategy. DDP enables parall elism by splitting the input data\\nacross the available devices and processi ng these data subsets simultaneously.\\n How does this work? PyTorch launches a separate process on each GPU, and each\\nprocess receives and keeps a copy of the model ; these copies will be synchronized\\nduring training. To illustrate this, suppose we have two GPUs that we want to use to\\ntrain a neural network, as shown in figure A.12. \\n Each of the two GPUs will receive a copy of the model. Then, in every training iter-\\nation, each model will receive a minibatch (or just “batch”) from the data loader. We\\nPyTorch on macOS \\nOn an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer models)\\ninstead of a computer with an Nvidia GPU, you can change \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nto\\ndevice = torch.device(\\n    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\\n)\\nto take advantage of this chip. \\nExercise A.4\\nCompare the run time of matrix multiplication on a CPU to a GPU. At what matrix size\\ndo you begin to see the matrix multiplicat ion on the GPU being faster than on the\\nCPU? Hint: use the %timeit command in Jupyter to compare the run time. For exam-\\nple, given matrices a and b, run the command %timeit a @ b in a new notebook cell.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 304, 'page_label': '283'}, page_content='283A.9 Optimizing training performance with GPUs\\ncan use a DistributedSampler to ensure that each GPU will receive a different, non-\\noverlapping batch when using DDP. \\n Since each model copy will see a different sample of the training data, the model\\ncopies will return different logits as ou tputs and compute different gradients during\\nthe backward pass. These gradients are then  averaged and synchronized during train-\\ning to update the models. This way, we ensu re that the models don’t diverge, as illus-\\ntrated in figure A.13.\\nThe benefit of using DDP is the enhanced speed it offers for processing the dataset com-\\npared to a single GPU. Barring a minor communication overhead between devices that\\nThe model is initialized\\non the CPU.\\nThe model is initialized\\non the CPU.\\nThe ﬁrst minibatch\\nFigure A.12 The model and data transfer in DDP involves two key steps. First, we create a \\ncopy of the model on each of the GPUs. Then we divide the input data into unique \\nminibatches that we pass on to each model copy.\\nEach GPU computes\\nthe outputs (logits)\\nindependently.\\nThe gradients are\\nsynced across the\\nGPUs to compute\\nthe weight updates\\nfor each GPU.\\nFigure A.13 The forward and backward passes in DDP are executed independently on each GPU with \\nits corresponding data subset. Once the forward and backward passes are completed, gradients from \\neach model replica (on each GPU) are synchronized across all GPUs. This ensures that every model \\nreplica has the same updated weights.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 305, 'page_label': '284'}, page_content='284 APPENDIX A Introduction to PyTorch\\ncomes with DDP use, it can theoretically process a training epoch in half the time with\\ntwo GPUs compared to just one. The time efficiency scales up with the number of GPUs,\\nallowing us to process an epoch eight times faster if we have eight GPUs, and so on.\\nNOTE DDP does not function properly wi thin interactive Python environ-\\nments like Jupyter notebooks, which don’t handle multiprocessing in the same\\nway a standalone Python script does. Therefore, the following code should be\\nexecuted as a script, not within a note book interface like Jupyter. DDP needs\\nto spawn multiple proce sses, and each process should have its own Python\\ninterpreter instance.\\nLet’s now see how this works in practice. For brevity, I focus on the core parts of the\\ncode that need to be adjusted for DDP training. However, readers who want to run the\\ncode on their own multi-GPU machine or a cl oud instance of their choice should use\\nthe standalone script provided in this book’s GitHub repository at https:/ /github\\n.com/rasbt/LLMs-from-scratch. \\n First, we import a few additional submodules, classes, and functions for distributed\\ntraining PyTorch, as shown in the following listing.\\nimport torch.multiprocessing as mp\\nfrom torch.utils.data.distributed import DistributedSampler\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\nfrom torch.distributed import init_process_group, destroy_process_group\\nBefore we dive deeper into the changes to  make the training compatible with DDP,\\nlet’s briefly go over the rationale and usage for these newly imported utilities that we\\nneed alongside the DistributedDataParallel class.\\n PyTorch’s multiprocessing submodule contains functions such as multiprocessing\\n.spawn, which we will use to spawn multiple processes and apply a function to multi-\\nple inputs in parallel. We will use it to  spawn one training pr ocess per GPU. If we\\nspawn multiple processes for training, we will need a way to divide the dataset among\\nthese different processes. For this, we will use the DistributedSampler.\\n init_process_group and destroy_process_group are used to initialize and quit\\nthe distributed training mods. The init_process_group function should be called\\nat the beginning of the traini ng script to initialize a process group for each process in\\nthe distributed setup, and destroy_process_group should be called at the end of the\\ntraining script to destroy a given process group and release its resources. The code in\\nthe following listing illustrates how thes e new components are used to implement\\nDDP training for the NeuralNetwork model we implemented earlier.\\ndef ddp_setup(rank, world_size):\\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"   \\nListing A.12 PyTorch utilities for distributed training\\nListing A.13 Model training with the DistributedDataParallel strategy\\nAddress of the \\nmain node\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 306, 'page_label': '285'}, page_content='285A.9 Optimizing training performance with GPUs\\n    os.environ[\"MASTER_PORT\"] = \"12345\"     \\n    init_process_group(\\n        backend=\"nccl\",             \\n        rank=rank,                        \\n        world_size=world_size           \\n    )\\n    torch.cuda.set_device(rank)       \\ndef prepare_dataset():\\n    # insert dataset preparation code \\n    train_loader = DataLoader(\\n        dataset=train_ds,\\n        batch_size=2,\\n        shuffle=False,            \\n        pin_memory=True,          \\n        drop_last=True,\\n        sampler=DistributedSampler(train_ds)   \\n    )    \\n    return train_loader, test_loader\\ndef main(rank, world_size, num_epochs):      \\n    ddp_setup(rank, world_size)\\n    train_loader, test_loader = prepare_dataset()\\n    model = NeuralNetwork(num_inputs=2, num_outputs=2)\\n    model.to(rank)\\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\\n    model = DDP(model, device_ids=[rank])\\n    for epoch in range(num_epochs):\\n    for features, labels in train_loader:\\n            features, labels = features.to(rank), labels.to(rank)     \\n            # insert model prediction and backpropagation code \\n            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\\n                  f\" | Batchsize {labels.shape[0]:03d}\"\\n                  f\" | Train/Val Loss: {loss:.2f}\")\\n    \\n    model.eval()\\n    train_acc = compute_accuracy(model, train_loader, device=rank)\\n    print(f\"[GPU{rank}] Training accuracy\", train_acc)\\n    test_acc = compute_accuracy(model, test_loader, device=rank)\\n    print(f\"[GPU{rank}] Test accuracy\", test_acc)\\n    destroy_process_group()                     \\nif __name__ == \"__main__\":\\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\\n    torch.manual_seed(123)\\n    num_epochs = 3\\n    world_size = torch.cuda.device_count()\\n    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size) \\nBefore we run this code, let’s summarize ho w it works in additi on to the preceding\\nannotations. We have a __name__ == \"__main__\" clause at the bottom containing code\\nexecuted when we run the code as a Python script instead of importing it as a module.\\nAny free port \\non the machine\\nnccl stands for NVIDIA Collective \\nCommunication Library.\\nrank refers to the index of \\nthe GPU we want to use.\\nworld_size\\nis the\\nnumber of\\nGPUs to\\nuse.\\nSets the current GPU device on \\nwhich tensors will be allocated and \\noperations will be performedDistibuted-\\nSampler\\ntakes care of\\nthe shuffling\\nnow.\\nEnables faster memory transfer \\nwhen training on GPU\\nSplits the dataset into distinct, \\nnon-overlapping subsets for \\neach process (GPU)\\nThe main function \\nrunning the model \\ntraining\\nrank is the\\nGPU ID\\nCleans up resource \\nallocation\\nLaunches the main function using multiple processes, where\\nnprocs=world_size means one process per GPU.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 307, 'page_label': '286'}, page_content='286 APPENDIX A Introduction to PyTorch\\nThis code first prints the number of available GPUs using torch.cuda.device_count(),\\nsets a random seed for reproducibility, and then spawns new processes using PyTorch’s\\nmultiprocessesing.spawn function. Here, the spawn function launches one process per\\nGPU setting nproces=world_size, where the world size is the number of available GPUs.\\nThis spawn function launches the code in the main function we define in the same script\\nwith some additional arguments provided via args. Note that the main function has a\\nrank argument that we don’t include in the mp.spawn() call. That’s because the rank,\\nwhich refers to the process ID we use as the GPU ID, is already passed automatically.\\n The main function sets up the di stributed environment via ddp_setup— another\\nfunction we defined — loads the training and test sets, sets up the model, and carries\\nout the training. Compared to the single-GP U training (section A.9.2), we now trans-\\nfer the model and data to the target device via .to(rank), which we use to refer to the\\nGPU device ID. Also, we wrap the model via DDP, which enables the synchronization of\\nthe gradients between the different GPUs du ring training. After the training finishes\\nand we evaluate the models, we use destroy_process_group() to cleanly exit the dis-\\ntributed training and free up the allocated resources.\\n Earlier I mentioned that each GPU will receive a different subsample of the train-\\ning data. To ensure this, we set sampler=DistributedSampler(train_ds) in the train-\\ning loader.\\n The last function to discuss is ddp_setup. It sets the main no de’s address and port\\nto allow for communication between the diffe rent processes, initializes the process\\ngroup with the NCCL backend (designed for GPU-to-GPU communication), and sets\\nthe rank (process identifier) and world size (t otal number of processes). Finally, it\\nspecifies the GPU device corresponding to the current model training process rank.\\nSELECTING AVAILABLE GPUS ON A MULTI-GPU MACHINE  \\nIf you wish to restrict the number of GPUs used for training on a multi-GPU machine,\\nthe simplest way is to use the CUDA_VISIBLE_DEVICES environment variable. To illus-\\ntrate this, suppose your machine has multip le GPUs, and you only want to use one\\nGPU— for example, the GPU with index 0. Instead of python some_script.py, you can\\nrun the following code from the terminal:\\nCUDA_VISIBLE_DEVICES=0 python some_script.py\\nOr, if your machine has four GPUs and you only want to use the first and third GPU,\\nyou can use\\nCUDA_VISIBLE_DEVICES=0,2 python some_script.py\\nSetting CUDA_VISIBLE_DEVICES in this way is a simple and effective way to manage\\nGPU allocation without modifying your PyTorch scripts.\\n Let’s now run this code and see how it works in practice by launching the code as a\\nscript from the terminal:\\npython ch02-DDP-script.py\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 308, 'page_label': '287'}, page_content='287A.9 Optimizing training performance with GPUs\\nNote that it should work on both single and multi-GPU machines. If we run this code\\non a single GPU, we should see the following output:\\nPyTorch version: 2.2.1+cu117\\nCUDA available: True\\nNumber of GPUs available: 1\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.62\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.32\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.11\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.02\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.03\\n[GPU0] Training accuracy 1.0\\n[GPU0] Test accuracy 1.0\\nThe code output looks similar to that using a single GPU (section A.9.2), which is a\\ngood sanity check. \\n Now, if we run the same command and code on a machine with two GPUs, we\\nshould see the following:\\nPyTorch version: 2.2.1+cu117\\nCUDA available: True\\nNumber of GPUs available: 2\\n[GPU1] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.60\\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.59\\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.16\\n[GPU1] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.17\\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\\n[GPU1] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\\n[GPU1] Training accuracy 1.0\\n[GPU0] Training accuracy 1.0\\n[GPU1] Test accuracy 1.0\\n[GPU0] Test accuracy 1.0\\nAs expected, we can see that some batches are processed on the first GPU ( GPU0) and\\nothers on the second ( GPU1). However, we see duplicated output lines when printing\\nthe training and test accuracies. Each proc ess (in other words, each GPU) prints the\\ntest accuracy independently. Since DDP replicates the model onto each GPU and\\neach process runs independently, if you ha ve a print statement inside your testing\\nloop, each process will execute it, leading to repeated output lines. If this bothers you,\\nyou can fix it using the rank of each process to control your print statements: \\nif rank == 0:                 \\n    print(\"Test accuracy: \", accuracy)\\nThis is, in a nutshell, how distributed traini ng via DDP works. If you are interested in\\nadditional details, I recommend checki ng the official AP I documentation at https:/ /\\nmng.bz/9dPr.\\nOnly print in the \\nfirst process\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 309, 'page_label': '288'}, page_content='288 APPENDIX A Introduction to PyTorch\\nSummary\\n\\uf0a1 PyTorch is an open source library with three core components: a tensor library,\\nautomatic differentiation functions, and deep learning utilities.\\n\\uf0a1 PyTorch’s tensor library is similar to array libraries like NumPy.\\n\\uf0a1 In the context of PyTorch, tensors are array-like data structures representing\\nscalars, vectors, matrices, and higher-dimensional arrays.\\n\\uf0a1 PyTorch tensors can be executed on the CPU, but one major advantage of\\nPyTorch’s tensor format is its GPU support to accelerate computations.\\n\\uf0a1 The automatic differentiation (autograd) capabilities in PyTorch allow us to\\nconveniently train neural networks using backpropagation without manually\\nderiving gradients.\\n\\uf0a1 The deep learning utilities in PyTorch provide building blocks for creating cus-\\ntom deep neural networks.\\n\\uf0a1 PyTorch includes Dataset and DataLoader classes to set up efficient data-load-\\ning pipelines.\\n\\uf0a1 It’s easiest to train models on a CPU or single GPU. \\n\\uf0a1 Using DistributedDataParallel is the simplest way in PyTorch to accelerate\\nthe training if multiple GPUs are available.\\nAlternative PyTorch APIs for multi-GPU training \\nIf you prefer a more straightforward way to use multiple GPUs in PyTorch, you can con-\\nsider add-on APIs like the open-source Fabric library. I wrote about it in “Accelerating\\nPyTorch Model Training: Using Mixed-Prec ision and Fully Sharded Data Parallelism”\\n(https:/ /mng.bz/jXle).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 310, 'page_label': '289'}, page_content='289\\nappendix B\\nReferences and\\nfurther reading\\nChapter 1\\nCustom-built LLMs are able to outperfo rm general-purpose LLMs as a team at\\nBloomberg showed via a version of GPT pretrained on finance data from scratch.\\nThe custom LLM outperformed ChatGPT on financial tasks while maintaining\\ngood performance on general LLM benchmarks:\\n\\uf0a1 “BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al.,\\nhttps:/ /arxiv.org/abs/2303.17564\\nExisting LLMs can be adapted and fine-tun ed to outperform general LLMs as well,\\nwhich teams from Google Research and Google DeepMind showed in a medical\\ncontext:\\n\\uf0a1 “Towards Expert-Level Medical Ques tion Answering with Large Language\\nModels” (2023) by Singhal et al., https:/ /arxiv.org/abs/2305.09617\\nThe following paper proposed the original transformer architecture:\\n\\uf0a1 “Attention Is All You Need” (2017) by Vaswani et al., https:/ /arxiv.org/abs/\\n1706.03762\\nOn the original encoder-style transformer, called BERT, see\\n\\uf0a1 “BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\\nstanding” (2018) by Devlin et al., https:/ /arxiv.org/abs/1810.04805\\nThe paper describing the decoder-style GPT-3 model, which inspired modern LLMs\\nand will be used as a template for implementing an LLM from scratch in this book, is\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 311, 'page_label': '290'}, page_content='290 APPENDIX B References and further reading\\n\\uf0a1 “Language Models are Few-Shot Le arners” (2020) by Brown et al., https:/ /\\narxiv.org/abs/2005.14165\\nThe following covers the original vision transformer for classifying images, which illus-\\ntrates that transformer architectures are not only restricted to text inputs:\\n\\uf0a1 “An Image is Worth 16x16 Words: Tr ansformers for Image Recognition at\\nScale” (2020) by Dosovitskiy et al., https:/ /arxiv.org/abs/2010.11929 \\nThe following experimental (but less popu lar) LLM architectures serve as examples\\nthat not all LLMs need to be based on the transformer architecture:\\n\\uf0a1 “RWKV: Reinventing RNNs for the Transformer Era” (2023) by Peng et al.,\\nhttps:/ /arxiv.org/abs/2305.13048\\n\\uf0a1 “Hyena Hierarchy: Towards Larger Convolutional Language Models” (2023) by\\nPoli et al., https:/ /arxiv.org/abs/2302.10866\\n\\uf0a1 “Mamba: Linear-Time Sequence Modeling  with Selective State Spaces” (2023)\\nby Gu and Dao, https:/ /arxiv.org/abs/2312.00752\\nMeta AI’s model is a popular implementation of a GPT-like model that is openly avail-\\nable in contrast to GPT-3 and ChatGPT:\\n\\uf0a1 “Llama 2: Open Foundation and Fine-Tuned Chat Models” (2023) by Touvron\\net al., https:/ /arxiv.org/abs/2307.092881\\nFor readers interested in additional details about the dataset references in section 1.5,\\nthis paper describes the publicly available The Pile dataset curated by Eleuther AI:\\n\\uf0a1 “The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by\\nGao et al., https:/ /arxiv.org/abs/2101.00027\\nThe following paper provides the referenc e for InstructGPT for fine-tuning GPT-3,\\nwhich was mentioned in section 1.6 and will be discussed in more detail in chapter 7:\\n\\uf0a1 “Training Language Models to Follow Instructions with Human Feedback”\\n(2022) by Ouyang et al., https:/ /arxiv.org/abs/2203.02155\\nChapter 2\\nReaders who are interested in  discussion and comparison  of embedding spaces with\\nlatent spaces and the general notion of vector representations can find more informa-\\ntion in the first chapter of my book:\\n\\uf0a1 Machine Learning Q and AI  (2023) by Sebastian Raschka, https:/ /leanpub.com/\\nmachine-learning-q-and-ai\\nThe following paper provides more in-depth discussions of how byte pair encoding is\\nused as a tokenization method:\\n\\uf0a1 “Neural Machine Translation of Rare Words with Subword Units” (2015) by\\nSennrich et al., https:/ /arxiv.org/abs/1508.07909\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 312, 'page_label': '291'}, page_content='291Chapter 3\\nThe code for the byte pair encoding toke nizer used to train GPT-2 was open-sourced\\nby OpenAI:\\n\\uf0a1 https:/ /github.com/openai/gpt-2/blob/master/src/encoder.py\\nOpenAI provides an interactive web UI to illustrate how the byte pair tokenizer in\\nGPT models works:\\n\\uf0a1 https:/ /platform.openai.com/tokenizer\\nFor readers interested in coding and training a BPE tokenizer from the ground\\nup, Andrej Karpathy’s GitHub repository minbpe offers a minimal and readable\\nimplementation:\\n\\uf0a1 “A Minimal Implementatio n of a BPE Tokenizer,” https:/ /github.com/karpa-\\nthy/minbpe\\nReaders who are interested in studying alternative tokenization schemes that are used\\nby some other popular LLMs can find mo re information in the SentencePiece and\\nWordPiece papers:\\n\\uf0a1 “SentencePiece: A Simple and Language Independent Subword Tokenizer and\\nDetokenizer for Neural Text Processi ng” (2018) by Kudo and Richardson,\\nhttps:/ /aclanthology.org/D18-2012/\\n\\uf0a1 “Fast WordPiece Tokenization” (2020) by Song et al., https:/ /arxiv.org/abs/\\n2012.15524\\nChapter 3\\nReaders interested in lear ning more about Bahdanau attention for RNN and lan-\\nguage translation can find detailed insights in the following paper:\\n\\uf0a1 “Neural Machine Translation by Jointl y Learning to Align and Translate”\\n(2014) by Bahdanau, Cho, and Bengio, https:/ /arxiv.org/abs/1409.0473\\nThe concept of self-attention as scaled do t-product attention was introduced in the\\noriginal transformer paper:\\n\\uf0a1 “Attention Is All You Need” (2 017) by Vaswani et al., https:/ /arxiv.org/abs/\\n1706.03762\\nFlashAttention is a highly efficient impl ementation of a self-attention mechanism,\\nwhich accelerates the computation process by optimizing memory access patterns.\\nFlashAttention is mathematically the same as the standard self-attention mechanism\\nbut optimizes the computational process for efficiency:\\n\\uf0a1 “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”\\n(2022) by Dao et al., https:/ /arxiv.org/abs/2205.14135\\n\\uf0a1 “FlashAttention-2: Faster Attention with Better Parallelism and Work Partition-\\ning” (2023) by Dao, https:/ /arxiv.org/abs/2307.08691\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 313, 'page_label': '292'}, page_content='292 APPENDIX B References and further reading\\nPyTorch implements a function for self-atte ntion and causal attention that supports\\nFlashAttention for efficiency. This function is beta and subject to change:\\n\\uf0a1 scaled_dot_product_attention documentation: https:/ /mng.bz/NRJd\\nPyTorch also implements an efficient MultiHeadAttention class based on the scaled_\\ndot_product function:\\n\\uf0a1 MultiHeadAttention documentation: https:/ /mng.bz/DdJV\\nDropout is a regularization technique used in neural networks to prevent overfitting\\nby randomly dropping units (along with their connections) from the neural network\\nduring training:\\n\\uf0a1 “Dropout: A Simple Way to Prevent Neur al Networks from Overfitting” (2014)\\nby Srivastava et al., https:/ /jmlr.org/papers/v15/srivastava14a.html\\nWhile using the multi-head attention based on scaled-dot product attention remains\\nthe most common variant of self-attention in practice, authors have found that it’s\\npossible to also achieve good performance without the value weight matrix and pro-\\njection layer:\\n\\uf0a1 “Simplifying Transformer Blocks ” (2023) by He and Hofmann, https:/ /arxiv\\n.org/abs/2311.01906\\nChapter 4\\nThe following paper introduces a technique that stabilizes the hidden state dynamics\\nneural networks by normalizing the summed inputs to the neurons within a hidden\\nlayer, significantly reducing training time compared to previously published methods:\\n\\uf0a1 “Layer Normalization” (2016) by Ba, Kiros, and Hinton, https:/ /arxiv.org/abs/\\n1607.06450\\nPost-LayerNorm, used in the original tran sformer model, applies layer normalization\\nafter the self-attention and feed forward networks. In contrast, Pre-LayerNorm, as\\nadopted in models like GPT- 2 and newer LLMs, applies layer normalization before\\nthese components, which can lead to more stable trai ning dynamics and has been\\nshown to improve performance in some cases, as discussed in the following papers:\\n\\uf0a1 “On Layer Normalization in the Transformer Architecture” (2020) by Xiong et\\nal., https:/ /arxiv.org/abs/2002.04745\\n\\uf0a1 “ResiDual: Transformer with Dual Resi dual Connections” (2023) by Tie et al.,\\nhttps:/ /arxiv.org/abs/2304.14802\\nA popular variant of LayerNorm used in  modern LLMs is RMSNorm due to its\\nimproved computing efficiency. This varian t simplifies the normalization process by\\nnormalizing the inputs using only the root  mean square of the inputs, without sub-\\ntracting the mean before squaring. This means it does not center the data before com-\\nputing the scale. RMSNorm is described in more detail in \\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 314, 'page_label': '293'}, page_content='293Chapter 5\\n\\uf0a1 “Root Mean Square Layer Normalizat ion” (2019) by Zhang and Sennrich,\\nhttps:/ /arxiv.org/abs/1910.07467\\nThe Gaussian Error Linear Unit (GELU) activation function combines the properties\\nof both the classic ReLU activation function and the normal distribution’s cumulative\\ndistribution function to mo del layer outputs, allowing for stochastic regularization\\nand nonlinearities in deep learning models:\\n\\uf0a1 “Gaussian Error Linear Units (GELUs) ” (2016) by Hendricks and Gimpel,\\nhttps:/ /arxiv.org/abs/1606.08415\\nThe GPT-2 paper introduced a series of tr ansformer-based LLMs with varying sizes—\\n124 million, 355 million, 774 million, and 1.5 billion parameters:\\n\\uf0a1 “Language Models Are Unsupervised Multitask Learners” (2019) by Radford et\\nal., https:/ /mng.bz/lMgo\\nOpenAI’s GPT-3 uses fundamentally the same architecture as GPT-2, except that the\\nlargest version (175 billion) is 100x larger than the largest GPT-2 model and has been\\ntrained on much more data. Interested read ers can refer to the official GPT-3 paper\\nby OpenAI and the technical overview by Lambda Labs, which calculates that training\\nGPT-3 on a single RTX 8000 consumer GPU would take 665 years:\\n\\uf0a1 “Language Models are Few-Shot Le arners” (2023) by Brown et al., https:/ /\\narxiv.org/abs/2005.14165\\n\\uf0a1 “OpenAI’s GPT-3 Language Model: A Technical Overview,” https:/ /lambdalabs\\n.com/blog/demystifying-gpt-3\\nNanoGPT is a code repository with a mini malist yet efficient implementation of a\\nGPT-2 model, similar to the model implemen ted in this book. While the code in this\\nbook is different from nanoGPT, this repository inspired the reorganization of a large\\nGPT Python parent class implementation into smaller submodules:\\n\\uf0a1 “NanoGPT, a Repository for Training Medium-Sized GPTs, https:/ /github.com/\\nkarpathy/nanoGPT\\nAn informative blog post showing that mo st of the computation in LLMs is spent in\\nthe feed forward layers rather than attent ion layers when the context size is smaller\\nthan 32,000 tokens is:\\n\\uf0a1 “In the Long (Context) Run” by Harm de Vries, https:/ /www.harmdevries.com/\\npost/context-length/\\nChapter 5\\nFor information on detailing the loss functi on and applying a lo g transformation to\\nmake it easier to handle for mathematical optimization, see my lecture video:\\n\\uf0a1 L8.2 Logistic Regre ssion Loss Function, https:/ /www.youtube.com/watch?v=\\nGxJe0DZvydM\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 315, 'page_label': '294'}, page_content='294 APPENDIX B References and further reading\\nThe following lecture and code example by  the author explain how PyTorch’s cross-\\nentropy functions works under the hood:\\n\\uf0a1 L8.7.1 OneHot Encoding and Mu lti-category Cross Entropy, https:/ /www\\n.youtube.com/watch?v=4n71-tZ94yk\\n\\uf0a1 Understanding Onehot Encoding and Cross Entropy in PyTorch, https:/ /\\nmng.bz/o05v\\nThe following two papers detail the dataset,  hyperparameter, and architecture details\\nused for pretraining LLMs:\\n\\uf0a1 “Pythia: A Suite for Analyzing Large Language Models Across Training and\\nScaling” (2023) by Biderman et al., https:/ /arxiv.org/abs/2304.01373\\n\\uf0a1 “OLMo: Accelerating the Science of La nguage Models” (2024) by Groeneveld\\net al., https:/ /arxiv.org/abs/2402.00838\\nThe following supplementary code available for this book contains instructions for\\npreparing 60,000 public domain books from Project Gutenberg for LLM training:\\n\\uf0a1 Pretraining GPT on the Project Gutenberg Dataset, https:/ /mng.bz/Bdw2\\nChapter 5 discusses the pret raining of LLMs, and appendix D covers more advanced\\ntraining functions, such as linear warmup and cosine annealing. The following paper\\nfinds that similar techniques can be succ essfully applied to continue pretraining\\nalready pretrained LLMs, along with additional tips and insights:\\n\\uf0a1 “Simple and Scalable Strategies to Co ntinually Pre-train Large Language Mod-\\nels” (2024) by Ibrahim et al., https:/ /arxiv.org/abs/2403.08763\\nBloombergGPT is an example of a domain-specific LLM created by training on both\\ngeneral and domain-specific text corpora, specifically in the field of finance:\\n\\uf0a1 “BloombergGPT: A Large Language Model for Finance” (2023) by Wu et al.,\\nhttps:/ /arxiv.org/abs/2303.17564\\nGaLore is a recent research project that aims to make LLM pretraining more efficient.\\nThe required code change boils down to just replacing PyTorch’s AdamW optimizer in\\nthe training function with the GaLoreAdamW optimizer provided by the galore-torch\\nPython package:\\n\\uf0a1 “GaLore: Memory-Efficient LLM Training  by Gradient Low-Rank Projection”\\n(2024) by Zhao et al., https:/ /arxiv.org/abs/2403.03507\\n\\uf0a1 GaLore code repository, https:/ /github.com/jiaweizzhao/GaLore\\nThe following papers and resources share op enly available, large-scale pretraining\\ndatasets for LLMs that consist of hundreds of gigabytes to terabytes of text data:\\n\\uf0a1 “Dolma: An Open Corpus of Three Trillion Tokens for LLM Pretraining\\nResearch” (2024) by Soldaini et al., https:/ /arxiv.org/abs/2402.00159\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 316, 'page_label': '295'}, page_content='295Chapter 6\\n\\uf0a1 “The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by\\nGao et al., https:/ /arxiv.org/abs/2101.00027\\n\\uf0a1 “The RefinedWeb Dataset for Falcon  LLM: Outperforming Curated Corpora\\nwith Web Data, and Web Data Only,” (2023) by Penedo et al., https:/ /arxiv.org/\\nabs/2306.01116\\n\\uf0a1 “RedPajama,” by Together AI, https:/ /mng.bz/d6nw\\n\\uf0a1 The FineWeb Dataset, which includes more than 15 trillion tokens of cleaned\\nand deduplicated English web data sourced from CommonCrawl, https:/ /\\nmng.bz/rVzy\\nThe paper that originally introduced top-k sampling is\\n\\uf0a1 “Hierarchical Neural Story Generation” (2018) by Fan et al., https:/ /arxiv.org/\\nabs/1805.04833\\nAn alternative to top-k sampling is top-p sampling (not covered in chapter 5), which\\nselects from the smallest set of top tokens  whose cumulative probability exceeds a\\nthreshold p, while top-k sampling picks from the top k tokens by probability:\\n\\uf0a1 Top-p sampling, https:/ /en.wikipedia.org/wiki/Top-p_sampling\\nBeam search (not covered in chapter 5) is an alternative decoding algorithm that gen-\\nerates output sequences by keeping only the top-scoring partial sequences at each step\\nto balance efficiency and quality:\\n\\uf0a1 “Diverse Beam Search: Decoding Dive rse Solutions from Neural Sequence\\nModels” (2016) by Vijayakumar et al., https:/ /arxiv.org/abs/1610.02424\\nChapter 6\\nAdditional resources that discuss the different types of fine-tuning are\\n\\uf0a1 “Using and Finetuning Pretrained Transformers,” https:/ /mng.bz/VxJG\\n\\uf0a1 “Finetuning Large Language Models,” https:/ /mng.bz/x28X\\nAdditional experiments, including a comparis on of fine-tuning the first output token\\nversus the last output token, can be fo und in the supplementa ry code material on\\nGitHub:\\n\\uf0a1 Additional spam classification experiments, https:/ /mng.bz/AdJx\\nFor a binary classification ta sk, such as spam classificati on, it is technically possible\\nto use only a single output node instead of  two output nodes, as I discuss in the fol-\\nlowing article:\\n\\uf0a1 “Losses Learned—Opti mizing Negative Log-Likeli hood and Cross-Entropy in\\nPyTorch,” https:/ /mng.bz/ZEJA\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 317, 'page_label': '296'}, page_content='296 APPENDIX B References and further reading\\nYou can find additional experi ments on fine-tuning different layers of an LLM in the\\nfollowing article, which shows that fine-tun ing the last transformer block, in addition\\nto the output layer, improves the predictive performance substantially:\\n\\uf0a1 “Finetuning Large Language Models,” https:/ /mng.bz/RZJv\\nReaders can find additional resources an d information for dealing with imbalanced\\nclassification datasets in the imbalanced-learn documentation:\\n\\uf0a1 “Imbalanced-Learn User Guide,” https:/ /mng.bz/2KNa\\nFor readers interested in classifying spam em ails rather than spam text messages, the\\nfollowing resource provides a large email sp am classification dataset in a convenient\\nCSV format similar to the dataset format used in chapter 6:\\n\\uf0a1 Email Spam Classification Dataset, https:/ /mng.bz/1GEq\\nGPT-2 is a model based on the decoder mo dule of the transformer architecture, and\\nits primary purpose is to generate new text. As an alternative, encoder-based models\\nsuch as BERT and RoBERTa can be effective for classification tasks:\\n\\uf0a1 “BERT: Pre-training of Deep Bidirect ional Transformers for Language Under-\\nstanding” (2018) by Devlin et al., https:/ /arxiv.org/abs/1810.04805\\n\\uf0a1 “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019) by Liu\\net al., https:/ /arxiv.org/abs/1907.11692\\n\\uf0a1 “Additional Experiments Classifying the Sentiment of 50k IMDB Movie Reviews,”\\nhttps:/ /mng.bz/PZJR\\nRecent papers are showing that the cla ssification performance can be further\\ni m p r o v e d  b y  r e m o v i n g  t h e  c a u s a l  m a s k  d uring classification fine-tuning alongside\\nother modifications:\\n\\uf0a1 “Label Supervised LLaMA Finetuning” (2023) by Li et al., https:/ /arxiv.org/\\nabs/2310.01208\\n\\uf0a1 “LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders”\\n(2024) by BehnamGhader et al., https:/ /arxiv.org/abs/2404.05961\\nChapter 7\\nThe Alpaca dataset for instruction fine-tun ing contains 52,000 instruction–response\\npairs and is one of the first and most popular publicly available datasets for instruction\\nfine-tuning:\\n\\uf0a1 “Stanford Alpaca: An Instruct ion-Following Llama Model,” https:/ /github\\n.com/tatsu-lab/stanford_alpaca\\nAdditional publicly accessible datasets suitable for instruction fine-tuning include\\n\\uf0a1 LIMA, https:/ /huggingface.co/datasets/GAIR/lima \\n– For more information, see “LIMA: Le ss Is More for Alignment,” Zhou et al.,\\nhttps:/ /arxiv.org/abs/2305.11206\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 318, 'page_label': '297'}, page_content='297Chapter 7\\n\\uf0a1 UltraChat, https:/ /huggingface.co/datasets/openchat/ultrachat-sharegpt\\n– A large-scale dataset consisting of 805 ,000 instruction–re sponse pairs; for\\nmore information, see “Enhancing Chat  Language Models by Scaling High-\\nquality Instructional Conversa tions,” by Ding et al., https:/ /arxiv.org/abs/\\n2305.14233\\n\\uf0a1 Alpaca GPT4, https:/ /mng.bz/Aa0p \\n– An Alpaca-like dataset with 52,000 inst ruction–response pairs generated with\\nGPT-4 instead of GPT-3.5\\nPhi-3 is a 3.8-billion-parameter model with an instruction-fine-tuned variant that is\\nreported to be comparable to much larger proprietary models, such as GPT-3.5:\\n\\uf0a1 “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\\nPhone” (2024) by Abdin et al., https:/ /arxiv.org/abs/2404.14219\\nResearchers propose a syntheti c instruction data generati on method that generates\\n300,000 high-quality instruction-response pa irs from an instruction fine-tuned Llama-\\n3 model. A pretrained Llama 3 base model fine-tuned on these instruction examples\\nperforms comparably to the original instruction fine-tuned Llama-3 model:\\n\\uf0a1 “Magpie: Alignment Data Synthesis fr om Scratch by Prompting Aligned LLMs\\nwith Nothing” (2024) by Xu et al., https:/ /arxiv.org/abs/2406.08464\\nResearch has shown that not masking the inst ructions and inputs in instruction fine-\\ntuning effectively improves performance on various NLP tasks and open-ended gener-\\nation benchmarks, particularly when trained on datasets with lengthy instructions and\\nbrief outputs or when using a small number of training examples:\\n\\uf0a1 “Instruction Tuning with Loss Ov er Instructions” (2024) by Shi, https:/ /\\narxiv.org/abs/2405.14394\\nPrometheus and PHUDGE are openly available LLMs that match GPT-4 in evaluating\\nlong-form responses with customizable crit eria. We don’t use these because at the\\ntime of this writing, they are not suppor ted by Ollama and thus cannot be executed\\nefficiently on a laptop:\\n\\uf0a1 “Prometheus: Inducing Fi negrained Evaluation Capability in Language Mod-\\nels” (2023) by Kim et al., https:/ /arxiv.org/abs/2310.08491\\n\\uf0a1 “PHUDGE: Phi-3 as Scalable Judge”  (2024) by Deshwal and Chawla, “ https:/ /\\narxiv.org/abs/2405.08029\\n\\uf0a1 “Prometheus 2: An Open Source Lang uage Model Specialized in Evaluating\\nOther Language Models” (2024), by Kim et al., https:/ /arxiv.org/abs/2405\\n.01535\\nThe results in the following report support the view that large language models pri-\\nmarily acquire factual knowledge during pretraining and that fine-tuning mainly\\nenhances their efficiency in using this kn owledge. Furthermore,  this study explores\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 319, 'page_label': '298'}, page_content='298 APPENDIX B References and further reading\\nhow fine-tuning large language models with new factual information affects their abil-\\nity to use preexisting knowledge, revealing that models learn new facts more slowly\\nand their introduction during fine-tuning increases the model’s tendency to generate\\nincorrect information:\\n\\uf0a1 “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” (2024)\\nby Gekhman, https:/ /arxiv.org/abs/2405.05904\\nPreference fine-tuning is an optional step  after instruction fine-tuning to align the\\nLLM more closely with human preferences. The following articles by the author pro-\\nvide more information about this process:\\n\\uf0a1 “LLM Training: RLHF and Its Alternatives,” https:/ /mng.bz/ZVPm\\n\\uf0a1 “Tips for LLM Pretraining and Evaluating Reward Models,” https:/ /mng.bz/\\nRNXj\\nAppendix A\\nWhile appendix A should be sufficient to ge t you up to speed, if you are looking for\\nmore comprehensive introductions to deep learning, I recommend the following\\nbooks:\\n\\uf0a1 Machine Learning with PyTorch and Scikit-Learn  (2022) by Sebastian Raschka,\\nHayden Liu, and Vahid Mirjalili. ISBN 978-1801819312\\n\\uf0a1 Deep Learning with PyTorch (2021) by Eli Stevens, Luca Antiga, and Thomas Vieh-\\nmann. ISBN 978-1617295263\\nFor a more thorough introduction to the co ncepts of tensors, readers can find a 15-\\nminute video tutorial that I recorded:\\n\\uf0a1 “Lecture 4.1: Tensors in Deep Learning,” https:/ /www.youtube.com/watch?v=\\nJXfDlgrfOBY\\nIf you want to learn more about model evaluation in machine learning, I recommend\\nmy article\\n\\uf0a1 “Model Evaluation, Model Selection, and Algorithm Selection in Machine\\nLearning” (2018) by Sebastian Raschka, https:/ /arxiv.org/abs/1811.12808\\nFor readers who are interested in a refresher or gentle introduction to calculus, I’ve\\nwritten a chapter on calculus that is freely available on my website:\\n\\uf0a1 “Introduction to Calculus,” by Sebastian Raschka, https:/ /mng.bz/WEyW\\nWhy does PyTorch not call optimizer.zero_grad() automatically for us in the back-\\nground? In some instances, it may be desirable to accu mulate the gradients, and\\nPyTorch will leave this as an option for us . If you want to learn more about gradient\\naccumulation, please see the following article:\\n\\uf0a1 “Finetuning Large Language Models on a Single GPU Using Gradient Accumu-\\nlation” by Sebastian Raschka, https:/ /mng.bz/8wPD\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 320, 'page_label': '299'}, page_content='299Appendix A\\nThis appendix covers DDP, which is a popu lar approach for training deep learning\\nmodels across multiple GPUs. For more ad vanced use cases where a single model\\ndoesn’t fit onto the GPU, you may also consider PyTorch’s Fully Sharded Data Parallel\\n(FSDP) method, which performs distributed data parallelism and distributes large lay-\\ners across different GPUs. For more informat ion, see this overview with further links\\nto the API documentation: \\n\\uf0a1 “Introducing PyTorch Fully Sharded Data Parallel (FSDP) API,” https:/ /mng\\n.bz/EZJR\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 321, 'page_label': '300'}, page_content='300\\nappendix C\\nExercise solutions\\nThe complete code examples for the exercises’ answers can be found in the supple-\\nmentary GitHub repository at https:/ /github.com/rasbt/LLMs-from-scratch.\\nChapter 2\\nExercise 2.1\\nYou can obtain the individual token IDs by prompting the encoder with one string\\nat a time:\\nprint(tokenizer.encode(\"Ak\"))\\nprint(tokenizer.encode(\"w\"))\\n# ...\\nThis prints\\n[33901]\\n[86]\\n# ...\\nYou can then use the following code to assemble the original string:\\nprint(tokenizer.decode([33901, 86, 343, 86, 220, 959]))\\nThis returns\\n\\'Akwirw ier\\'\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 322, 'page_label': '301'}, page_content='301Chapter 3\\nExercise 2.2\\nThe code for the data loader with max_length=2 and stride=2:\\ndataloader = create_dataloader(\\n    raw_text, batch_size=4, max_length=2, stride=2\\n)\\nIt produces batches of the following format:\\ntensor([[  40,  367],\\n        [2885, 1464],\\n        [1807, 3619],\\n        [ 402,  271]])\\nThe code of the second data loader with max_length=8 and stride=2:\\ndataloader = create_dataloader(\\n    raw_text, batch_size=4, max_length=8, stride=2\\n)\\nAn example batch looks like\\ntensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\\n        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\\n        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\\n        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\\nChapter 3\\nExercise 3.1\\nThe correct weight assignment is\\nsa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\\nsa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\\nsa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\\nExercise 3.2\\nTo achieve an output dimension of 2, similar to what we had in single-head attention,\\nwe need to change the projection dimension d_out to 1.\\nd_out = 1\\nmha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\\nExercise 3.3\\nThe initialization for the smallest GPT-2 model is\\nblock_size = 1024\\nd_in, d_out = 768, 768\\nnum_heads = 12\\nmha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 323, 'page_label': '302'}, page_content='302 APPENDIX C Exercise solutions\\nChapter 4\\nExercise 4.1\\nWe can calculate the number of parameters in the feed forward and attention mod-\\nules as follows:\\nblock = TransformerBlock(GPT_CONFIG_124M)\\ntotal_params = sum(p.numel() for p in block.ff.parameters())\\nprint(f\"Total number of parameters in feed forward module: {total_params:,}\")\\ntotal_params = sum(p.numel() for p in block.att.parameters())\\nprint(f\"Total number of parameters in attention module: {total_params:,}\")\\nAs we can see, the feed forward module contains approximately twice as many param-\\neters as the attention module:\\nTotal number of parameters in feed forward module: 4,722,432\\nTotal number of parameters in attention module: 2,360,064\\nExercise 4.2\\nTo instantiate the other GPT model sizes, we can modify the configuration dictionary\\nas follows (here shown for GPT-2 XL):\\nGPT_CONFIG = GPT_CONFIG_124M.copy()\\nGPT_CONFIG[\"emb_dim\"] = 1600\\nGPT_CONFIG[\"n_layers\"] = 48\\nGPT_CONFIG[\"n_heads\"] = 25\\nmodel = GPTModel(GPT_CONFIG)\\nThen, reusing the code from section 4.6 to calculate the number of parameters and\\nRAM requirements, we find\\ngpt2-xl:\\nTotal number of parameters: 1,637,792,000\\nNumber of trainable parameters considering weight tying: 1,557,380,800\\nTotal size of the model: 6247.68 MB\\nExercise 4.3\\nThere are three distinct places in chapter 4 where we used dropout layers: the embed-\\nding layer, shortcut layer, and multi-head attention module. We can control the drop-\\nout rates for each of the layers by coding them separately in the config file and then\\nmodifying the code implementation accordingly. \\n The modified configuration is as follows:\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,\\n    \"context_length\": 1024,\\n    \"emb_dim\": 768,\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 324, 'page_label': '303'}, page_content='303Chapter 4\\n    \"n_heads\": 12,\\n    \"n_layers\": 12,\\n    \"drop_rate_attn\": 0.1,     \\n    \"drop_rate_shortcut\": 0.1,     \\n    \"drop_rate_emb\": 0.1,     \\n    \"qkv_bias\": False\\n}\\nThe modified TransformerBlock and GPTModel look like\\nclass TransformerBlock(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.att = MultiHeadAttention(\\n            d_in=cfg[\"emb_dim\"],\\n            d_out=cfg[\"emb_dim\"],\\n            context_length=cfg[\"context_length\"],\\n            num_heads=cfg[\"n_heads\"], \\n            dropout=cfg[\"drop_rate_attn\"],     \\n            qkv_bias=cfg[\"qkv_bias\"])\\n        self.ff = FeedForward(cfg)\\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\\n        self.drop_shortcut = nn.Dropout(       \\n            cfg[\"drop_rate_shortcut\"]          \\n        )                                      \\n    def forward(self, x):\\n        shortcut = x\\n        x = self.norm1(x)\\n        x = self.att(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut\\n        shortcut = x\\n        x = self.norm2(x)\\n        x = self.ff(x)\\n        x = self.drop_shortcut(x)\\n        x = x + shortcut\\n        return x\\nclass GPTModel(nn.Module):\\n    def __init__(self, cfg):\\n        super().__init__()\\n        self.tok_emb = nn.Embedding(\\n            cfg[\"vocab_size\"], cfg[\"emb_dim\"]\\n        )\\n        self.pos_emb = nn.Embedding(\\n            cfg[\"context_length\"], cfg[\"emb_dim\"]\\n        )\\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"])   \\nDropout for multi-\\nhead attention\\nDropout for shortcut \\nconnections\\nDropout for \\nembedding layer\\nDropout for multi-\\nhead attention\\nDropout for shortcut \\nconnections\\nDropout for \\nembedding \\nlayer\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 325, 'page_label': '304'}, page_content='304 APPENDIX C Exercise solutions\\n        self.trf_blocks = nn.Sequential(\\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\\n        self.out_head = nn.Linear(\\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\\n        )\\n    def forward(self, in_idx):\\n        batch_size, seq_len = in_idx.shape\\n        tok_embeds = self.tok_emb(in_idx)\\n        pos_embeds = self.pos_emb(\\n            torch.arange(seq_len, device=in_idx.device)\\n        )\\n        x = tok_embeds + pos_embeds\\n        x = self.drop_emb(x)\\n        x = self.trf_blocks(x)\\n        x = self.final_norm(x)\\n        logits = self.out_head(x)\\n        return logitss\\nChapter 5\\nExercise 5.1\\nWe can print the number of times the toke n (or word) “pizza” is sampled using the\\nprint_sampled_tokens function we defined in this section. Let’s start with the code\\nwe defined in section 5.3.1.\\n The “pizza” token is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32× if\\nthe temperature is scaled up to 5. The estimated probability is 32/1000 × 100% = 3.2%.\\n The actual probability is 4.3% and is contained in the rescaled softmax probability\\ntensor (scaled_probas[2][6]).\\nExercise 5.2\\nTop-k sampling and temperature scaling are settings that have to be adjusted based on\\nthe LLM and the desired degree of diversity and randomness in the output. \\n When using relatively small top-k values  (e.g., smaller than 10) and when the tem-\\nperature is set below 1, the model’s output becomes less random and more determin-\\nistic. This setting is useful when we need  the generated text to be more predictable,\\ncoherent, and closer to the most likely outcomes based on the training data. \\n Applications for such low k and temper ature settings include generating formal\\ndocuments or reports where clarity and accuracy are most important. Other examples\\nof applications include technical analysis or code-generation tasks, where precision is\\ncrucial. Also, question answering and educ ational content requir e accurate answers\\nwhere a temperature below 1 is helpful.\\n On the other hand, larger top-k values (e.g ., values in the range of 20 to 40) and\\ntemperature values above 1 are useful when using LLMs for brainstorming or generat-\\ning creative content, such as fiction.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 326, 'page_label': '305'}, page_content='305Chapter 5\\nExercise 5.3\\nThere are multiple ways to force deterministic behavior with the generate function:\\n1 Setting to top_k=None and applying no temperature scaling\\n2 Setting top_k=1\\nExercise 5.4\\nIn essence, we have to load the model and optimizer that we saved in the main chapter:\\ncheckpoint = torch.load(\"model_and_optimizer.pth\")\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nThen, call the train_simple_function with num_epochs=1 to train the model for\\nanother epoch.\\nExercise 5.5\\nWe can use the following code to calculate the training and validation set losses of the\\nGPT model:\\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\\nval_loss = calc_loss_loader(val_loader, gpt, device)\\nThe resulting losses for the 124-million parameter are as follows:\\nTraining loss: 3.754748503367106\\nValidation loss: 3.559617757797241\\nThe main observation is that the training and validation set performances are in the\\nsame ballpark. This can have multiple explanations:\\n1 “The Verdict” was not part of the pr etraining dataset when OpenAI trained\\nGPT-2. Hence, the model is not explicitly overfitting to the training set and per-\\nforms similarly well on the training and validation set portions of “The Verdict.”\\n(The validation set loss is slightly lo wer than the training set loss, which is\\nunusual in deep learning. However, it’s  likely due to random noise since the\\ndataset is relatively small. In practice, if  there is no overfitting, the training and\\nvalidation set performances are expected to be roughly identical).\\n2 “The Verdict” was part of GPT-2’s traini ng dataset. In this case, we can’t tell\\nwhether the model is overfitting the training data because the validation set\\nwould have been used for training as well. To evaluate the degree of overfitting,\\nwe’d need a new dataset generated afte r OpenAI finished training GPT-2 to\\nmake sure that it couldn’t have been part of the pretraining.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 327, 'page_label': '306'}, page_content='306 APPENDIX C Exercise solutions\\nExercise 5.6\\nIn the main chapter, we experimented with the smallest GPT-2 model, which has only\\n124-million parameters. The reason was to keep the resource requirements as low as\\npossible. However, you can ea sily experiment with larger  models with minimal code\\nchanges. For example, instea d of loading the 1,558 million instead of 124 million\\nmodel weights in chapter 5, the only two line s of code that we have to change are the\\nfollowing:\\nhparams, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\\nmodel_name = \"gpt2-small (124M)\"\\nThe updated code is\\nhparams, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\\nmodel_name = \"gpt2-xl (1558M)\"\\nChapter 6\\nExercise 6.1\\nWe can pad the inputs to the maximum numb er of tokens the model supports by set-\\nting the max length to max_length = 1024 when initializing the datasets:\\ntrain_dataset = SpamDataset(..., max_length=1024, ...)\\nval_dataset = SpamDataset(..., max_length=1024, ...)\\ntest_dataset = SpamDataset(..., max_length=1024, ...)\\nHowever, the additional padding results in  a substantially worse test accuracy of\\n78.33% (vs. the 95.67% in the main chapter).\\nExercise 6.2\\nInstead of fine-tuning just the final tran sformer block, we ca n fine-tune the entire\\nmodel by removing the following lines from the code:\\nfor param in model.parameters():\\n    param.requires_grad = False\\nThis modification results in a 1% improved test accuracy of 96.67% (vs. the 95.67% in\\nthe main chapter).\\nExercise 6.3\\nRather than fine-tuning the la st output token, we can fine-tune the first output token\\nby changing model(input_batch)[:, -1, :] to model(input_batch)[:, 0, :] every-\\nwhere in the code.\\n As expected, since the first token contains less information than the last token, this\\nchange results in a substantially worse test  accuracy of 75.00% (vs. the 95.67% in the\\nmain chapter).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 328, 'page_label': '307'}, page_content='307Chapter 7\\nChapter 7\\nExercise 7.1\\nThe Phi-3 prompt format, which is shown in figure 7.4, looks like the following for a\\ngiven example input:\\n<user>\\nIdentify the correct spelling of the following word: \\'Occasion\\'\\n<assistant>\\nThe correct spelling is \\'Occasion\\'.\\nTo use this template, we can modify the format_input function as follows:\\ndef format_input(entry):\\n    instruction_text = (\\n        f\"<|user|>\\\\n{entry[\\'instruction\\']}\"\\n    )\\n    input_text = f\"\\\\n{entry[\\'input\\']}\" if entry[\"input\"] else \"\"\\n    return instruction_text + input_text\\nLastly, we also have to update the way we extract the generated response when we col-\\nlect the test set responses:\\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\\n    input_text = format_input(entry)\\n    tokenizer=tokenizer\\n    token_ids = generate(\\n        model=model,\\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\\n        max_new_tokens=256,\\n        context_size=BASE_CONFIG[\"context_length\"],\\n        eos_id=50256\\n    )\\n    generated_text = token_ids_to_text(token_ids, tokenizer)\\n    response_text = (                      \\n        generated_text[len(input_text):]\\n        .replace(\"<|assistant|>:\", \"\")\\n        .strip()\\n    )\\n    test_data[i][\"model_response\"] = response_text\\nFine-tuning the model with the Phi-3 templa te is approximately 17% faster since it\\nresults in shorter model inputs. The score is close to 50, which is in the same ballpark\\nas the score we previously achieved with the Alpaca-style prompts.\\nExercise 7.2\\nTo mask out the instructions as shown in figure 7.13, we need to make slight modifica-\\ntions to the InstructionDataset class and custom_collate_fn function. We can\\nmodify the InstructionDataset class to collect the lengths of the instructions, which\\nNew: Adjust \\n###Response to \\n<|assistant|>\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 329, 'page_label': '308'}, page_content='308 APPENDIX C Exercise solutions\\nwe will use in the collate function to locate the instruction content positions in the tar-\\ngets when we code the collate function, as follows:\\nclass InstructionDataset(Dataset):\\n    def __init__(self, data, tokenizer):\\n        self.data = data\\n        self.instruction_lengths = []    \\n        self.encoded_texts = []\\n        \\n        for entry in data:\\n            instruction_plus_input = format_input(entry)\\n            response_text = f\"\\\\n\\\\n### Response:\\\\n{entry[\\'output\\']}\"\\n            full_text = instruction_plus_input + response_text\\n            \\n            self.encoded_texts.append(\\n                tokenizer.encode(full_text)\\n            )\\n            instruction_length = ( \\n                len(tokenizer.encode(instruction_plus_input)\\n            )\\n            self.instruction_lengths.append(instruction_length)     \\n            \\n    def __getitem__(self, index):   \\n        return self.instruction_lengths[index], self.encoded_texts[index]\\n    def __len__(self):\\n        return len(self.data)\\nNext, we update the custom_collate_fn where each batch is now a tuple contain-\\ning (instruction_length, item) instead of just item due to the changes in the\\nInstructionDataset dataset. In addition, we now mask the corresponding instruc-\\ntion tokens in the target ID list:\\ndef custom_collate_fn(\\n    batch,\\n    pad_token_id=50256,\\n    ignore_index=-100,\\n    allowed_max_length=None,\\n    device=\"cpu\"\\n):\\n    batch_max_length = max(len(item)+1 for instruction_length, item in batch)\\n    inputs_lst, targets_lst = [], []         \\n    for instruction_length, item in batch:   \\n        new_item = item.copy()\\n        new_item += [pad_token_id]\\n        padded = (\\n            new_item + [pad_token_id] * (batch_max_length - len(new_item)\\n        )\\n        inputs = torch.tensor(padded[:-1])\\n        targets = torch.tensor(padded[1:])\\n        mask = targets == pad_token_id\\nSeparate list \\nfor instruction \\nlengths\\nCollects\\ninstruction\\nlengths\\nReturns both instruction\\nlengths and texts separately\\nbatch is now \\na tuple.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 330, 'page_label': '309'}, page_content='309Chapter 7\\n        indices = torch.nonzero(mask).squeeze()\\n        if indices.numel() > 1:\\n            targets[indices[1:]] = ignore_index\\n        targets[:instruction_length-1] = -100      \\n        \\n        if allowed_max_length is not None:\\n            inputs = inputs[:allowed_max_length]\\n            targets = targets[:allowed_max_length]\\n        \\n        inputs_lst.append(inputs)\\n        targets_lst.append(targets)\\n    inputs_tensor = torch.stack(inputs_lst).to(device)\\n    targets_tensor = torch.stack(targets_lst).to(device)\\n    return inputs_tensor, targets_tensor\\nWhen evaluating a model fine-tuned with this instruction masking method, it per-\\nforms slightly worse (approximately 4 poin ts using the Ollama Llama 3 method from\\nchapter 7). This is consistent with observa tions in the “Instruction Tuning With Loss\\nOver Instructions” paper (https:/ /arxiv.org/abs/2405.14394).\\nExercise 7.3\\nTo fine-tune the model on the original Stanford Alpaca dataset ( https:/ /github.com/\\ntatsu-lab/stanford_alpaca), we just have to change the file URL from\\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/\\n01_main-chapter-code/instruction-data.json\"\\nto\\nurl = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/\\nalpaca_data.json\"\\nNote that the dataset contains 52,000 entr ies (50x more than in  chapter 7), and the\\nentries are longer than the ones we worked with in chapter 7.\\n Thus, it’s highly recommended that the training be run on a GPU.\\n If you encounter out-of-memory errors, consider reducing the batch size from 8 to\\n4, 2, or 1. In addition to lowering the batch size, you may also want to consider lower-\\ning the \\nallowed_max_length from 1024 to 512 or 256.\\n Below are a few examples from the Alpaca dataset, including the generated model\\nresponses:\\nExercise 7.4\\nTo instruction fine-tune the model using LoRA, use the relevant classes and functions\\nfrom appendix E:\\nfrom appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\\nMasks all input and \\ninstruction tokens \\nin the targets\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 331, 'page_label': '310'}, page_content='310 APPENDIX C Exercise solutions\\nNext, add the following lines of code below the model loading code in section 7.5:\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters before: {total_params:,}\")\\nfor param in model.parameters():\\n    param.requires_grad = False\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters after: {total_params:,}\")\\nreplace_linear_with_lora(model, rank=16, alpha=16)\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\\nmodel.to(device)\\nNote that, on an Nvidia L4 GPU, the fine-tuning with LoRA takes 1.30 min to run on\\nan L4. On the same GPU, the original co de takes 1.80 minutes to run. So, LoRA is\\napproximately 28% faster in this case. The score, evaluated with the Ollama Llama 3\\nmethod from chapter 7, is ar ound 50, which is in the same ballpark as the original\\nmodel.\\nAppendix A\\nExercise A.1\\nThe network has two inputs and two outputs. In addition, there are two hidden layers\\nwith 30 and 20 nodes, respectively. Progra mmatically, we can calculate the number of\\nparameters as follows:\\nmodel = NeuralNetwork(2, 2)\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis returns\\n752\\nWe can also calculate this manually:\\n\\uf0a1 First hidden layer—2 inputs times 30 hidden units plus 30 bias units\\n\\uf0a1 Second hidden layer—30 incoming units times 20 nodes plus 20 bias units\\n\\uf0a1 Output layer—20 incoming nodes times 2 out put nodes plus 2 bias units\\nThen, adding all the parameters in each layer results in 2 × 30 + 30 + 30 × 20 + 20 + 20\\n× 2 + 2 = 752.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 332, 'page_label': '311'}, page_content='311Appendix A\\nExercise A.2\\nThe exact run-time results will be specific to the hardware used for this experiment. In\\nmy experiments, I observed significant speedups even for small matrix multiplications\\nas the following one when using a Google Colab instance connected to a V100 GPU:\\na = torch.rand(100, 200)\\nb = torch.rand(200, 300)\\n%timeit a@b\\nOn the CPU, this resulted in\\n63.8 μs ± 8.7 μs per loop\\nWhen executed on a GPU,\\na, b = a.to(\"cuda\"), b.to(\"cuda\")\\n%timeit a @ b\\nthe result was\\n13.8 μs ± 425 ns per loop\\nIn this case, on a V100, the computation was approximately four times faster.\\nExercise A.3\\nThe network has two inputs and two outputs. In addition, there are 2 hidden layers\\nwith 30 and 20 nodes, respectively. Programmatically, we can calculate the number of\\nparameters as follows:\\nmodel = NeuralNetwork(2, 2)\\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(\"Total number of trainable model parameters:\", num_params)\\nThis returns\\n752\\nWe can also calculate this manually as follows:\\n\\uf0a1 First hidden layer: 2 inputs times 30 hidden units plus 30 bias units\\n\\uf0a1 Second hidden layer: 30 incoming units times 20 nodes plus 20 bias units\\n\\uf0a1 Output layer: 20 incoming nodes times 2 output nodes plus 2 bias units\\nThen, adding all the parameters in ea ch layer results in 2×30+30 + 30×20+20 +\\n20×2+2 = 752.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 333, 'page_label': '312'}, page_content='312 APPENDIX C Exercise solutions\\nExercise A.4\\nThe exact run-time results will be specific to the hardware used for this experiment. In\\nmy experiments, I observed significant speed-ups even for small matrix multiplications\\nwhen using a Google Colab instance connected to a V100 GPU:\\na = torch.rand(100, 200)\\nb = torch.rand(200, 300)\\n%timeit a@b\\nOn the CPU this resulted in\\n63.8 μs ± 8.7 μs per loop\\nWhen executed on a GPU\\na, b = a.to(\"cuda\"), b.to(\"cuda\")\\n%timeit a @ b\\nThe result was\\n13.8 μs ± 425 ns per loop\\nIn this case, on a V100, the computation was approximately four times faster.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 334, 'page_label': '313'}, page_content='313\\nappendix D\\nAdding bells and whistles\\nto the training loop\\nIn this appendix, we enhance the training  function for the pretraining and fine-\\ntuning processes covered in chapters 5 to 7. In particular, it covers learning rate war-\\nmup, cosine decay, and gradient clipping. We then incorporate these techniques into\\nthe training function and pretrain an LLM. \\n To make the code self-contained, we reinitialize the model we trained in\\nchapter 5:\\nimport torch\\nfrom chapter04 import GPTModel\\nGPT_CONFIG_124M = {\\n    \"vocab_size\": 50257,         \\n    \"context_length\": 256,      \\n    \"emb_dim\": 768,          \\n    \"n_heads\": 12,           \\n    \"n_layers\": 12,          \\n    \"drop_rate\": 0.1,        \\n    \"qkv_bias\": False        \\n}\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\nmodel.eval()\\nAfter initializing the model, we need to initialize the data loaders. First, we load the\\n“The Verdict” short story:\\nVocabulary size\\nShortened context \\nlength (orig: 1024)\\nEmbedding dimension\\nNumber of attention heads\\nNumber of layers\\nDropout rate\\nQuery-key-value bias\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 335, 'page_label': '314'}, page_content='314 APPENDIX D Adding bells and whistles to the training loop\\nimport os\\nimport urllib.request\\nfile_path = \"the-verdict.txt\"\\nurl = (\\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\\n    \"main/ch02/01_main-chapter-code/the-verdict.txt\"\\n)\\nif not os.path.exists(file_path):\\n    with urllib.request.urlopen(url) as response:\\n        text_data = response.read().decode(\\'utf-8\\')\\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\\n        file.write(text_data)\\nelse:\\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\\n        text_data = file.read()\\nNext, we load the text_data into the data loaders:\\nfrom previous_chapters import create_dataloader_v1\\ntrain_ratio = 0.90\\nsplit_idx = int(train_ratio * len(text_data))\\ntorch.manual_seed(123)\\ntrain_loader = create_dataloader_v1(\\n    text_data[:split_idx],\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=True,\\n    shuffle=True,\\n    num_workers=0\\n)\\nval_loader = create_dataloader_v1(\\n    text_data[split_idx:],\\n    batch_size=2,\\n    max_length=GPT_CONFIG_124M[\"context_length\"],\\n    stride=GPT_CONFIG_124M[\"context_length\"],\\n    drop_last=False,\\n    shuffle=False,\\n    num_workers=0\\n)\\nD.1 Learning rate warmup\\nImplementing a learning rate warmup can stabilize the training  of complex models\\nsuch as LLMs. This process involves gradually increasing the learning rate from a very\\nlow initial value ( initial_lr) to a maximum value sp ecified by the user ( peak_lr).\\nStarting the training with smaller weight  updates decreases the risk of the model\\nencountering large, destabilizing updates during its training phase.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 336, 'page_label': '315'}, page_content='315D.1 Learning rate warmup\\n Suppose we plan to train an LLM for 15 epochs, starting with an initial learning\\nrate of 0.0001 and increasing it to a maximum learning rate of 0.01: \\nn_epochs = 15\\ninitial_lr = 0.0001\\npeak_lr = 0.01\\nwarmup_steps = 20\\nThe number of warmup steps is usually se t between 0.1% and 20% of the total num-\\nber of steps, which we can calculate as follows:\\ntotal_steps = len(train_loader) * n_epochs\\nwarmup_steps = int(0.2 * total_steps)      \\nprint(warmup_steps)\\nThis prints 27, meaning that we have 20 warmup steps to increase the initial learning\\nrate from 0.0001 to 0.01 in the first 27 training steps.\\n Next, we implement a simple training loop template to illustrate this warmup process:\\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\\nlr_increment = (peak_lr - initial_lr) / warmup_steps   \\nglobal_step = -1\\ntrack_lrs = []\\nfor epoch in range(n_epochs):   \\n    for input_batch, target_batch in train_loader:\\n        optimizer.zero_grad()\\n        global_step += 1\\n    \\n        if global_step < warmup_steps:            \\n            lr = initial_lr + global_step * lr_increment\\n        else:\\n            lr = peak_lr\\n        \\n        for param_group in optimizer.param_groups:   \\n            param_group[\"lr\"] = lr\\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])  \\nAfter running the preceding code, we visual ize how the learning rate was changed by\\nthe training loop to verify that the learning rate warmup works as intended:\\nimport matplotlib.pyplot as plt\\nplt.ylabel(\"Learning rate\")\\nplt.xlabel(\"Step\")\\ntotal_training_steps = len(train_loader) * n_epochs\\nplt.plot(range(total_training_steps), track_lrs);\\nplt.show()\\n20% warmup\\nThis increment is \\ndetermined by how \\nmuch we increase the \\ninital_lr in each of the \\n20 warmup steps.\\nExecutes a typical \\ntraining loop iterating \\nover the batches in the \\ntraining loader in each \\nepoch\\nUpdates the learning \\nrate if we are still in \\nthe warmup phase\\nApplies the\\ncalculated\\nlearning\\nrate to the\\noptimizer\\nIn a complete training loop, the loss and the model updates\\nwould be calculated, which are omitted here for simplicity.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 337, 'page_label': '316'}, page_content='316 APPENDIX D Adding bells and whistles to the training loop\\nThe resulting plot shows that the learning rate starts with a low value and increases for\\n20 steps until it reaches the maximum value after 20 steps (figure D.1).\\nNext, we will modify the learning rate furt her so that it decreases after reaching the\\nmaximum learning rate, which further helps improve the model training.\\nD.2 Cosine decay\\nAnother widely adopted technique for trai ning complex deep neural networks and\\nLLMs is cosine decay. This method modulates the learning rate throughout the training\\nepochs, making it follow a cosine curve after the warmup stage. \\n In its popular variant, cosine decay reduces (or decays) the learning rate to nearly\\nzero, mimicking the trajectory of a half-cosine cycle. The gradual learning decrease in\\ncosine decay aims to decelerate the pace at  which the model updates its weights. This\\nis particularly important because it helps minimize the risk of overshooting the loss\\nminima during the training process, which is essential for ensuring the stability of the\\ntraining during its later phases.\\n We can modify the training loop template by adding cosine decay:\\nimport math\\nmin_lr = 0.1 * initial_lr\\ntrack_lrs = []\\nlr_increment = (peak_lr - initial_lr) / warmup_steps\\nglobal_step = -1\\nfor epoch in range(n_epochs):\\n    for input_batch, target_batch in train_loader:\\n        optimizer.zero_grad()\\n        global_step += 1\\n        if global_step < warmup_steps:                    \\n            lr = initial_lr + global_step * lr_increment  \\n        else:                                               \\n            progress = ((global_step - warmup_steps) / \\n                        (total_training_steps - warmup_steps))\\nFigure D.1 The learning rate warmup \\nincreases the learning rate for the first \\n20 training steps. After 20 steps, the \\nlearning rate reaches the peak of 0.01 \\nand remains constant for the rest of \\nthe training.\\nApplies linear \\nwarmup\\nUses cosine \\nannealing \\nafter warmup\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 338, 'page_label': '317'}, page_content='317D.3 Gradient clipping\\n            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\\n                1 + math.cos(math.pi * progress)\\n            )\\n        \\n        for param_group in optimizer.param_groups:\\n            param_group[\"lr\"] = lr\\n        track_lrs.append(optimizer.param_groups[0][\"lr\"])\\nAgain, to verify that the learning rate has changed as intended, we plot the learning rate:\\nplt.ylabel(\"Learning rate\")\\nplt.xlabel(\"Step\")\\nplt.plot(range(total_training_steps), track_lrs)\\nplt.show()\\nThe resulting learning rate plot shows that the learning rate starts with a linear warmup\\nphase, which increases for 20 steps until it  reaches the maximum value after 20 steps.\\nAfter the 20 steps of linear warmup, cosine  decay kicks in, reducing the learning rate\\ngradually until it reaches its minimum (figure D.2).\\nD.3 Gradient clipping\\nGradient clipping is another important technique for enhancing stability during LLM\\ntraining. This method involves setting a threshold above which gradients are down-\\nscaled to a predetermined maximum magnitude. This process ensures that the updates\\nto the model’s parameters during backpropagation stay within a manageable range. \\n For example, applying the max_norm=1.0 setting within PyTorch’s clip_grad_\\nnorm_ function ensures that the norm of the gradients does not surpass 1.0. Here, the\\nterm “norm” signifies the measure of the gradient vector’s length, or magnitude,\\nwithin the model’s parameter space, specifically referring to the L2 norm, also known\\nas the Euclidean norm.\\n In mathematical terms, for a vector v composed of components v = [v1, v2, ..., vn],\\nthe L2 norm is \\nFigure D.2 The first 20 steps of \\nlinear learning rate warmup are \\nfollowed by a cosine decay, which \\nreduces the learning rate in a half-\\ncosine cycle until it reaches its \\nminimum point at the end of training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 339, 'page_label': '318'}, page_content=\"318 APPENDIX D Adding bells and whistles to the training loop\\nThis calculation method is also applied to  matrices. For instance, consider a gradient\\nmatrix given by\\nIf we want to clip these gradients to a max_norm of 1, we first compute the L2 norm of\\nthese gradients, which is\\nGiven that |G|2 = 5 exceeds our max_norm of 1, we scale down the gradients to ensure\\ntheir norm equals exactly 1. This is achi eved through a scaling factor, calculated as\\nmax_norm/|G|2 = 1/5. Consequently, the adjusted gradient matrix G' becomes\\nTo illustrate this gradient clipping proce ss, we begin by initializing a new model and\\ncalculating the loss for a training batch, si milar to the procedure in a standard train-\\ning loop:\\nfrom chapter05 import calc_loss_batch\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\nloss = calc_loss_batch(input_batch, target_batch, model, device)\\nloss.backward()\\nUpon calling the .backward() method, PyTorch calculates the loss gradients and\\nstores them in a .grad attribute for each model weight (parameter) tensor.\\n To clarify the point, we can define the following find_highest_gradient utility\\nfunction to identify the highest gr adient value by scanning all the .grad attributes of\\nthe model’s weight tensors after calling .backward():\\ndef find_highest_gradient(model):\\n    max_grad = None\\n    for param in model.parameters():\\n        if param.grad is not None:\\n            grad_values = param.grad.data.flatten()\\n            max_grad_param = grad_values.max()\\n            if max_grad is None or max_grad_param > max_grad:\\n                max_grad = max_grad_param\\n    return max_grad\\nprint(find_highest_gradient(model))\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 340, 'page_label': '319'}, page_content='319D.4 The modified training function\\nThe largest gradient value identified by the preceding code is \\ntensor(0.0411)\\nLet’s now apply gradient clipping and see how this affects the largest gradient value:\\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\nprint(find_highest_gradient(model))\\nThe largest gradient value after applying the gradient clipping with the max norm of 1\\nis substantially smaller than before:\\ntensor(0.0185)\\nD.4 The modified training function\\nFinally, we improve the train_model_simple training function (see chapter 5) by add-\\ning the three concepts introduced herein: linear warmup, cosine decay, and gradient\\nclipping. Together, these methods help stabilize LLM training.\\n The code, with the changes compared to the train_model_simple annotated, is as\\nfollows:\\nfrom chapter05 import evaluate_model, generate_and_print_sample\\ndef train_model(model, train_loader, val_loader, optimizer, device,\\n                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\\n                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\\n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\\n    tokens_seen, global_step = 0, -1\\n    peak_lr = optimizer.param_groups[0][\"lr\"]  \\n    total_training_steps = len(train_loader) * n_epochs    \\n    lr_increment = (peak_lr - initial_lr) / warmup_steps   \\n    for epoch in range(n_epochs):\\n        model.train()\\n        for input_batch, target_batch in train_loader:\\n            optimizer.zero_grad()\\n            global_step += 1\\n            if global_step < warmup_steps:  \\n                lr = initial_lr + global_step * lr_increment  \\n            else:\\n                progress = ((global_step - warmup_steps) / \\n                            (total_training_steps - warmup_steps))\\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\\n                    1 + math.cos(math.pi * progress))\\nRetrieves the initial learning rate from the optimizer,\\nassuming we use it as the peak learning rate\\nCalculates the \\ntotal number of \\niterations in the \\ntraining process\\nCalculates the learning \\nrate increment during \\nthe warmup phase\\nAdjusts the \\nlearning rate \\nbased on the \\ncurrent phase \\n(warmup or \\ncosine \\nannealing)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 341, 'page_label': '320'}, page_content='320 APPENDIX D Adding bells and whistles to the training loop\\n            for param_group in optimizer.param_groups:  \\n                param_group[\"lr\"] = lr\\n            track_lrs.append(lr)\\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\\n            loss.backward()\\n            if global_step > warmup_steps:        \\n                torch.nn.utils.clip_grad_norm_(\\n                    model.parameters(), max_norm=1.0\\n                )\\n                     \\n            optimizer.step() \\n            tokens_seen += input_batch.numel()\\n            if global_step % eval_freq == 0:\\n                train_loss, val_loss = evaluate_model(\\n                    model, train_loader, val_loader,\\n                    device, eval_iter\\n                )\\n                train_losses.append(train_loss)\\n                val_losses.append(val_loss)\\n                track_tokens_seen.append(tokens_seen)\\n                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\\n                      f\"Train loss {train_loss:.3f}, \"\\n                      f\"Val loss {val_loss:.3f}\"\\n                )\\n                \\n        generate_and_print_sample(\\n            model, tokenizer, device, start_context\\n        )\\n    return train_losses, val_losses, track_tokens_seen, track_lrs\\nAfter defining the train_model function, we can use it in a similar fashion to train the\\nmodel compared to the train_model_simple method we used for pretraining:\\nimport tiktoken\\ntorch.manual_seed(123)\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.to(device)\\npeak_lr = 5e-4\\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nn_epochs = 15\\ntrain_losses, val_losses, tokens_seen, lrs = train_model(\\n    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\\n    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\\n    tokenizer=tokenizer, warmup_steps=warmup_steps, \\n    initial_lr=1e-5, min_lr=1e-5\\n)\\nApplies the calculated \\nlearning rate to the optimizer\\nApplies gradient clipping \\nafter the warmup phase \\nto avoid exploding \\ngradients\\nEverything below here \\nremains unchanged \\ncompared to the \\ntrain_model_simple \\nfunction used in \\nchapter 5.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 342, 'page_label': '321'}, page_content='321D.4 The modified training function\\nThe training will take about 5 minutes to complete on a MacBook Air or similar lap-\\ntop and prints the following outputs:\\nEp 1 (Iter 000000): Train loss 10.934, Val loss 10.939\\nEp 1 (Iter 000005): Train loss 9.151, Val loss 9.461 \\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\\nEp 2 (Iter 000010): Train loss 7.949, Val loss 8.184 \\nEp 2 (Iter 000015): Train loss 6.362, Val loss 6.876 \\nEvery effort moves you,,,,,,,,,,,,,,,,,,, the,,,,,,,,, the,,,,,,,,,,, \\nthe,,,,,,,, \\n... \\nEp 15 (Iter 000130): Train loss 0.041, Val loss 6.915 \\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him \\nvindicated--and by me!\"  He laughed again, and threw back his head to look up \\nat the sketch of the donkey. \"There were days when I\\nLike pretraining, the model begins to overfi t after a few epochs since it is a very small\\ndataset, and we iterate over it multiple ti mes. Nonetheless, we can see that the func-\\ntion is working since it minimizes the training set loss.\\n Readers are encouraged to train the mode l on a larger text dataset and compare\\nthe results obtained with this more sophisti cated training function to the results that\\ncan be obtained with the train_model_simple function.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 343, 'page_label': '322'}, page_content='322\\nappendix E\\nParameter-efficient\\nfine-tuning with LoRA\\nLow-rank adaptation (LoRA) is one of the most widely used techniques for parameter-\\nefficient fine-tuning. The following discussion is based on the spam classification fine-\\ntuning example given in chapter 6. However, LoRA fine-tuning is also applicable to\\nthe supervised instruction fine-tuning discussed in chapter 7.\\nE.1 Introduction to LoRA\\nLoRA is a technique that adap ts a pretrained model to be tter suit a specific, often\\nsmaller dataset by adjusting only a small subset of the model’s weight parameters.\\nThe “low-rank” aspect refers to the mathematical concept of limiting model adjust-\\nments to a smaller dimensional subspace of  the total weight parameter space. This\\neffectively captures the most influential directions of the weight parameter changes\\nduring training. The LoRA method is useful and popular because it enables effi-\\ncient fine-tuning of large models on task -specific data, signif icantly cutting down\\non the computational costs and resources usually required for fine-tuning.\\n Suppose a large weight matrix W is associated with a specific layer. LoRA can be\\napplied to all linear layers in an LLM. Ho wever, we focus on a single layer for illus-\\ntration purposes.\\n When training deep neural networks , during backpropag ation, we learn a ΔW\\nmatrix, which contains information on ho w much we want to update the original\\nweight parameters to minimize the loss fu nction during training. Hereafter, I use\\nthe term “weight” as shorthand for the model’s weight parameters.\\n In regular training and fine-tuning, the weight update is defined as follows:\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 344, 'page_label': '323'}, page_content='323E.1 Introduction to LoRA\\nThe LoRA method, proposed by Hu et al. ( https:/ /arxiv.org/abs/2106.09685), offers\\na more efficient alte rnative to computing the weight updates ΔW by learning an\\napproximation of it:\\nwhere A and B are two matrices much smaller than W, and AB represents the matrix\\nmultiplication product between A and B. \\n Using LoRA, we can then reformulate the weight update we defined earlier:\\nFigure E.1 illustrates the weight update formulas for full fine-tuning and LoRA side\\nby side.\\nIf you paid close attention, you might have  noticed that the visual representations of\\nfull fine-tuning and LoRA in figure E.1 diffe r slightly from the earlier presented for-\\nmulas. This variation is attributed to the distributive law of matrix multiplication,\\nwhich allows us to separate the original  and updated weights rather than combine\\nthem. For example, in the case of regular fine-tuning with x as the input data, we can\\nexpress the computation as \\nPretrained\\nweights\\nW\\nWeight\\nupdate\\nΔW\\nOutputs\\nPretrained\\nweights\\nW\\nInputs\\nd\\nInputs\\nWeight update in regular ﬁne-tuning Weight update in LoRA\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nLoRA matrices andAB\\napproximate the weight\\nupdate matrix .ΔW\\nThe weight\\nparameters in any\\nof the neural\\nnetwork layers\\nThe values by which the\\nweights are updated\\nduring training\\nFigure E.1 A comparison between weight update methods: regular fine-tuning and LoRA. Regular fine-tuning \\ninvolves updating the pretrained weight matrix W directly with ΔW (left). LoRA uses two smaller matrices, A and \\nB, to approximate ΔW, where the product AB is added to W, and r denotes the inner dimension, a tunable \\nhyperparameter (right).\\nPretrained\\nweights\\nW\\nWeight\\nupdate\\nΔW\\nOutputs\\nPretrained\\nweights\\nW\\nInputs\\nd\\nInputs\\nWeight update in regular ﬁne-tuning Weight update in LoRA\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nLoRA matrices andAB\\napproximate the weight\\nupdate matrix .ΔW\\nThe weight\\nparameters in any\\nof the neural\\nnetwork layers\\nThe values by which the\\nweights are updated\\nduring training\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 345, 'page_label': '324'}, page_content='324 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nSimilarly, we can write the following for LoRA:\\nBesides reducing the number of weights to update during training, the ability to keep\\nthe LoRA weight matrices separate from the original model weights makes LoRA even\\nmore useful in practice. Prac tically, this allows for th e pretrained model weights to\\nremain unchanged, with the LoRA matrices being applied dynamically after training\\nwhen using the model.\\n Keeping the LoRA weights separate is very  useful in practice because it enables\\nmodel customization without needing to store multiple complete versions of an LLM.\\nThis reduces storage requirements and improv es scalability, as only the smaller LoRA\\nmatrices need to be adjusted and saved when we customize LLMs for each specific cus-\\ntomer or application.\\n Next, let’s see how LoRA can be used to fine-tune an LLM for spam classification,\\nsimilar to the fine-tuning example in chapter 6.\\nE.2 Preparing the dataset\\nBefore applying LoRA to the spam classifica tion example, we must load the dataset\\nand pretrained model we will work with. Th e code here repeats the data preparation\\nfrom chapter 6. (Instead of repeating the code, we could open and run the chapter 6\\nnotebook and insert the LoRA code from section E.4 there.)\\n First, we download the dataset and save it as CSV files.\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom ch06 import (\\n    download_and_unzip_spam_data,\\n    create_balanced_dataset,\\n    random_split\\n)\\nurl = \\\\ \\n\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\\nzip_path = \"sms_spam_collection.zip\"\\nextracted_path = \"sms_spam_collection\"\\ndata_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\\ndf = pd.read_csv(\\n    data_file_path, sep=\"\\\\t\", header=None, names=[\"Label\", \"Text\"]\\n)\\nbalanced_df = create_balanced_dataset(df)\\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\\ntrain_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\\ntrain_df.to_csv(\"train.csv\", index=None)\\nListing E.1 Downloading and preparing the dataset\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 346, 'page_label': '325'}, page_content='325E.2 Preparing the dataset\\nvalidation_df.to_csv(\"validation.csv\", index=None)\\ntest_df.to_csv(\"test.csv\", index=None)\\nNext, we create the SpamDataset instances.\\nimport torch\\nfrom torch.utils.data import Dataset\\nimport tiktoken\\nfrom chapter06 import SpamDataset\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\ntrain_dataset = SpamDataset(\"train.csv\", max_length=None, \\n    tokenizer=tokenizer\\n)\\nval_dataset = SpamDataset(\"validation.csv\", \\n    max_length=train_dataset.max_length, tokenizer=tokenizer\\n)\\ntest_dataset = SpamDataset(\\n    \"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\\n)\\nAfter creating the PyTorch dataset objects, we instantiate the data loaders.\\nfrom torch.utils.data import DataLoader\\nnum_workers = 0\\nbatch_size = 8\\ntorch.manual_seed(123)\\ntrain_loader = DataLoader(\\n    dataset=train_dataset,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=num_workers,\\n    drop_last=True,\\n)\\nval_loader = DataLoader(\\n    dataset=val_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\ntest_loader = DataLoader(\\n    dataset=test_dataset,\\n    batch_size=batch_size,\\n    num_workers=num_workers,\\n    drop_last=False,\\n)\\nListing E.2 Instantiating PyTorch datasets\\nListing E.3 Creating PyTorch data loaders\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 347, 'page_label': '326'}, page_content='326 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nAs a verification step, we iterate through the data loaders and check that the batches\\ncontain eight training examples each, wher e each training example consists of 120\\ntokens:\\nprint(\"Train loader:\")\\nfor input_batch, target_batch in train_loader:\\n    pass\\nprint(\"Input batch dimensions:\", input_batch.shape)\\nprint(\"Label batch dimensions\", target_batch.shape)\\nThe output is \\nTrain loader:\\nInput batch dimensions: torch.Size([8, 120])\\nLabel batch dimensions torch.Size([8])\\nLastly, we print the total number of batches in each dataset:\\nprint(f\"{len(train_loader)} training batches\")\\nprint(f\"{len(val_loader)} validation batches\")\\nprint(f\"{len(test_loader)} test batches\")\\nIn this case, we have the following number of batches per dataset:\\n130 training batches\\n19 validation batches\\n38 test batches\\nE.3 Initializing the model\\nWe repeat the code from chapter 6 to lo ad and prepare the pretrained GPT model.\\nWe begin by downloading the model weights and loading them into the GPTModel\\nclass.\\nfrom gpt_download import download_and_load_gpt2\\nfrom chapter04 import GPTModel\\nfrom chapter05 import load_weights_into_gpt\\nCHOOSE_MODEL = \"gpt2-small (124M)\"\\nINPUT_PROMPT = \"Every effort moves\"\\nBASE_CONFIG = {\\n    \"vocab_size\": 50257,        \\n    \"context_length\": 1024,     \\n    \"drop_rate\": 0.0,           \\n    \"qkv_bias\": True            \\n}\\nListing E.4 Loading a pretrained GPT model\\nVocabulary size\\nContext length\\nDropout rate\\nQuery-key-value bias\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 348, 'page_label': '327'}, page_content='327E.3 Initializing the model\\nmodel_configs = {\\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\\n}\\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\\nsettings, params = download_and_load_gpt2(\\n    model_size=model_size, models_dir=\"gpt2\"\\n)\\nmodel = GPTModel(BASE_CONFIG)\\nload_weights_into_gpt(model, params)\\nmodel.eval()\\nTo ensure that the model was loaded correc ted, let’s double-check that it generates\\ncoherent text:\\nfrom chapter04 import generate_text_simple\\nfrom chapter05 import text_to_token_ids, token_ids_to_text\\ntext_1 = \"Every effort moves you\"\\ntoken_ids = generate_text_simple(\\n    model=model,\\n    idx=text_to_token_ids(text_1, tokenizer),\\n    max_new_tokens=15,\\n    context_size=BASE_CONFIG[\"context_length\"]\\n)\\nprint(token_ids_to_text(token_ids, tokenizer))\\nThe following output shows that the model ge nerates coherent text, which is an indi-\\ncator that the model weights are loaded correctly:\\nEvery effort moves you forward.\\nThe first step is to understand the importance of your work\\nNext, we prepare the model for classification  fine-tuning, similar to chapter 6, where\\nwe replace the output layer:\\ntorch.manual_seed(123)\\nnum_classes = 2\\nmodel.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\nLastly, we calculate the initial classification  accuracy of the not-fine-tuned model (we\\nexpect this to be around 50%, which means that the model is not able to distinguish\\nbetween spam and nonspam messages yet reliably):\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 349, 'page_label': '328'}, page_content='328 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nfrom chapter06 import calc_accuracy_loader\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nThe initial prediction accuracies are \\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nE.4 Parameter-efficient fine-tuning with LoRA\\nNext, we modify and fine-tune the LLM usin g LoRA. We begin by initializing a LoRA-\\nLayer that creates the matrices A and B, along with the alpha scaling factor and the\\nrank (r) setting. This layer can accept an in put and compute the corresponding out-\\nput, as illustrated in figure E.2.\\nIn code, this LoRA layer can be implemented as follows.\\n \\n \\n \\n \\nInputs\\nOutputs\\nThe inner dimension r\\nis a hyperparameter.\\nInitialize LoRA matrices\\nABand , which approximate\\nthe weight update matrix ΔW\\n.\\nFigure E.2 The LoRA matrices A and B are \\napplied to the layer inputs and are involved in \\ncomputing the model outputs. The inner \\ndimension r of these matrices serves as a \\nsetting that adjusts the number of trainable \\nparameters by varying the sizes of A and B.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 350, 'page_label': '329'}, page_content='329E.4 Parameter-efficient fine-tuning with LoRA\\nimport math\\nclass LoRALayer(torch.nn.Module):\\n    def __init__(self, in_dim, out_dim, rank, alpha):\\n        super().__init__()\\n        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\\n        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))   \\n        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\\n        self.alpha = alpha\\n    def forward(self, x):\\n        x = self.alpha * (x @ self.A @ self.B)\\n        return x\\nThe rank governs the inner dimension of matrices A and B. Essentially, this setting\\ndetermines the number of extra parameters  introduced by LoRA, which creates bal-\\nance between the adaptability of the mode l and its efficiency via the number of\\nparameters used.\\n The other important setting, alpha, functions as a scaling factor for the output\\nfrom the low-rank adaptation . It primarily dictates the degree to which the output\\nfrom the adapted layer can affect the original layer’s output. This can be seen as a way\\nto regulate the effect of the low-rank adaptation on the layer’s output. The LoRALayer\\nclass we have implemented so far enables us to transform the inputs of a layer.\\n In LoRA, the typical goal is to substitute existing Linear layers, allowing weight\\nupdates to be applied directly to the pre-ex isting pretrained weights, as illustrated in\\nfigure E.3.\\nListing E.5 Implementing a LoRA layer\\nThe same initialization\\nused for Linear layers\\nin PyTorch\\nThe original weights\\nin a given layer of\\na model\\nComputing the outputs involves\\nboth the original weights and\\nthe LoRA weights\\nPretrained\\nweights\\nW\\nInputs\\nOutputs\\nLoRA matrices and ,AB which\\napproximate the weight\\nupdate matrix ΔW\\nFigure E.3 The integration of LoRA into a model layer. The original pretrained weights (W) \\nof a layer are combined with the outputs from LoRA matrices (A and B), which approximate \\nthe weight update matrix (ΔW). The final output is calculated by adding the output of the \\nadapted layer (using LoRA weights) to the original output.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 351, 'page_label': '330'}, page_content='330 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nTo integrate the original Linear layer weights, we now create a LinearWithLoRA layer.\\nThis layer utilizes the previously implemented LoRALayer and is designed to replace\\nexisting Linear layers within a neural network, such as the self-attention modules or\\nfeed-forward modules in the GPTModel.\\nclass LinearWithLoRA(torch.nn.Module):\\n    def __init__(self, linear, rank, alpha):\\n        super().__init__()\\n        self.linear = linear\\n        self.lora = LoRALayer(\\n            linear.in_features, linear.out_features, rank, alpha\\n        )\\n    def forward(self, x):\\n        return self.linear(x) + self.lora(x)\\nThis code combines a standard Linear layer with the LoRALayer. The forward method\\ncomputes the output by adding the result s from the original linear layer and the\\nLoRA layer.\\n Since the weight matrix B (self.B in LoRALayer) is initialized with zero values, the\\nproduct of matrices A and B results in a zero matrix. This  ensures that the multiplica-\\ntion does not alter the original weights, as adding zero does not change them.\\n To apply LoRA to the earlier defined GPTModel, we introduce a replace_linear_\\nwith_lora function. This function  will swap all existing Linear layers in the model\\nwith the newly created LinearWithLoRA layers:\\ndef replace_linear_with_lora(model, rank, alpha):\\n    for name, module in model.named_children():\\n        if isinstance(module, torch.nn.Linear):    \\n            setattr(model, name, LinearWithLoRA(module, rank, alpha))\\n        else:   \\n            replace_linear_with_lora(module, rank, alpha)\\nWe have now implemented all the necessary code to replace the Linear layers in the\\nGPTModel with the newly developed LinearWithLoRA layers for parameter-efficient\\nfine-tuning. Next, we will apply the LinearWithLoRA upgrade to all Linear layers\\nfound in the multihead attention, feed-for ward modules, and the output layer of the\\nGPTModel, as shown in figure E.4.\\n \\n \\n \\n \\nListing E.6 Replacing a LinearWithLora layer with Linear layers\\nReplaces the Linear layer \\nwith LinearWithLoRA\\nRecursively applies the same\\nfunction to child modules\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 352, 'page_label': '331'}, page_content='331E.4 Parameter-efficient fine-tuning with LoRA\\nBefore we apply the LinearWithLoRA layer upgrades, we first freeze the original model\\nparameters:\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters before: {total_params:,}\")\\nfor param in model.parameters():\\n    param.requires_grad = False\\nWe update the layersLinear\\nwith layers.LinearWithLoRA\\nThe GPT model we\\nimplemented and\\nused in previous\\nchapters.\\nGPT\\nmodel\\nMasked multihead\\nattention\\nLayerNorm 2\\nFeed forward\\nLayerNorm 1\\nDropout\\nDropout\\nToken embedding layer\\nTokenized text\\nFinal LayerNorm\\nLinear output layer\\n12\\nDropout\\nPositional embedding layer\\nEvery effort moves you\\nTransformer\\nblock\\nFigure E.4 The architecture of the GPT model. It highlights the parts of the model where Linear layers are \\nupgraded to LinearWithLoRA layers for parameter-efficient fine-tuning.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 353, 'page_label': '332'}, page_content='332 APPENDIX E Parameter-efficient fine-tuning with LoRA\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable parameters after: {total_params:,}\")\\nNow, we can see that none of the 124 million model parameters are trainable:\\nTotal trainable parameters before: 124,441,346\\nTotal trainable parameters after: 0\\nNext, we use the replace_linear_with_lora to replace the Linear layers:\\nreplace_linear_with_lora(model, rank=16, alpha=16)\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\\nAfter adding the LoRA layers, the number of trainable parameters is as follows:\\nTotal trainable LoRA parameters: 2,666,528\\nAs we can see, we reduced the number of  trainable parameters by almost 50× when\\nusing LoRA. A rank and alpha of 16 are good default choices, but it is also common to\\nincrease the rank parameter, which in turn increases the number of trainable parame-\\nters. Alpha is usually chosen to be half, double, or equal to the rank.\\n Let’s verify that the layers have been mo dified as intended by  printing the model\\narchitecture:\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\nprint(model)\\nThe output is\\nGPTModel(\\n  (tok_emb): Embedding(50257, 768)\\n  (pos_emb): Embedding(1024, 768)\\n  (drop_emb): Dropout(p=0.0, inplace=False)\\n  (trf_blocks): Sequential(\\n    ...\\n    (11): TransformerBlock(\\n      (att): MultiHeadAttention(\\n        (W_query): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (W_key): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (W_value): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 354, 'page_label': '333'}, page_content='333E.4 Parameter-efficient fine-tuning with LoRA\\n        (out_proj): LinearWithLoRA(\\n          (linear): Linear(in_features=768, out_features=768, bias=True)\\n          (lora): LoRALayer()\\n        )\\n        (dropout): Dropout(p=0.0, inplace=False)\\n      )\\n      (ff): FeedForward(\\n        (layers): Sequential(\\n          (0): LinearWithLoRA(\\n            (linear): Linear(in_features=768, out_features=3072, bias=True)\\n            (lora): LoRALayer()\\n          )\\n          (1): GELU()\\n          (2): LinearWithLoRA(\\n            (linear): Linear(in_features=3072, out_features=768, bias=True)\\n            (lora): LoRALayer()\\n          )\\n        )\\n      )\\n      (norm1): LayerNorm()\\n      (norm2): LayerNorm()\\n      (drop_resid): Dropout(p=0.0, inplace=False)\\n    )\\n  )\\n  (final_norm): LayerNorm()\\n  (out_head): LinearWithLoRA(\\n    (linear): Linear(in_features=768, out_features=2, bias=True)\\n    (lora): LoRALayer()\\n  )\\n)\\nThe model now includes the new LinearWithLoRA layers, which themselves consist of\\nthe original Linear layers, set to nontrainable, and the new LoRA layers, which we will\\nfine-tune.\\n Before we begin fine-tuning the model, let’s calculate the initial classification\\naccuracy:\\ntorch.manual_seed(123)\\ntrain_accuracy = calc_accuracy_loader(\\n    train_loader, model, device, num_batches=10\\n)\\nval_accuracy = calc_accuracy_loader(\\n    val_loader, model, device, num_batches=10\\n)\\ntest_accuracy = calc_accuracy_loader(\\n    test_loader, model, device, num_batches=10\\n)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 355, 'page_label': '334'}, page_content='334 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nThe resulting accuracy values are \\nTraining accuracy: 46.25%\\nValidation accuracy: 45.00%\\nTest accuracy: 48.75%\\nThese accuracy values are iden tical to the values from chapter 6. This result occurs\\nbecause we initialized the LoRA matrix B with zeros. Consequently, the product of\\nmatrices AB results in a zero matrix. This ensu res that the multiplication does not\\nalter the original weights since adding zero does not change them.\\n Now let’s move on to the exciting part —fine-tuning the model using the training\\nfunction from chapter 6. The training takes about 15 minutes on an M3 MacBook Air\\nlaptop and less than half a minute on a V100 or A100 GPU.\\nimport time\\nfrom chapter06 import train_classifier_simple\\nstart_time = time.time()\\ntorch.manual_seed(123)\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\\nnum_epochs = 5\\ntrain_losses, val_losses, train_accs, val_accs, examples_seen = \\\\\\n    train_classifier_simple(\\n        model, train_loader, val_loader, optimizer, device,\\n        num_epochs=num_epochs, eval_freq=50, eval_iter=5,\\n        tokenizer=tokenizer\\n    )\\nend_time = time.time()\\nexecution_time_minutes = (end_time - start_time) / 60\\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\\nThe output we see during the training is\\nEp 1 (Step 000000): Train loss 3.820, Val loss 3.462 \\nEp 1 (Step 000050): Train loss 0.396, Val loss 0.364 \\nEp 1 (Step 000100): Train loss 0.111, Val loss 0.229 \\nTraining accuracy: 97.50% | Validation accuracy: 95.00% \\nEp 2 (Step 000150): Train loss 0.135, Val loss 0.073 \\nEp 2 (Step 000200): Train loss 0.008, Val loss 0.052 \\nEp 2 (Step 000250): Train loss 0.021, Val loss 0.179 \\nTraining accuracy: 97.50% | Validation accuracy: 97.50%\\nEp 3 (Step 000300): Train loss 0.096, Val loss 0.080 \\nEp 3 (Step 000350): Train loss 0.010, Val loss 0.116 \\nTraining accuracy: 97.50% | Validation accuracy: 95.00% \\nEp 4 (Step 000400): Train loss 0.003, Val loss 0.151 \\nEp 4 (Step 000450): Train loss 0.008, Val loss 0.077 \\nEp 4 (Step 000500): Train loss 0.001, Val loss 0.147 \\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nListing E.7 Fine-tuning a model with LoRA layers\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 356, 'page_label': '335'}, page_content='335E.4 Parameter-efficient fine-tuning with LoRA\\nEp 5 (Step 000550): Train loss 0.007, Val loss 0.094 \\nEp 5 (Step 000600): Train loss 0.000, Val loss 0.056 \\nTraining accuracy: 100.00% | Validation accuracy: 97.50%\\nTraining completed in 12.10 minutes. \\nTraining the model with LoRA took longer than training it withou t LoRA (see chap-\\nter 6) because the LoRA layers introduce an  additional computation during the for-\\nward pass. However, for larger models, wh ere backpropagation becomes more costly,\\nmodels typically train faster with LoRA than without it.\\n As we can see, the model received perfect training and very high validation accuracy.\\nLet’s also visualize the loss curves to better see whether the training has converged:\\nfrom chapter06 import plot_values\\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\\nplot_values(\\n    epochs_tensor, examples_seen_tensor, \\n    train_losses, val_losses, label=\"loss\"\\n)\\nFigure E.5 plots the results.\\nIn addition to evaluating the model based on the loss curves, let’s also calculate the\\naccuracies on the full training, validati on, and test set (during the training, we\\napproximated the training and validation set accuracies from five batches via the\\neval_iter=5 setting):\\ntrain_accuracy = calc_accuracy_loader(train_loader, model, device)\\nval_accuracy = calc_accuracy_loader(val_loader, model, device)\\ntest_accuracy = calc_accuracy_loader(test_loader, model, device)\\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\\nFigure E.5 The training and \\nvalidation loss curves over six epochs \\nfor a machine learning model. \\nInitially, both training and validation \\nloss decrease sharply and then they \\nlevel off, indicating the model is \\nconverging, which means that it is \\nnot expected to improve noticeably \\nwith further training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 357, 'page_label': '336'}, page_content='336 APPENDIX E Parameter-efficient fine-tuning with LoRA\\nThe resulting accuracy values are \\nTraining accuracy: 100.00%\\nValidation accuracy: 96.64%\\nTest accuracy: 98.00%\\nThese results show that the model perfor ms well across training, validation, and test\\ndatasets. With a training accuracy of 100% , the model has perfectly learned the train-\\ning data. However, the slightly lower vali dation and test accuracies (96.64% and\\n97.33%, respectively) suggest a small degree of overfitting, as the model does not gen-\\neralize quite as well on unseen data compared  to the training set. Overall, the results\\nare very impressive, considering we fine-tuned only a relatively small number of model\\nweights (2.7 million LoRA weights instead of the original 124 million model weights).\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 358, 'page_label': '337'}, page_content='337\\nindex\\nSymbols\\n[BOS] (beginning of sequence) token 32\\n[EOS] (end of sequence) token 32\\n[PAD] (padding) token 32\\n@ operator 261\\n%timeit command 282\\n<|endoftext|> token 34\\n<|unk|> tokens 29 – 31, 34\\n== comparison operator 277\\nNumerics\\n04_preference-tuning-with-dpo folder 247\\n124M parameter 161\\n355M parameter 227\\nA\\nAdamW optimizer 148 , 294\\nAI (artificial intelligence) 252\\nallowed_max_length 224 , 233, 309\\nAlpaca dataset 233 , 296\\nalpha scaling factor 328\\narchitectures, transformer 7 – 10\\nargmax function 134 , 152– 155, 190, 277\\narXiv 248\\nassign utility function 165\\nattention mechanisms\\ncausal 74– 82\\ncoding 50, 54\\nimplementing self-attention with trainable \\nweights 64– 74\\nmulti-head attention 82– 91\\nproblem with modeling long sequences 52\\nself-attention mechanism 55– 64\\nattention scores 57\\nattention weights, computing step by step\\n65– 70\\nattn_scores 71\\nautograd engine 264\\nautomatic differentiation 263 – 265\\nengine 252\\npartial derivatives and gradients 263\\nautoregressive model 13\\nAxolotl 249\\nB\\nbackpropagation 137\\n.backward() method 112 , 318\\nBahdanau attention mechanism 54\\nbase model 7\\nbatch normalization layers 276\\nbatch_size 233\\nBERT (bidirectional encoder representations from \\ntransformers) 8\\nBPE (byte pair encoding) 32 – 35\\nC\\ncalc_accuracy_loader function 192\\ncalc_loss_batch function 145 , 193– 194\\ncalc_loss_loader function 144 , 194\\ncalculating, training and validation 140 , 142\\nCausalAttention class 80 – 81, 86, 90\\nmodule 83– 84\\nobject 86\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 359, 'page_label': '338'}, page_content='INDEX338\\ncausal attention mask 190\\ncausal attention mechanism 74 – 82\\ncfg dictionary 115 , 119\\nclassification\\nfine-tuning\\ncategories of 170\\npreparing dataset 172 – 175\\nfine-tuning for\\nadding classification head 183 – 190\\ncalculating classification loss and \\naccuracy 190 – 194\\nsupervised data 195 – 200\\nusing LLM as spam classifier 200\\ntasks 7\\nclassify_review function 200\\nclip_grad_norm_ function 317\\nclipping, gradient 317\\ncode for data loaders 301\\ncoding\\nattention mechanisms 54\\nGPT model 117– 122\\ncollate function 211\\ncomputation graphs 261\\ncompute_accuracy function 277 – 278\\ncomputing gradients 258\\nconnections, shortcut 109 – 113\\ncontext, adding special tokens 29 – 32\\ncontext_length 47 , 95\\ncontext vectors 57 , 64, 85\\nconversational performance 236\\nconverting tokens into token IDs 24 – 29\\ncosine decay 313 , 316\\ncreate_dataloader_v1 function 39\\ncross_entropy function 138 – 139\\nCUDA_VISIBLE_DEVICES environment \\nvariable 286\\ncustom_collate_draft_1 215\\ncustom_collate_draft_2 218\\ncustom_collate_fn function 224 , 308\\nD\\ndata, sampling with sliding window 35 – 41\\nDataFrame 173\\ndata list 207 , 209\\nDataLoader class 38 , 211, 224, 270– 272\\ndata loaders 175 – 181\\ncode for 301\\ncreating for instruction dataset\\n224– 226\\nefficient 270– 274\\nDataset class 38 , 177, 270– 272, 274\\ndatasets\\ndownloading 207\\npreparing 324\\nutilizing large 10\\nDDP (DistributedDataParallel) strategy 282\\nddp_setup function 286\\ndecode method 27 , 33– 34\\ndecoder 52\\ndecoding strategies to control randomness\\n151– 159\\nmodifying text generation function 157\\ntemperature scaling 152– 155\\ntop-k sampling 155\\ndeep learning 253\\nlibrary 252\\ndestroy_process_group function 284\\ndevice variable 224\\ndim parameter 101 – 102\\nDistributedDataParallel class 284\\nDistributedSampler 283 – 284\\nDolma: An Open Corpus of Three Trillion Tokens \\nfor LLM Pretraining Research (Soldaini \\net al.) 11\\ndot products 58\\nd_out argument 90 , 301\\ndownload_and_load_gpt2 function 161 , 163, 182\\ndrop_last parameter 273\\ndropout\\ndefined 78\\nlayers 276\\ndrop_rate 95\\n.dtype attribute 259\\nDummyGPTClass 98\\nDummyGPTModel 95 , 97– 98, 117\\nDummyLayerNorm 97 , 99, 117\\nplaceholder 100\\nDummyTransformerBlock 97 , 117\\nE\\nemb_dim 95\\nEmbedding layer 161\\nembedding size 46\\nemergent behavior 14\\nencode method 27 , 33, 37\\nencoder 52\\nencoding word positions 43 – 47\\nentry dictionary 209\\neps variable 103\\n.eval() mode 126\\neval_iter value 200\\nevaluate_model function 147 – 148, 196\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 360, 'page_label': '339'}, page_content=\"INDEX 339\\nF\\nfeedforward layer 267\\nFeedForward module 107 – 108, 113\\nfeed forward network, implementing with GELU \\nactivations 105 – 109\\nfind_highest_gradient function 318\\nfine-tuning\\ncategories of 170\\ncreating data loaders for instruction \\ndataset 224– 226\\nevaluating fine-tuned LLMs 238– 247\\nextracting and saving responses 233– 238\\nfor classification 169\\nadding classification head 183 – 190\\ncalculating classification loss and \\naccuracy 190 – 194\\ndata loaders 175 – 181\\nfine-tuning model on supervised data\\n195– 200\\ninitializing model with pretrained \\nweights 181\\npreparing dataset 172 – 175\\nusing LLM as spam classifier 200\\ninstruction data 230– 233\\ninstruction fine-tuning, overview 205\\nLLMs, to follow instructions 204\\norganizing data into training batches 211– 223\\nsupervised instruction fine-tuning, preparing \\ndataset for 207– 211\\nFineWeb Dataset 295\\nfirst_batch variable 39\\nformat_input function 209 – 210, 242, 307\\nforward method 97 , 109, 267, 330\\nfoundation model 7\\nfully connected layer 267\\nfunctools standard library 224\\nG\\nGELU (Gaussian error linear unit) 105 , 107, 293\\nactivation function 104, 111\\nGenAI (generative AI) 3\\ngenerate_and_print_sample function 147 – 148, \\n151, 154\\ngenerate function 157 , 159, 167, 228, 234– 235, \\n237, 305\\ngenerate_model_scores function 246\\ngenerate_simple function 157 , 159\\ngenerate_text_simple function 125 – 126, 131– 132, \\n134, 148, 151– 153\\ngenerative text models, evaluating 129\\n__getitem__ method 271\\nGoogle Colab 257\\nGPT-2 94\\nmodel 230\\ntokenizer 176\\ngpt2-medium355M-sft.pth file 238\\nGPT-3 11 , 94\\nGPT-4 239\\nGPT_CONFIG_124M dictionary 95 , 97, 107, \\n116– 117, 120, 127, 130\\nGPTDatasetV1 class 38 – 39\\ngpt_download.py Python module 161\\nGPT (Generative Pre-trained Transformer) 8 , \\n18, 93\\narchitecture 12– 14\\ncoding 117– 122\\ncoding architecture 93– 99\\nimplementing feed forward network with GELU \\nactivations 105– 109\\nimplementing from scratch, shortcut \\nconnections\\n109– 113\\nimplementing from scratch to generate text\\n92, 122\\nimplementing model from scratch 99– 105, \\n113– 116\\nGPTModel 119 , 121– 122, 133, 146, 182, 330\\nclass 122, 130, 182, 326\\ncode 141\\nimplementation 166\\ninstance 131, 159, 164– 167\\nGPUs (graphics processing units), optimizing \\ntraining performance with 279 – 288\\n.grad attribute 318\\ngrad_fn value 268\\ngrad function 264\\ngradient clipping 313 , 317\\ngradients 263\\ngreedy decoding 125 , 152\\nI\\ninformation leakage 76\\n__init__ constructor 71 , 81, 119, 266– 267, 271\\ninitializing model 326\\ninitial_lr 314\\ninit_process_group function 284\\ninput_chunk tensor 38\\ninput_embeddings 47\\n'input' object 208\\ninstruction data, fine-tuning LLMs on 230 – 233\\ninstruction dataset 205\\nInstructionDataset class 212 , 224, 308\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 361, 'page_label': '340'}, page_content=\"INDEX340\\ninstruction fine-tuning 7 , 170, 322\\ninstruction following, creating data loaders for \\ninstruction dataset 224– 226\\n'instruction' object 208\\ninstruction–response pairs 207\\nloading pretrained LLMs 226– 229\\noverview 205\\nK\\nkeepdim parameter 101\\nL\\nLayerNorm 103 , 115, 117, 119\\nlayer normalization 99 – 105\\nlearning rate warmup 313 – 314\\n__len__ method 271\\nLIMA dataset 296\\nLinear layers 95 , 107, 329– 330, 332– 333\\nLinear layer weights 330\\nLinearWithLoRA layer 330 – 331, 333\\nLitGPT 249\\nLLama 2 model 141\\nLlama 3 model 238\\nllama.cpp library 238\\nLLMs (large language models) 17 – 18\\napplications of 4\\nbuilding and using 5– 7, 14\\ncoding architecture 93– 99\\ncoding attention mechanisms, causal attention \\nmechanism 74– 82\\nfine-tuning 230– 233, 238– 247, 295\\nfine-tuning for classification 183– 194, 200\\nimplementing GPT model, implementing feed \\nforward network with GELU \\nactivations 105– 109\\ninstruction fine-tuning, loading pretrained \\nLLMs 226– 229\\noverview of 1– 4\\npretraining 132, 140, 142, 146– 151, 159\\ntraining function 313, 319– 321\\ntraining loop, gradient clipping 317\\ntransformer architecture 7\\n– 10\\nutilizing large datasets 10\\nworking with text data, word embeddings 18– 20\\nloading, pretrained weights from OpenAI\\n160– 167\\nload_state_dict method 160\\nload_weights_into_gpt function 165 – 166, 182\\nlogistic regression loss function 293\\nlogits tensor 139\\nLoRALayer class 329 – 330\\nLoRA (low-rank adaptation) 247 , 322\\nparameter-efficient fine-tuning 324, 326\\nloss.backward() function 112\\nlosses 140 , 142\\nloss metric 132\\nlr (learning rate) 275\\nM\\nmachine learning 253\\nMachine Learning Q and AI (Raschka) 290\\nmacOS 282\\nmain function 286\\nmasked attention 74\\n.matmul method 261\\nmatrices 258 – 261\\nmax_length 38 , 141, 178, 306\\nminbpe repository 291\\nmodel_configs table 164\\nmodel.eval() function 160\\nmodel.named_parameters() function 112\\nmodel.parameters() method 129\\nmodel_response 238\\nmodel.train() setting 276\\nmodel weights, loading and saving in PyTorch\\n159\\nModule base class 265\\nmps device 224\\nmp.spawn() call 286\\nmulti-head attention 80 , 82– 91\\nimplementing with weight splits 86– 91\\nstacking multiple single-head attention \\nlayers 82– 85\\nMultiHeadAttention class 86 – 87, 90– 91, 292\\nMultiHeadAttentionWrapper class 83 – 87, 90\\nmultilayer neural networks, implementing\\n265– 269\\nmultinomial function 153 – 155\\nmultiprocessing.spawn function 284\\nmultiprocessing submodule 284\\nN\\nNeuralNetwork model 284\\nneural networks\\nimplementing feed forward network with \\nGELU activations 105– 109\\nimplementing multilayer neural networks\\n265– 269\\nNEW_CONFIG dictionary 164\\nn_heads 95\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 362, 'page_label': '341'}, page_content=\"INDEX 341\\nnn.Linear layers 72\\nnn.Module 71 , 97\\nnumel() method 120\\nnum_heads dimension 88\\nnum_tokens dimension 88\\nO\\nOllama application 238 , 241\\nOllama Llama 3 method 309\\nollama run command 242\\nollama run llama3 command 240 – 241\\nollama serve command 239 – 242\\nOLMo 294\\none-dimensional tensor (vector) 259\\nOpenAI, loading pretrained weights from\\n160– 167\\nOpenAI’s GPT-3 Language Model: A Technical \\nOverview 293\\noptimizer.step() method 276\\noptimizer.zero_grad() method 276\\nout_head 97\\noutput layer nodes 183\\n'output' object 208\\nP\\nparameter-efficient fine-tuning 322\\nLoRA (low-rank adaptation) 322\\npreparing dataset 324\\nparameters 129\\ncalculating 302\\nparams dictionary 162 , 164– 165\\npartial derivatives 263\\npartial function 224\\npeak_lr 314\\nperplexity 139\\nPhi-3 model 297\\nPHUDGE model 297\\npip installer 33\\nplot_losses function 232\\nplot_values function 199\\npos_embeddings 47\\nPost-LayerNorm 115\\npreference fine-tuning 298\\nPre-LayerNorm 115\\npretokenizes 212\\npretrained weights, initializing model with 181\\npretraining 7\\ncalculating text generation loss 132\\ncalculating training and validation set losses\\n140, 142\\ndecoding strategies to control randomness\\n151– 159\\nloading and saving model weights in \\nPyTorch 159\\nloading pretrained weights from OpenAI\\n160– 167\\non unlabeled data 128\\ntraining LLMs 146– 151\\nusing GPT to generate text 130\\nprint_gradients function 112\\nprint_sampled_tokens function 155 , 304\\nprint statement 24\\nPrometheus model 297\\nprompt styles 209\\n.pth extension 159\\nPython version 254\\nPyTorch\\nand Torch 256\\nautomatic differentiation 263– 265\\ncomputation graphs 261\\ndata loaders 210\\ndataset objects 325\\nefficient data loaders 270– 274\\nimplementing multilayer neural networks\\n265– 269\\ninstalling 254– 257\\nloading and saving model weights in 159\\noptimizing training performance with \\nGPUs 279– 288\\noverview 251– 257\\nsaving and loading models 278\\ntraining loops 274– 278\\nunderstanding tensors 258– 261\\nwith a NumPy-like API 258\\nQ\\nqkv_bias 95\\nQ query matrix 88\\nquery_llama function 243\\nquery_model function 242 – 243\\nR\\nrandom_split function 175\\nrank argument 286\\nraw text 6\\nregister_buffer 81\\nre library 22\\nReLU (rectified linear unit) 100 , 105\\n.replace() method 235\\nreplace_linear_with_lora function 330 , 332\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 363, 'page_label': '342'}, page_content='INDEX342\\n.reshape method 260 – 261\\nre.split command 22\\nresponses, extracting and saving 233 – 238\\nretrieval-augmented generation 19\\nr/LocalLLaMA subreddit 248\\nRMSNorm 292\\nRNNs (recurrent neural networks) 52\\nS\\nsaving and loading models 278\\nscalars 258 – 261\\nscaled dot-product attention 64\\nscaled_dot_product function 292\\nscale parameter 103\\nsci_mode parameter 102\\nSelfAttention class 90\\nself-attention mechanism 55 – 64\\ncomputing attention weights for all input \\ntokens 61– 64\\nimplementing with trainable weights 64– 74\\nwithout trainable weights 56– 61\\nSelfAttention_v1 class 71 , 73\\nSelfAttention_v2 class 73\\nself.out_proj layer 90\\nself.register_buffer() call 81\\nself.use_shortcut attribute 111\\nSequential class 267\\nset_printoptions method 277\\nsettings dictionary 162 , 164\\nSGD (stochastic gradient descent) 275\\n.shape attribute 260 , 271\\nshift parameter 103\\nshortcut connections 109 – 113\\nSimpleTokenizerV1 class 27\\nSimpleTokenizerV2 class 29 , 31, 33\\nsingle-head attention, stacking multiple layers\\n82– 85\\nsliding window 35 – 41\\nsoftmax function 269 , 276\\nsoftmax_naive function 60\\nSpamDataset class 176 , 178\\nspawn function 286\\nspecial context tokens 29 – 32\\nstate_dict 160 , 279\\nstride setting 39\\nstrip() function 229\\nsupervised data, fine-tuning model on 195 – 200\\nsupervised instruction fine-tuning 205\\npreparing dataset for 207– 211\\nsupervised learning 253\\nSwiGLU (Swish-gated linear unit) 105\\nT\\ntarget_chunk tensor 38\\ntargets tensor 139\\ntemperature scaling 151 – 152, 154– 155\\ntensor2d 259\\ntensor3d 259\\nTensor class 258\\ntensor library 252\\ntensors 258 – 261\\ncommon tensor operations 260\\nscalars, vectors, matrices, and tensors 258– 261\\ntensor data types 259\\nthree-dimensional tensor 259\\ntwo-dimensional tensor 259\\ntest_data set 246\\ntest_loader 272\\ntest_set dictionary 237 – 238\\ntext completion 205\\ntext data 17\\nadding special context tokens 29– 32\\nconverting tokens into token IDs 24– 29\\ncreating token embeddings 42– 43\\nencoding word positions 43– 47\\nsliding window 35– 41\\ntokenization, byte pair encoding 33– 35\\nword embeddings 18– 20\\ntext_data 314\\ntext generation 122\\nusing GPT to generate text 130\\ntext generation function, modifying 157\\ntext generation loss 132\\ntext_to_token_ids function 131\\ntiktoken package 176 , 178\\n.T method 261\\n.to() method 259 , 280\\ntoken_embedding_layer 46 – 47\\ntoken embeddings 42 – 43\\ntoken IDs 24 – 29\\ntoken_ids_to_text function 131\\ntokenization, byte pair encoding 33 – 35\\ntokenizing text 21 – 24\\ntop-k sampling 151 , 155– 156\\ntorch.argmax function 125\\ntorchaudio library 255\\ntorch.manual_seed(123) 272\\ntorch.nn.Linear layers 267\\ntorch.no_grad() context manager 269\\ntorch.save function 159\\ntorch.sum method 277\\ntorch.tensor function 258\\ntorchvision library 255\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 364, 'page_label': '343'}, page_content='INDEX 343\\ntotal_loss variable 145\\nToyDataset class 271\\ntqdm progress bar utility 242\\ntrain_classifier_simple function 197 , 200\\ntrain_data subset 143\\ntraining, optimizing performance with GPUs\\n279– 288\\nPyTorch computations on GPU devices 279\\nselecting available GPUs on multi-GPU \\nmachine 286– 288\\nsingle-GPU training 280\\ntraining with multiple GPUs 282– 288\\ntraining batches, organizing data into 211 – 223\\ntraining function 319 – 321\\nenhancing 313\\nmodified 319– 321\\ntraining loops 274 – 278\\ncosine decay 316\\ngradient clipping 317\\nlearning rate warmup 314\\ntrain_loader 272\\ntrain_model_simple function 147 , 149, 160, 195\\ntrain_ratio 142\\ntrain_simple_function 305\\ntransformer architecture 3 , 7– 10, 55\\nTransformerBlock class 115\\ntransformer blocks 93 , 185\\nconnecting attention and linear layers in\\n113– 116\\n.transpose method 87\\ntril function 75\\nU\\nUltraChat dataset 297\\nunbiased parameter 103\\nunlabeled data, decoding strategies to control \\nrandomness 151 – 159\\nV\\nval_data subset 143\\nvariable-length inputs 142\\nvectors 258 – 261\\n.view method 87\\nvocab_size 95\\nv vector 317\\nW\\n.weight attribute 129 , 161\\nweight_decay parameter 200\\nweight parameters 66 , 129\\nweights\\ninitializing model with pretrained weights 181\\nloading pretrained weights from OpenAI\\n160– 167\\nweight splits 86 – 91\\nW\\nk matrix 65 , 71\\nWord2Vec 19\\nword embeddings 18 – 20\\nword positions, encoding 43 – 47\\nW\\nq matrix 65 , 71, 88\\nWv matrix 65 , 71\\nX\\nX training example 268\\nZ\\nzero-dimensional tensor (scalar) 259\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 365, 'page_label': 'Promo page'}, page_content='For ordering information, go to www.manning.com\\nRELATED MANNING TITLES\\nDeep Learning with Python, Second Edition\\nby Francois Chollet\\nISBN 9781617296864\\n504 pages, $59.99 \\nOctober 2021\\nGenerative AI in Action\\nby Amit Bahree\\nISBN 9781633436947\\n469 pages (estimated), $59.99\\nOctober 2024 (estimated)\\nMachine Learning Algorithms in Depth\\nby Vadim Smolyakov\\nISBN 9781633439214\\n328 pages, $79.99\\nJuly 2024\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 366, 'page_label': 'Promo page'}, page_content='For ordering information, go to www.manning.com\\nRELATED MANNING TITLES\\nInside Deep Learning\\nby Edward Raff \\nForeword by Kirk Borne\\nISBN 9781617298639\\n600 pages, $59.99 \\nApril 2022\\nMath and Architectures of Deep Learning\\nby Krishnendu Chaudhury \\nwith Ananya H. Ashok, Sujay Narumanchi,  \\nDevashish Shankar \\nForeword by Prith Banerjee\\nISBN 9781617296482\\n552 pages, $69.99 \\nApril 2024\\nTransformers in Action\\nby Nicole Koenigstein\\nISBN 9781633437883\\n393 pages (estimated), $59.99\\nFebruary 2025 (estimated)\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 367, 'page_label': 'Promo page'}, page_content='Hands-on projects for learning your way\\nliveProjects are an exciting way to develop your skills that’s just like learning on the job.\\nIn a Manning liveProject, you tackle a real-world IT challenge and work out your own \\nsolutions. T o make sure you succeed, you’ll get 90 days of full and unlimited access to a \\nhand-picked list of Manning book and video resources.\\nHere’s how liveProject works:\\n•  Achievable milestones. Each project is broken down into steps and sections so \\nyou can keep track of your progress.\\n•  Collaboration and advice. Work with other liveProject participants through \\nchat, working groups, and peer project reviews.\\n•  Compare your results. See how your work shapes up against an expert \\nimplementation by the liveProject’s creator.\\n•  Everything you need to succeed. Datasets and carefully selected learning \\nresources come bundled with every liveProject.\\n•  Build your portfolio. All liveProjects teach skills that are in demand from \\nindustry. When you’re finished, you’ll have the satisfaction that comes with \\nsuccess and a real project to add to your portfolio.\\nExplore dozens of data, development, and cloud engineering \\nliveProjects at www.manning.com!\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 368, 'page_label': 'IBC'}, page_content='GPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text: This is an example\\nOutput text\\nPostprocessing steps\\nToken IDs:\\n.\\n12\\nThis section covers the\\nconcept of splitting\\ntext into tokens\\n389133205240134\\nA view of the text processing steps in the context of an LLM. The process starts with input \\ntext, which is broken down into tokens and then converted into numerical token IDs. These IDs \\nare linked to token embeddings that serve as the input for the GPT model. The model processes \\nthese embeddings and generates output text. Finally, the output undergoes postprocessing \\nsteps to produce the final text. This flow illustrates the basic operations of tokenization, \\nembedding, transformation, and postprocessing in a GPT model that is implemented from \\nthe ground up in this book.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'author': 'Sebastian Raschka', 'moddate': '2025-01-17T02:08:30-05:00', 'title': 'Build a Large Language Model (From Scratch)', 'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf', 'total_pages': 370, 'page': 369, 'page_label': 'Back cover'}, page_content='Sebastian Raschka\\nISBN-13: 978-1-63343-716-6\\nP\\nhysicist Richard P . Feynman reportedly said, “I don’t \\nunderstand anything I can’t build.” Based on this same \\npowerful principle, bestselling author Sebastian \\nRaschka guides you step by step as you build a \\nGPT-style \\nLLM that you can run on your laptop. Th is is an engaging \\nbook that covers each stage of the process, from planning \\nand coding to training and fi ne-tuning.\\nBuild a Large Language Model (From Scratch)  is a practical and \\neminently-satisfying hands-on journey into the foundations\\nof generative \\nAI. Without relying on any existing LLM \\nlibraries, you’ll code a base model, evolve it into a text clas-\\nsifi er, and ultimately create a chatbot that can follow your \\nconversational instructions. And you’ll really understand it \\nbecause you built it yourself!\\nWhat’s Inside\\n●  Plan and code an LLM comparable to GPT-2\\n●  Load pretrained weights\\n●  Construct a complete training pipeline\\n●  Fine-tune your LLM for text classifi cation\\n●  Develop LLMs that follow human instructions\\nReaders need intermediate Python skills and some know-\\nledge of machine learning. Th e LLM you create will run on \\nany modern laptop and can optionally utilize GPUs.\\nSebastian Raschka  is a Staff  Research Engineer at Lightning \\nAI, where he works on LLM research and develops open-\\nsource software.\\nTh e technical editor on this book was David Caswell.\\nFor print book owners, all ebook formats are free:\\nhttps:/ /www.manning.com/freebook\\nBUILD A  Large Language Model (FROM SCRATCH)\\nAI\\nMANNING\\n“\\nT ruly inspirational! It \\nmotivates you to put your \\n  new skills into action.\\n”—Benjamin Muskalla\\nSenior Engineer, GitHub \\n“\\nTh e most understandable \\nand comprehensive explana-\\ntion of language models yet! \\nIts unique and practical \\nteaching style achieves a level \\nof understanding you can’t \\nget any other way.\\n”—Cameron Wolfe\\nSenior Scientist, Netfl ix \\n“\\nSebastian combines deep \\nknowledge with practical \\nengineering skills and a knack \\nfor making complex ideas \\nsimple. Th is is the guide \\n  you need!\\n”—Chip Huyen, author of \\nDesigning Machine Learning Systems \\nand AI Engineering\\n“\\nDefi nitive, up-to-date cover-\\nage. Highly recommended!\\n”—Dr. Vahid Mirjalili, Senior Data \\nScientist, FM Global \\nSee first page')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = loader = PyPDFDirectoryLoader(\n",
    "    path = \"./pdfs/\",\n",
    "    glob = \"**/[!.]*.pdf\",\n",
    "    mode = \"page\",\n",
    "    headers = None,\n",
    "    extraction_mode = \"plain\",\n",
    "    # extraction_kwargs = None,\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778317f",
   "metadata": {},
   "source": [
    "Document Processing : Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a99168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c92443",
   "metadata": {},
   "source": [
    "Vector Database setup : Weaviate (Collection creation and Schema definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab60a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Document collection\n"
     ]
    }
   ],
   "source": [
    "# Delete the collection if it exists\n",
    "client.collections.delete(\"Chatbot\")\n",
    "\n",
    "# Create the collection with the new API\n",
    "client.collections.create(\n",
    "                name=\"Chatbot\",\n",
    "                properties=[\n",
    "                    weaviate.classes.config.Property(\n",
    "                        name=\"content\", data_type=weaviate.classes.config.DataType.TEXT\n",
    "                    ),\n",
    "                    weaviate.classes.config.Property(\n",
    "                        name=\"source\", data_type=weaviate.classes.config.DataType.TEXT\n",
    "                    ),\n",
    "                ],\n",
    "                vector_config=[\n",
    "                    Configure.Vectors.text2vec_huggingface(\n",
    "                        name=\"content_vector\",\n",
    "                        source_properties=[\"content\"],\n",
    "                        model=\"sentence-transformers/all-MiniLM-L6-v2\", #Embeddings model\n",
    "                        wait_for_model=True,\n",
    "                        use_cache=True,\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "\n",
    "print(\"Created Document collection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bfb8d",
   "metadata": {},
   "source": [
    "Custom Vectorstore implementation (As we are using version 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2866eae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='score to all other tokens.\\nFigure 6.12 The causal attention \\nmechanism, where the attention scores \\nbetween input tokens are displayed in a \\nmatrix format. The empty cells indicate \\nmasked positions due to the causal attention \\nmask, preventing tokens from attending to \\nfuture tokens. The values in the cells \\nrepresent attention scores; the last token, \\ntime, is the only one that computes \\nattention scores for all preceding tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='with trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='74 CHAPTER 3 Coding attention mechanisms\\nNext, we will make enhancements to the self-attention mechanism, focusing specifically\\non incorporating causal and multi-head elem ents. The causal aspect involves modify-\\ning the attention mechanism to prevent the model from accessing future information\\nin the sequence, which is crucial for tasks like language modeling, where each word\\nprediction should only depend on previous words. \\n The multi-head component involves spli tting the attention mechanism into multi-\\nple “heads.” Each head learns different aspects of the data, allowing the model to\\nsimultaneously attend to information from different representation subspaces at dif-\\nferent positions. This improves the model’s performance in complex tasks.\\n3.5 Hiding future words with causal attention\\nFor many LLM tasks, you will want the self -attention mechanism to consider only the\\ntokens that appear prior to the current posi tion when predicting the next token in a'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='attention\\nA simpliﬁed self-attention\\ntechnique to introduce the\\nbroader idea\\nSelf-attention with trainable\\nweights that forms the basis of\\nthe mechanism used in LLMs\\nA type of self-attention used in LLMs\\nthat allows a model to consider only\\nprevious and current inputs in a\\nsequence, ensuring temporal order\\nduring the text generation\\nAn extension of self-attention and\\ncausal attention that enables the\\nmodel to simultaneously attend\\nto information from different\\nrepresentation subspaces\\nFigure 3.2 The figure depicts different attention mechanisms we will code in this chapter, starting \\nwith a simplified version of self-attention before adding the trainable weights. The causal attention \\nmechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, \\nmulti-head attention organizes the attention mechanism into multiple heads, allowing the model to \\ncapture various aspects of the input data in parallel.'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='how this self-attention mechanism fits in to the broader context of implementing\\nan LLM. \\n1) Simpliﬁed\\nself-attention 2) Self-attention 3) Causal attention 4) Multi-head\\nattention\\nWe already implemented\\na simpliﬁed attention\\nmechanism.\\nWe will now extend the\\nself-attention mechanism\\nwith trainable weights.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM Foundation model\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.13 Previously, we coded a simplified attention mechanism to understand the basic mechanism behind \\nattention mechanisms. Now, we add trainable weights to this attention mechanism. Later, we will extend this \\nself-attention mechanism by adding a causal mask and multiple heads.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='82 CHAPTER 3 Coding attention mechanisms\\nThe resulting context vector is a three-di mensional tensor where each token is now\\nrepresented by a two-dimensional embedding:\\ncontext_vecs.shape: torch.Size([2, 6, 2])\\nFigure 3.23 summarizes what we have accomp lished so far. We have focused on the\\nconcept and implementation of  causal attention in neural networks. Next, we will\\nexpand on this concept and implement a multi-head attention module that imple-\\nments several causal attent ion mechanisms in parallel.\\n3.6 Extending single-head attention to multi-head \\nattention\\nOur final step will be to extend the previously implemented causal attention class over\\nmultiple heads. This is also called multi-head attention. \\n The term “multi-head” refers to dividi ng the attention mechanism into multiple\\n“heads,” each operating independently. In this context, a single causal attention mod-\\nule can be considered single-head attention,  where there is only one set of attention'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='50\\nCoding attention\\nmechanisms\\nAt this point, you know how to prepare the input text for training LLMs by splitting\\ntext into individual word and subword tokens, which can be encoded into vector rep-\\nresentations, embeddings, for the LLM. \\n Now, we will look at an integral part of the LLM architecture itself, attention\\nmechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\\nin isolation and focus on them at a mechanistic level. Then we will code the remaining\\nThis chapter covers\\n\\uf0a1 The reasons for using attention mechanisms in \\nneural networks\\n\\uf0a1 A basic self-attention framework, progressing to \\nan enhanced self-attention mechanism \\n\\uf0a1 A causal attention module that allows LLMs to \\ngenerate one token at a time\\n\\uf0a1 Masking randomly selected attention weights with \\ndropout to reduce overfitting\\n\\uf0a1 Stacking multiple causal attention modules into a \\nmulti-head attention module\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='ca = CausalAttention(d_in, d_out, context_length, 0.0)\\ncontext_vecs = ca(batch)\\nprint(\"context_vecs.shape:\", context_vecs.shape)\\nListing 3.3 A compact causal attention class\\nCompared \\nto the previous \\nSelfAttention_v1 \\nclass, we added a \\ndropout layer.\\nThe register_buffer call is also a new addition \\n(more information is provided in the following text).\\nWe transpose \\ndimensions 1 and 2, \\nkeeping the batch \\ndimension at the first \\nposition (0).\\nIn PyTorch, operations\\nwith a trailing underscore\\nare performed in-place,\\navoiding unnecessary\\nmemory copies.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='86 CHAPTER 3 Coding attention mechanisms\\n3.6.2 Implementing multi-head attention with weight splits\\nSo far, we have created a MultiHeadAttentionWrapper to implement multi-head\\nattention by stacking multiple single-head attention modules. This was done by instan-\\ntiating and combining several CausalAttention objects.\\n Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\\nCausalAttention, we can combine these concepts into a single MultiHeadAttention\\nclass. Also, in addition to merging the MultiHeadAttentionWrapper with the Causal-\\nAttention code, we will make some other mo difications to implement multi-head\\nattention more efficiently.\\n In the MultiHeadAttentionWrapper, multiple heads are implemented by creating\\na list of CausalAttention objects ( self.heads), each representing a separate atten-\\ntion head. The CausalAttention class independently performs the attention mecha-\\nnism, and the results from each head are concatenated. In contrast, the following'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='ward module and those that are contained in the multi-head attention module.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='833.6 Extending single-head attention to multi-head attention\\n Figure 3.24 illustrates the structure of  a multi-head attention module, which con-\\nsists of multiple single-head attention modules, as previously depicted in figure 3.18,\\nstacked on top of each other.\\nAs mentioned before, the main idea behind multi-head attention is to run the attention\\nmechanism multiple times (in parallel) with different, learned linear projections—the\\nresults of multiplying the input data (like the query, key, and value vectors in attention\\nmechanisms) by a weight matrix. In code, we can achieve this by implementing a sim-\\nple \\nMultiHeadAttentionWrapper class that stacks multiple  instances of our previously\\nimplemented CausalAttention module.\\n \\n \\n \\n0.7 0.2 0.1\\nInputs\\nFor multi-head attention with\\ntwo heads, we obtain two\\nattention weight matrices,\\nincluding causal and dropout\\nmasks.\\nQueries Keys Values\\n-0.7 -0.1\\nContext\\nvectors\\nWeight\\nmatrix\\nWeight\\nmatrix\\nWeight\\nmatrix\\nThe embedded input tokens'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='sequence. Causal attention, also known as masked attention, is a specialized form of self-\\nattention. It restricts a model to only consider previous and current inputs in a sequence\\nwhen processing any given token when computing attention scores. This is in contrast\\nto the standard self-attention mechanism, which allows access to the entire input\\nsequence at once.\\n Now, we will modify the standard self-attention mechanism to create a causal\\nattention mechanism, which is essential for developing an LLM in the subsequent\\nchapters. To achieve this in  GPT-like LLMs, for each token processed, we mask out\\nthe future tokens, which come after the current token in the input text, as illus-\\ntrated in figure 3.19. We mask out the attention weights above the diagonal, and we\\nYour\\nstarts\\nwith\\none\\nstep\\njourney\\nYour startswith one stepjourney\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.19 0.16 0.16 0.15 0.17 0.15\\n0.20 0.16 0.16 0.14 0.16 0.14\\n0.18 0.16 0.16 0.15 0.16 0.15\\n0.18 0.16 0.16 0.15 0.16 0.15'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='51\\nparts of the LLM surrounding the self-attention mechanism to see it in action and to\\ncreate a model to generate text.\\n We will implement four different variants of attention mechanisms, as illustrated in\\nfigure 3.2. These different attention variants  build on each other, and the goal is to\\nThis chapter implements the\\nattention mechanism, an important\\nbuilding block of GPT-like LLMs\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2 STAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights 8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.1 The three main stages of coding an LLM. This chapter focuses on step 2 of stage 1: implementing \\nattention mechanisms, which are an integral part of the LLM architecture.\\n1) Simpliﬁed\\nself-attention 2) Self-attention 3) Causal attention 4) Multi-head\\nattention'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='1) Mask with −∞\\nabove diagonal\\n2) Apply\\nsoftmax\\nAttention scores\\n(unnormalized)\\nMasked attention scores\\n(unnormalized)\\nMasked attention weights\\n(normalized)\\nFigure 3.21 A more efficient way to obtain the masked attention weight matrix in \\ncausal attention is to mask the attention scores with negative infinity values before \\napplying the softmax function.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='56 CHAPTER 3 Coding attention mechanisms\\nSince self-attention can appear complex, espe cially if you are encountering it for the\\nfirst time, we will begin by examining a simp lified version of it. Then we will imple-\\nment the self-attention mechanism with trainable weights used in LLMs.\\n3.3.1 A simple self-attention mechanism without trainable weights\\nLet’s begin by implementing a simplified variant of self-attention, free from any train-\\nable weights, as summarized in  figure 3.7. The goal is to illustrate a few key concepts\\nin self-attention before adding trainable weights.\\nThe “self” in self-attention  \\nIn self-attention, the “self” refers to the mechanism’s ability to compute attention\\nweights by relating different positions within a single input sequence. It assesses and\\nlearns the relationships and dependencies between various parts of the input itself,\\nsuch as words in a sentence or pixels in an image.'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='weights processing the input sequentially.\\n We will tackle this expansion from causal  attention to multi-head attention. First,\\nwe will intuitively build a multi-head at tention module by stacking multiple Causal-\\nAttention modules. Then we will then impl ement the same multi-head attention\\nmodule in a more complicated but more computationally efficient way.\\n3.6.1 Stacking multiple single-head attention layers\\nIn practical terms, implemen ting multi-head attention involves creating multiple\\ninstances of the self-attention mechanism (see figure 3.18), each with its own weights,\\nand then combining their outputs. Using multiple instances of  the self-attention\\nmechanism can be computationally intensive, but it’s crucial for the kind of complex\\npattern recognition that models like transformer-based LLMs are known for. \\n1) Simpliﬁed\\nself-attention 3) Causal attention2) Self-attention 4) Multi-head\\nattention\\nIn the previous section,\\nwe implemented a\\nself-attention mechanism'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='applied to the\\nattention scores will\\nzero out certain\\nattention scores\\nAttention weight for input\\ntokens corresponding to\\n“step” and “Your”\\n1.0\\n0.38 0.30 0.31\\n0.24 0.24\\n0.19 0.18\\n0.16 0.16 0.15 0.16\\nFigure 3.22 Using the causal attention mask (upper left), we apply an additional \\ndropout mask (upper right) to zero out additional attention weights to reduce overfitting \\nduring training.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='ever, the key insight is that when we renormalize the attention weights after masking,\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='CONTENTSviii\\n2.5 Byte pair encoding 33\\n2.6 Data sampling with a sliding window 35\\n2.7 Creating token embeddings 41\\n2.8 Encoding word positions 43\\n3 Coding attention mechanisms 50\\n3.1 The problem with modeling long sequences 52\\n3.2 Capturing data depend encies with attention \\nmechanisms 54\\n3.3 Attending to different parts of the input with \\nself-attention 55\\nA simple self-attention mechanism without trainable weights 56\\nComputing attention weights for all input tokens 61\\n3.4 Implementing self-attention with trainable weights 64\\nComputing the attention weights step by step 65 ■ Implementing a \\ncompact self-attention Python class 70\\n3.5 Hiding future words with causal attention 74\\nApplying a causal attention mask 75 ■ Masking additional \\nattention weights with dropout 78 ■ Implementing a compact \\ncausal attention class 80\\n3.6 Extending single-head attention to multi-head \\nattention 82\\nStacking multiple single-head attention layers 82 ■ Implementing'),\n",
       " Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='tracker at https:/ /github.com/pytorch/pytorch/issues/121595.\\n Having gained an understanding of causal attention and dropout masking, we can\\nnow develop a concise Python class. This cl ass is designed to facilitate the efficient\\napplication of these two techniques.\\n3.5.3 Implementing a compac t causal attention class\\nWe will now incorporate the causal atte ntion and dropout modifications into the\\nSelfAttention Python class we developed in section 3.4. This class will then serve as a\\ntemplate for developing multi-head attention, which is the final attention class we will\\nimplement.\\n But before we begin, let’s ensure that the code can handle batches consisting of\\nmore than one input so that the CausalAttention class supports the batch outputs\\nproduced by the data loader we implemented in chapter 2.\\n For simplicity, to simulate such batch inputs, we duplicate the input text example:\\nbatch = torch.stack((inputs, inputs), dim=0)\\nprint(batch.shape)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain_community.vectorstores import VectorStore\n",
    "from typing import List, Optional, Any\n",
    "from pydantic import Field\n",
    "\n",
    "class WeaviateV4Retriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever for Weaviate v4\"\"\"\n",
    "    vectorstore: Any = Field(description=\"The vectorstore to use\")\n",
    "    search_kwargs: dict = Field(default_factory=dict, description=\"Search kwargs\")\n",
    "\n",
    "# class WeaviateV4Retriever(BaseRetriever):\n",
    "#     def __init__(self, vectorstore, search_kwargs):\n",
    "#         self.vectorstore = vectorstore\n",
    "#         self.search_kwargs = search_kwargs\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:  # Changed method name\n",
    "        k = self.search_kwargs.get(\"k\", 4)\n",
    "        return self.vectorstore.similarity_search(query, k)\n",
    "\n",
    "class WeaviateV4VectorStore(VectorStore):\n",
    "    def __init__(self, client, collection_name):\n",
    "        self.client = client\n",
    "        self.collection_name = collection_name\n",
    "        self.collection = client.collections.get(collection_name)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_texts(cls, texts, embedding=None, metadatas=None, **kwargs):\n",
    "        \"\"\"Required method for VectorStore inheritance\"\"\"\n",
    "        client = kwargs.get(\"client\")\n",
    "        collection_name = kwargs.get(\"collection_name\")\n",
    "        \n",
    "        if not client or not collection_name:\n",
    "            raise ValueError(\"client and collection_name are required\")\n",
    "        \n",
    "        vectorstore = cls(client, collection_name)\n",
    "        \n",
    "        # Create documents from texts\n",
    "        documents = []\n",
    "        for i, text in enumerate(texts):\n",
    "            metadata = metadatas[i] if metadatas else {}\n",
    "            doc = Document(page_content=text, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "        \n",
    "        vectorstore.add_documents(documents)\n",
    "        return vectorstore\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"Add documents to Weaviate\"\"\"\n",
    "        data_objects = []\n",
    "        for doc in documents:\n",
    "            data_obj = {\n",
    "                \"content\": doc.page_content,\n",
    "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            }\n",
    "            data_objects.append(data_obj)\n",
    "        \n",
    "        self.collection.data.insert_many(data_objects)\n",
    "        return self\n",
    "    \n",
    "    def similarity_search(self, query, k=4, **kwargs):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        response = self.collection.query.near_text(\n",
    "            query=query,\n",
    "            limit=k,\n",
    "            return_properties=[\"content\", \"source\"]\n",
    "        )\n",
    "        \n",
    "        documents = []\n",
    "        for obj in response.objects:\n",
    "            doc = Document(\n",
    "                page_content=obj.properties.get(\"content\", \"\"),\n",
    "                metadata={\"source\": obj.properties.get(\"source\", \"\")}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def as_retriever(self, search_kwargs=None):\n",
    "        \"\"\"Return as LangChain retriever\"\"\"\n",
    "        return WeaviateV4Retriever(\n",
    "            vectorstore=self,\n",
    "            search_kwargs=search_kwargs or {}\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "vectordb = WeaviateV4VectorStore(client, \"Chatbot\")\n",
    "vectordb.add_documents(chunks)\n",
    "query = \"What is Causal Attention?\"\n",
    "vectordb.similarity_search(query, k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eaa022",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed744a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a1288",
   "metadata": {},
   "source": [
    "Query Processing\n",
    "\n",
    "<pre style=\"font-size: 12px;\">\n",
    "User Question → Query Embedding → Vector Search → Top-k Docs → DocuFormat → Prompt Template →  LLM    →   Answer\n",
    "     ↓               ↓               ↓              ↓          ↓            ↓                   ↓           ↓\n",
    "\"What is ML?\" → [0.1,0.3,...] → Similarity → [Doc1,2,3] → \"doc1\\ndoc2...\" → \"Use context...\" → \"ML is...\" → \"Final answer\"\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6666ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Causal Attention?',\n",
       " 'context': [Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='score to all other tokens.\\nFigure 6.12 The causal attention \\nmechanism, where the attention scores \\nbetween input tokens are displayed in a \\nmatrix format. The empty cells indicate \\nmasked positions due to the causal attention \\nmask, preventing tokens from attending to \\nfuture tokens. The values in the cells \\nrepresent attention scores; the last token, \\ntime, is the only one that computes \\nattention scores for all preceding tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='score to all other tokens.\\nFigure 6.12 The causal attention \\nmechanism, where the attention scores \\nbetween input tokens are displayed in a \\nmatrix format. The empty cells indicate \\nmasked positions due to the causal attention \\nmask, preventing tokens from attending to \\nfuture tokens. The values in the cells \\nrepresent attention scores; the last token, \\ntime, is the only one that computes \\nattention scores for all preceding tokens.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='with trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='with trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>')],\n",
       " 'answer': 'Causal attention is a mechanism in transformer models that prevents tokens from attending to future tokens, ensuring that the model processes data in a sequential order. It uses a causal mask to achieve this, effectively ignoring future tokens during the attention computation. This is crucial for tasks like language modeling where the order of tokens matters.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use with LangChain chains\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mistral-saba-24b\")\n",
    "\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "query = \"What is Causal Attention?\"\n",
    "chain.invoke({\"input\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e1ee4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain masked multi-head attention??',\n",
       " 'context': [Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='ever, the key insight is that when we renormalize the attention weights after masking,\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='ever, the key insight is that when we renormalize the attention weights after masking,\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='with trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>'),\n",
       "  Document(metadata={'source': 'pdfs\\\\Build_a_Large_Language_Model.pdf'}, page_content='with trainable weights.\\nIn this section, we extended\\nthe self-attention mechanism\\nwith a causal mask and\\ndropout mask.\\nIn the next section, we\\nextend causal attention\\nto multi-head attention.\\nFigure 3.23 Here’s what we’ve done so far. We began with a simplified attention mechanism, added trainable \\nweights, and then added a causal attention mask. Next, we will extend the causal attention mechanism and code \\nmulti-head attention, which we will use in our LLM.\\nLicensed to Gowtham Arulmozhi <arulmozg@oregonstate.edu>')],\n",
       " 'answer': \"Masked multi-head attention is an extension of the self-attention mechanism that incorporates both a causal mask and dropout mask. It involves using multiple attention heads to focus on different parts of the input sequence, each with its own set of trainable weights and applied masks. This method enhances the model's ability to capture various aspects of the input data.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain masked multi-head attention??\"\n",
    "vectordb.similarity_search(query, k=20)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bd6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
