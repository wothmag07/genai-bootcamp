{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP74tE3EEJU6Oo6UsAfx/8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wothmag07/genai-bootcamp/blob/main/BERTFinetuning_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0-5YqIQrOfx"
      },
      "outputs": [],
      "source": [
        "! pip install -q BitsandBytes transformers datasets seqeval evaluate accelerate tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from transformers import BertTokenizerFast, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n"
      ],
      "metadata": {
        "id": "Wy6tkMfsruYs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"eriktks/conll2003\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "YrfDSHN-0Rnw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CoNLL-2003 NER Dataset**\n",
        "\n",
        "The CoNLL-2003 dataset is a widely used benchmark for NER, featuring language-independent named entity recognition. It includes four entity types:\n",
        "\n",
        "* PER (Persons)\n",
        "* LOC (Locations)\n",
        "* ORG (Organizations)\n",
        "* MISC (Miscellaneous entities)\n",
        "\n",
        "**Dataset Format**\n",
        "\n",
        "Each data sample follows a structured format with four columns:\n",
        "\n",
        "* Word - The actual token in the sentence.\n",
        "* POS Tag - The part-of-speech tag.\n",
        "* Chunk Tag - The syntactic chunking label.\n",
        "* NER Tag - The named entity label in IOB2 format.\n",
        "\n",
        "**IOB2 Tagging Scheme**\n",
        "\n",
        "The dataset follows the IOB2 tagging scheme:\n",
        "\n",
        "* B-TYPE (Beginning) - Marks the first word of a named entity.\n",
        "* I-TYPE (Inside) - Marks subsequent words of a named entity.\n",
        "* O (Outside) - Indicates words that are not part of any named entity.\n",
        "\n",
        "Each word is placed on a separate line, and sentences are separated by empty lines.\n",
        "\n",
        "This dataset is commonly used for training NER models with deep learning and machine learning techniques, including LSTMs, CRFs, and Transformers (like BERT)."
      ],
      "metadata": {
        "id": "SSc3OGMj0sd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "gacqfYH8soKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de7754c-4983-444e-cd58-737c3106d044"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peUi-fB9zBYg",
        "outputId": "26233a89-e0c7-4c25-b6ee-c83ab3d7aad9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "    num_rows: 14041\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eg = dataset['train'][70]\n",
        "eg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-AfzIgjzIbB",
        "outputId": "d038a813-b831-470c-9a06-46717f5b341d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '70',\n",
              " 'tokens': ['GOV',\n",
              "  'LAW',\n",
              "  'GERMAN',\n",
              "  'HOME',\n",
              "  'CTRY',\n",
              "  '=',\n",
              "  'TAX',\n",
              "  'PROVS',\n",
              "  'STANDARD'],\n",
              " 'pos_tags': [22, 22, 22, 22, 22, 34, 21, 24, 38],\n",
              " 'chunk_tags': [11, 12, 12, 12, 12, 21, 11, 12, 21],\n",
              " 'ner_tags': [0, 0, 7, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].features"
      ],
      "metadata": {
        "id": "oSq3zD_9zMCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90568c09-1e2f-4b41-ac1c-c7471ddfcebb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': Value(dtype='string', id=None),\n",
              " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
              " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
              " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
              " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary describes the schema of a dataset, likely from a Natural Language Processing (NLP) task involving tokenization, Part-of-Speech (POS) tagging, chunking, and Named Entity Recognition (NER). Letâ€™s break it down:\n",
        "\n",
        "1. id\n",
        "Type: string\n",
        "Represents a unique identifier for each data sample.\n",
        "2. tokens\n",
        "Type: Sequence(Value(dtype='string'))\n",
        "A sequence of word tokens from the text.\n",
        "3. pos_tags (Part-of-Speech Tags)\n",
        "Type: Sequence(ClassLabel(...))\n",
        "Each token has a corresponding POS tag based on the Penn Treebank POS tagset, which includes:\n",
        "NN (noun, singular)\n",
        "VB (verb, base form)\n",
        "JJ (adjective)\n",
        "IN (preposition), etc.\n",
        "POS tagging helps in syntactic and semantic analysis of sentences.\n",
        "4. chunk_tags (Syntactic Chunking)\n",
        "Type: Sequence(ClassLabel(...))\n",
        "Indicates phrase chunks (e.g., noun phrases, verb phrases) using Inside-Outside-Beginning (IOB) tagging:\n",
        "B-NP (Begin noun phrase)\n",
        "I-NP (Inside noun phrase)\n",
        "B-VP (Begin verb phrase)\n",
        "I-VP (Inside verb phrase)\n",
        "O (Outside any chunk)\n",
        "Helps in grouping words into meaningful phrases.\n",
        "5. ner_tags (Named Entity Recognition)\n",
        "Type: Sequence(ClassLabel(...))\n",
        "Labels each token with an NER tag, identifying named entities like:\n",
        "B-PER (Beginning of a personâ€™s name)\n",
        "I-PER (Inside a personâ€™s name)\n",
        "B-ORG (Beginning of an organization)\n",
        "I-ORG (Inside an organization)\n",
        "B-LOC (Beginning of a location)\n",
        "I-LOC (Inside a location)\n",
        "O (Outside any entity)\n",
        "This helps in recognizing important entities in text."
      ],
      "metadata": {
        "id": "a8x_JiRteKiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", padding=True, truncation=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "447YWkoweL3X",
        "outputId": "ef215805-a54b-43b8-c64b-84a3af7dfd76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ids = tokenizer(eg[\"tokens\"],is_split_into_words=True)\n",
        "print(tokenized_ids)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_ids[\"input_ids\"])\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWBo89rbZtLt",
        "outputId": "3816bf74-63dd-4023-df7a-299a97bb59d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 18079, 2375, 2446, 2188, 14931, 2854, 1027, 4171, 4013, 15088, 3115, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "['[CLS]', 'gov', 'law', 'german', 'home', 'ct', '##ry', '=', 'tax', 'pro', '##vs', 'standard', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "ner_pipeline(\"My name is Wolfgang and I live in Berlin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nMv4lGxhltK",
        "outputId": "c33c03a6-9292-4d07-dfec-15c7194544ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'LABEL_4',\n",
              "  'score': np.float32(0.14410286),\n",
              "  'index': 1,\n",
              "  'word': 'my',\n",
              "  'start': 0,\n",
              "  'end': 2},\n",
              " {'entity': 'LABEL_4',\n",
              "  'score': np.float32(0.1356418),\n",
              "  'index': 2,\n",
              "  'word': 'name',\n",
              "  'start': 3,\n",
              "  'end': 7},\n",
              " {'entity': 'LABEL_6',\n",
              "  'score': np.float32(0.14858478),\n",
              "  'index': 3,\n",
              "  'word': 'is',\n",
              "  'start': 8,\n",
              "  'end': 10},\n",
              " {'entity': 'LABEL_2',\n",
              "  'score': np.float32(0.18200429),\n",
              "  'index': 4,\n",
              "  'word': 'wolfgang',\n",
              "  'start': 11,\n",
              "  'end': 19},\n",
              " {'entity': 'LABEL_8',\n",
              "  'score': np.float32(0.18727687),\n",
              "  'index': 5,\n",
              "  'word': 'and',\n",
              "  'start': 20,\n",
              "  'end': 23},\n",
              " {'entity': 'LABEL_6',\n",
              "  'score': np.float32(0.16109316),\n",
              "  'index': 6,\n",
              "  'word': 'i',\n",
              "  'start': 24,\n",
              "  'end': 25},\n",
              " {'entity': 'LABEL_0',\n",
              "  'score': np.float32(0.15144825),\n",
              "  'index': 7,\n",
              "  'word': 'live',\n",
              "  'start': 26,\n",
              "  'end': 30},\n",
              " {'entity': 'LABEL_4',\n",
              "  'score': np.float32(0.14949417),\n",
              "  'index': 8,\n",
              "  'word': 'in',\n",
              "  'start': 31,\n",
              "  'end': 33},\n",
              " {'entity': 'LABEL_0',\n",
              "  'score': np.float32(0.1467645),\n",
              "  'index': 9,\n",
              "  'word': 'berlin',\n",
              "  'start': 34,\n",
              "  'end': 40}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_tokenize(samples, label_all_tokens=True):\n",
        "  \"\"\"\n",
        "  Tokenizes and aligns labels with sub-tokens for a given dataset sample.\n",
        "  \"\"\"\n",
        "\n",
        "  # Tokenization (no change)\n",
        "  tokenized_inputs = tokenizer(samples['tokens'], truncation=True, is_split_into_words=True)\n",
        "  labels = []\n",
        "\n",
        "  for i, label in enumerate(samples['ner_tags']):\n",
        "      word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "\n",
        "      prev_word_index = None\n",
        "      label_ids = []\n",
        "\n",
        "      for word_index in word_ids:\n",
        "          if word_index is None:\n",
        "              label_ids.append(-100)\n",
        "          elif word_index != prev_word_index:\n",
        "              label_ids.append(label[word_index])\n",
        "          else:\n",
        "              label_ids.append(label[word_index] if label_all_tokens else -100)\n",
        "          prev_word_index = word_index\n",
        "      labels.append(label_ids)\n",
        "  # Return (no change)\n",
        "  tokenized_inputs['labels'] = labels # changed to label_ids\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "XfmTUfusiRyr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = label_tokenize(samples=dataset['train'][0:1])\n",
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S33yezNWVgwd",
        "outputId": "9fd00a99-724e-49dc-d95b-b5206659119c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q[\"labels\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtXoNxjj-WZq",
        "outputId": "28a128f1-d3e0-423f-c019-2bc758f9d1c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
        "    print(f\"{token:_<40} {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ0GHvjBWWmK",
        "outputId": "a58ed1c9-7b33-48a1-8ae3-80eb11943bc2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]___________________________________ -100\n",
            "eu______________________________________ 3\n",
            "rejects_________________________________ 0\n",
            "german__________________________________ 7\n",
            "call____________________________________ 0\n",
            "to______________________________________ 0\n",
            "boycott_________________________________ 0\n",
            "british_________________________________ 7\n",
            "lamb____________________________________ 0\n",
            "._______________________________________ 0\n",
            "[SEP]___________________________________ -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = dataset.map(label_tokenize, batched=True)\n",
        "tokenized_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNU8C8dCi6cw",
        "outputId": "f86b60f1-32bd-4a88-8011-c67b25482169"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listing the labels"
      ],
      "metadata": {
        "id": "2urzEQ4l_Dvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list=dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_id_dict = {i: label for i, label in enumerate(label_list)}\n",
        "label_id_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIRFn0gz_DPV",
        "outputId": "b1bef9dc-a379-4732-cd0d-1929106316d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-PER',\n",
              " 2: 'I-PER',\n",
              " 3: 'B-ORG',\n",
              " 4: 'I-ORG',\n",
              " 5: 'B-LOC',\n",
              " 6: 'I-LOC',\n",
              " 7: 'B-MISC',\n",
              " 8: 'I-MISC'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets consider our eg sample"
      ],
      "metadata": {
        "id": "btybJgb9AC2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRIkxGGJACQY",
        "outputId": "2776e7b7-ef76-44d6-a461-532f845682a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '70',\n",
              " 'tokens': ['GOV',\n",
              "  'LAW',\n",
              "  'GERMAN',\n",
              "  'HOME',\n",
              "  'CTRY',\n",
              "  '=',\n",
              "  'TAX',\n",
              "  'PROVS',\n",
              "  'STANDARD'],\n",
              " 'pos_tags': [22, 22, 22, 22, 22, 34, 21, 24, 38],\n",
              " 'chunk_tags': [11, 12, 12, 12, 12, 21, 11, 12, 21],\n",
              " 'ner_tags': [0, 0, 7, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label_list[i] for i in eg[\"ner_tags\"]]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCqrgSU8AH3J",
        "outputId": "c2fc394e-01c6-4bfc-fa19-a3c623090e7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in model.parameters():\n",
        "#   print(param)"
      ],
      "metadata": {
        "id": "Hj-mVloxlcd0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [[label_id_dict[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    true_labels = [[label_id_dict[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "W7dwoPq5OYVt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "args = TrainingArguments(\"valid-ner\",\n",
        "                         evaluation_strategy = \"epoch\",\n",
        "                         learning_rate=3e-4,\n",
        "                         per_device_train_batch_size=16,\n",
        "                         per_device_eval_batch_size=16,\n",
        "                         num_train_epochs=25,\n",
        "                         weight_decay=0.01,\n",
        "                         report_to=\"none\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C52KeovwVim",
        "outputId": "dd9f7417-791b-453b-8dd8-380fe10fb12c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-21-2a6ef10fb2ef>:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "r15S6SQZVibK",
        "outputId": "e390bcfb-3cb9-4552-ce3c-aff34b4e364e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8880' max='21950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8880/21950 27:53 < 41:04, 5.30 it/s, Epoch 10.11/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.220800</td>\n",
              "      <td>0.153866</td>\n",
              "      <td>0.783666</td>\n",
              "      <td>0.807249</td>\n",
              "      <td>0.795283</td>\n",
              "      <td>0.957663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.139400</td>\n",
              "      <td>0.142965</td>\n",
              "      <td>0.817209</td>\n",
              "      <td>0.828728</td>\n",
              "      <td>0.822928</td>\n",
              "      <td>0.961348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.110700</td>\n",
              "      <td>0.155932</td>\n",
              "      <td>0.811806</td>\n",
              "      <td>0.809263</td>\n",
              "      <td>0.810532</td>\n",
              "      <td>0.958235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.097900</td>\n",
              "      <td>0.226210</td>\n",
              "      <td>0.750453</td>\n",
              "      <td>0.786889</td>\n",
              "      <td>0.768239</td>\n",
              "      <td>0.947496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0.169302</td>\n",
              "      <td>0.821925</td>\n",
              "      <td>0.826155</td>\n",
              "      <td>0.824035</td>\n",
              "      <td>0.960808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.586100</td>\n",
              "      <td>0.973507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.789108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.925800</td>\n",
              "      <td>0.976074</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.789108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.921100</td>\n",
              "      <td>0.987920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.789108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.918400</td>\n",
              "      <td>0.986001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.789108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.923100</td>\n",
              "      <td>0.975173</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.789108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}